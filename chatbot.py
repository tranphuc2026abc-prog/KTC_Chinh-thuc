import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import uuid
import unicodedata 
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator
from collections import defaultdict

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply() 
    
    # Th·ª≠ import PyMuPDF (fitz) - Th∆∞ vi·ªán quan tr·ªçng cho x·ª≠ l√Ω PDF n√¢ng cao
    try:
        import fitz
    except ImportError:
        st.error("Thi·∫øu th∆∞ vi·ªán pymupdf. H√£y ch·∫°y: pip install pymupdf")
        fitz = None

    try:
        from llama_parse import LlamaParse 
    except ImportError:
        LlamaParse = None
        
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================================================================
# 1. MODULE X·ª¨ L√ù PDF N√ÇNG CAO (ADVANCED PROCESSING KERNEL)
# Ph·∫ßn n√†y ƒë∆∞·ª£c t√≠ch h·ª£p tr·ª±c ti·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh "Context-Aware" cho KHKT
# ==============================================================================

class VietnameseTextbookProcessor:
    """
    B·ªô x·ª≠ l√Ω chuy√™n d·ª•ng cho SGK Vi·ªát Nam (Tin h·ªçc 10 KNTT).
    Nhi·ªám v·ª•: C·∫Øt vƒÉn b·∫£n theo c·∫•u tr√∫c Ch∆∞∆°ng/B√†i thay v√¨ c·∫Øt m√π qu√°ng.
    """
    
    # C√°c m·∫´u Regex ƒë·ªÉ nh·∫≠n di·ªán c·∫•u tr√∫c
    NOISE_PATTERNS = [
        r'K·∫æT\s+N·ªêI\s+TRI\s+TH·ª®C\s+V·ªöI\s+CU·ªòC\s+S·ªêNG',
        r'TIN\s+H·ªåC\s+\d+',
        r'CH∆Ø∆†NG\s+TR√åNH\s+GI√ÅO\s+D·ª§C',
        r'PH√ÇN\s+PH·ªêI\s+CH∆Ø∆†NG\s+TR√åNH',
        r'^\s*\d+\s*$',  # S·ªë trang ƒë·ª©ng m·ªôt m√¨nh
    ]
    
    # M·∫´u nh·∫≠n di·ªán Ch·ªß ƒë·ªÅ (VD: Ch·ªß ƒë·ªÅ 1. M√ÅY T√çNH...)
    TOPIC_PATTERN = re.compile(r'(?:^|\n)\s*CH·ª¶\s+ƒê·ªÄ\s+(\d+)[\.:]?\s+(.+)', re.IGNORECASE)
    
    # M·∫´u nh·∫≠n di·ªán B√†i h·ªçc (VD: B√ÄI 1. TH√îNG TIN...)
    LESSON_PATTERN = re.compile(r'(?:^|\n)\s*B√ÄI\s+(\d+)[\.:]?\s+(.+)', re.IGNORECASE)

    @staticmethod
    def clean_text(text: str) -> str:
        """L√†m s·∫°ch vƒÉn b·∫£n c∆° b·∫£n"""
        # Chu·∫©n h√≥a Unicode (d·ª±ng s·∫µn)
        text = unicodedata.normalize('NFC', text)
        return text.strip()

    @classmethod
    def process_pdf(cls, pdf_path: str, chunk_size: int = 1000) -> List[Document]:
        """
        H√†m x·ª≠ l√Ω ch√≠nh: ƒê·ªçc PDF -> Ph√¢n t√≠ch c·∫•u tr√∫c -> T·∫°o Document c√≥ Metadata x·ªãn
        """
        if not fitz:
            raise ImportError("C·∫ßn c√†i ƒë·∫∑t th∆∞ vi·ªán pymupdf (pip install pymupdf)")

        doc = fitz.open(pdf_path)
        processed_docs = []
        
        # Bi·∫øn tr·∫°ng th√°i (State Machine)
        current_topic = "Ch∆∞a ph√¢n lo·∫°i"
        current_lesson = "N·ªôi dung chung"
        
        # B·ªô ƒë·ªám n·ªôi dung cho b√†i h·ªçc hi·ªán t·∫°i
        current_content_buffer = []
        current_page_nums = set()
        
        print(f"üîÑ ƒêang x·ª≠ l√Ω file: {os.path.basename(pdf_path)}...")

        for page_num, page in enumerate(doc):
            text = page.get_text("text")
            lines = text.split('\n')
            
            for line in lines:
                line = cls.clean_text(line)
                if not line: continue
                
                # 1. L·ªçc nhi·ªÖu (Noise Filtering)
                is_noise = False
                for pattern in cls.NOISE_PATTERNS:
                    if re.search(pattern, line, re.IGNORECASE):
                        is_noise = True
                        break
                if is_noise: continue
                
                # 2. Ph√°t hi·ªán c·∫•u tr√∫c (Structure Detection)
                # Ki·ªÉm tra xem c√≥ ph·∫£i b·∫Øt ƒë·∫ßu Ch·ªß ƒë·ªÅ m·ªõi kh√¥ng
                topic_match = cls.TOPIC_PATTERN.search(line)
                if topic_match:
                    # L∆∞u n·ªôi dung b√†i c≈© tr∆∞·ªõc khi sang ch·ªß ƒë·ªÅ m·ªõi
                    if current_content_buffer:
                        processed_docs.extend(cls._create_chunks(
                            current_content_buffer, current_topic, current_lesson, 
                            list(current_page_nums), os.path.basename(pdf_path), chunk_size
                        ))
                        current_content_buffer = []
                        current_page_nums = set()
                    
                    current_topic = f"Ch·ªß ƒë·ªÅ {topic_match.group(1)}: {topic_match.group(2)}"
                    current_lesson = "Gi·ªõi thi·ªáu ch·ªß ƒë·ªÅ" # Reset lesson
                    continue

                # Ki·ªÉm tra xem c√≥ ph·∫£i b·∫Øt ƒë·∫ßu B√†i h·ªçc m·ªõi kh√¥ng
                lesson_match = cls.LESSON_PATTERN.search(line)
                if lesson_match:
                    # L∆∞u n·ªôi dung b√†i c≈© tr∆∞·ªõc khi sang b√†i m·ªõi
                    if current_content_buffer:
                        processed_docs.extend(cls._create_chunks(
                            current_content_buffer, current_topic, current_lesson, 
                            list(current_page_nums), os.path.basename(pdf_path), chunk_size
                        ))
                        current_content_buffer = []
                        current_page_nums = set()
                    
                    current_lesson = f"B√†i {lesson_match.group(1)}: {lesson_match.group(2)}"
                    continue

                # 3. T√≠ch l≈©y n·ªôi dung
                current_content_buffer.append(line)
                current_page_nums.add(page_num + 1)

        # L∆∞u ph·∫ßn c√≤n l·∫°i cu·ªëi c√πng
        if current_content_buffer:
            processed_docs.extend(cls._create_chunks(
                current_content_buffer, current_topic, current_lesson, 
                list(current_page_nums), os.path.basename(pdf_path), chunk_size
            ))
            
        return processed_docs

    @staticmethod
    def _create_chunks(buffer: List[str], topic: str, lesson: str, pages: List[int], source: str, chunk_size: int) -> List[Document]:
        """Chia nh·ªè n·ªôi dung c·ªßa m·ªôt b√†i h·ªçc th√†nh c√°c chunk v·ª´a ph·∫£i"""
        full_text = "\n".join(buffer)
        if len(full_text) < 50: return [] # B·ªè qua n·ªôi dung qu√° ng·∫Øn
        
        # D√πng RecursiveCharacterTextSplitter nh∆∞ng ch·ªâ √°p d·ª•ng TRONG PH·∫†M VI 1 B√ÄI H·ªåC
        # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o kh√¥ng bao gi·ªù 1 chunk lai t·∫°p gi·ªØa 2 b√†i kh√°c nhau
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        texts = splitter.split_text(full_text)
        
        docs = []
        for t in texts:
            # Metadata Enrichment (Quan tr·ªçng cho UI hi·ªÉn th·ªã)
            metadata = {
                "source": source,
                "chapter": topic,   # Map v√†o bi·∫øn 'topic' c·ªßa UI
                "lesson": lesson,   # Map v√†o bi·∫øn 'lesson' c·ªßa UI
                "page": pages[0] if pages else 0 # L·∫•y trang ƒë·∫ßu ti√™n c·ªßa ƒëo·∫°n n√†y
            }
            docs.append(Document(page_content=t, metadata=metadata))
        return docs

# H√†m wrapper ƒë·ªÉ g·ªçi d·ªÖ d√†ng
def process_pdf_advanced(pdf_path: str) -> List[Document]:
    return VietnameseTextbookProcessor.process_pdf(pdf_path)


# ==============================================================================
# 2. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================================================================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    MODELS = {
        "Llama 3 70B": "llama3-70b-8192",
        "Mixtral 8x7B": "mixtral-8x7b-32768",
        "Gemma 7B": "gemma-7b-it"
    }
    
    # Vector DB Config
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    # Paths
    VECTOR_DB_PATH = "faiss_index"
    UPLOAD_DIR = "uploaded_docs"

# ==============================================================================
# 3. RAG ENGINE (CORE LOGIC)
# ==============================================================================

class RAGEngine:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
        self.vector_store = None
        self.ensure_directories()
        
    def ensure_directories(self):
        os.makedirs(AppConfig.UPLOAD_DIR, exist_ok=True)
        
    def get_groq_client(self):
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            if "GROQ_API_KEY" in st.session_state:
                api_key = st.session_state.GROQ_API_KEY
            else:
                return None
        return Groq(api_key=api_key)

    def _read_and_process_files(self, files) -> List[Document]:
        """
        H√†m ƒë·ªçc file ƒë√£ ƒë∆∞·ª£c N√ÇNG C·∫§P ƒë·ªÉ s·ª≠ d·ª•ng thu·∫≠t to√°n m·ªõi
        """
        documents = []
        progress_text = "ƒêang ph√¢n t√≠ch c·∫•u tr√∫c t√†i li·ªáu..."
        my_bar = st.progress(0, text=progress_text)
        
        for idx, file in enumerate(files):
            temp_path = os.path.join(AppConfig.UPLOAD_DIR, file.name)
            
            # L∆∞u file t·∫°m
            with open(temp_path, "wb") as f:
                f.write(file.getbuffer())
            
            try:
                if file.name.endswith('.pdf'):
                    # --- [KHKT HIGHLIGHT] G·ªåI THU·∫¨T TO√ÅN X·ª¨ L√ù N√ÇNG CAO ---
                    # Thay v√¨ d√πng PyPDFLoader th√¥ng th∆∞·ªùng, ta g·ªçi h√†m x·ª≠ l√Ω th√¥ng minh
                    st.info(f"üöÄ ƒêang k√≠ch ho·∫°t ch·∫ø ƒë·ªô ƒë·ªçc hi·ªÉu c·∫•u tr√∫c cho: {file.name}")
                    file_docs = process_pdf_advanced(temp_path)
                    
                    if not file_docs:
                        st.warning(f"Kh√¥ng t√¨m th·∫•y n·ªôi dung trong {file.name}")
                    else:
                        documents.extend(file_docs)
                        
                # X·ª≠ l√Ω c√°c lo·∫°i file kh√°c n·∫øu c·∫ßn (txt, docx...)
                else:
                    st.warning(f"Hi·ªán t·∫°i ch·ªâ h·ªó tr·ª£ t·ªëi ∆∞u cho PDF: {file.name}")
                    
            except Exception as e:
                st.error(f"L·ªói khi x·ª≠ l√Ω {file.name}: {str(e)}")
            finally:
                # D·ªçn d·∫πp file t·∫°m
                if os.path.exists(temp_path):
                    os.remove(temp_path)
            
            my_bar.progress((idx + 1) / len(files), text=progress_text)
            
        my_bar.empty()
        return documents

    def build_vector_store(self, uploaded_files):
        """X√¢y d·ª±ng vector store t·ª´ file upload"""
        if not uploaded_files:
            return False

        with st.spinner("üîÑ ƒêang c·∫•u tr√∫c h√≥a d·ªØ li·ªáu (Semantic Segmentation)..."):
            # 1. X·ª≠ l√Ω file v·ªõi thu·∫≠t to√°n m·ªõi
            docs = self._read_and_process_files(uploaded_files)
            
            if not docs:
                st.error("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c d·ªØ li·ªáu kh·∫£ d·ª•ng.")
                return False
            
            # 2. T·∫°o Vector Store
            try:
                self.vector_store = FAISS.from_documents(docs, self.embeddings)
                self.vector_store.save_local(AppConfig.VECTOR_DB_PATH)
                st.success(f"‚úÖ ƒê√£ n·∫°p th√†nh c√¥ng {len(docs)} ph√¢n ƒëo·∫°n ki·∫øn th·ª©c chu·∫©n c·∫•u tr√∫c!")
                return True
            except Exception as e:
                st.error(f"L·ªói kh·ªüi t·∫°o Vector Store: {str(e)}")
                return False

    def load_vector_store(self):
        """Load vector store ƒë√£ l∆∞u"""
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                self.vector_store = FAISS.load_local(
                    AppConfig.VECTOR_DB_PATH, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                return True
            except Exception:
                return False
        return False

    def query(self, user_question: str, model_name: str, k: int = 4):
        """Truy v·∫•n v√† tr·∫£ l·ªùi"""
        client = self.get_groq_client()
        if not client or not self.vector_store:
            return "Vui l√≤ng nh·∫≠p API Key v√† n·∫°p d·ªØ li·ªáu.", []

        # 1. Retrieve (Truy t√¨m)
        retriever = self.vector_store.as_retriever(search_kwargs={"k": k*2}) # L·∫•y d∆∞ ƒë·ªÉ rerank
        docs = retriever.invoke(user_question)
        
        # 2. Rerank (S·∫Øp x·∫øp l·∫°i - T√πy ch·ªçn n√¢ng cao)
        # (·ªû ƒë√¢y gi·ªØ logic ƒë∆°n gi·∫£n ƒë·ªÉ ƒë·∫£m b·∫£o t·ªëc ƒë·ªô, c√≥ th·ªÉ b·∫≠t FlashRank n·∫øu c·∫ßn)
        final_docs = docs[:k]

        # 3. T·∫°o Context
        context_parts = []
        evidence_list = []
        
        for doc in final_docs:
            # L·∫•y metadata chu·∫©n ƒë√£ x·ª≠ l√Ω
            chapter = doc.metadata.get("chapter", "Ch∆∞∆°ng ch∆∞a x√°c ƒë·ªãnh")
            lesson = doc.metadata.get("lesson", "B√†i ch∆∞a x√°c ƒë·ªãnh")
            page = doc.metadata.get("page", "?")
            source = doc.metadata.get("source", "T√†i li·ªáu")
            
            context_parts.append(f"""
            [Ngu·ªìn: {source} | Trang: {page}]
            [V·ªã tr√≠: {chapter} > {lesson}]
            N·ªôi dung: {doc.page_content}
            """)
            
            evidence_list.append({
                "source": source,
                "chapter": chapter,
                "lesson": lesson,
                "page": page,
                "content": doc.page_content,
                "max_score": 0.9, # Fake score cho UI
                "count": 1
            })

        context_str = "\n---\n".join(context_parts)
        
        # 4. Generate Answer
        system_prompt = f"""B·∫°n l√† Tr·ª£ l√Ω AI gi√°o d·ª•c c·ªßa tr∆∞·ªùng THCS & THPT Ph·∫°m Ki·ªát.
        Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
        
        Y√äU C·∫¶U:
        1. Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn, s∆∞ ph·∫°m.
        2. B·∫ÆT BU·ªòC tr√≠ch d·∫´n ngu·ªìn (B√†i n√†o, trang n√†o) n·∫øu th√¥ng tin c√≥ trong ng·ªØ c·∫£nh.
        3. N·∫øu kh√¥ng c√≥ th√¥ng tin trong ng·ªØ c·∫£nh, h√£y n√≥i "Xin l·ªói, t√†i li·ªáu hi·ªán t·∫°i ch∆∞a ƒë·ªÅ c·∫≠p v·∫•n ƒë·ªÅ n√†y."
        
        NG·ªÆ C·∫¢NH H·ªåC LI·ªÜU:
        {context_str}
        """

        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_question}
                ],
                model=AppConfig.MODELS.get(model_name, "llama3-70b-8192"),
                temperature=0.3, # Gi·∫£m nhi·ªát ƒë·ªô ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
                max_tokens=2048,
            )
            return chat_completion.choices[0].message.content, evidence_list
        except Exception as e:
            return f"L·ªói khi g·ªçi API: {str(e)}", []

# ==============================================================================
# 4. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (STREAMLIT UI)
# Gi·ªØ nguy√™n 100% ƒë·ªÉ ƒë·∫£m b·∫£o tr·∫£i nghi·ªám quen thu·ªôc
# ==============================================================================

def main():
    # --- CSS T√πy ch·ªânh ---
    st.markdown("""
    <style>
    .evidence-card {
        background-color: #f0f2f6;
        border-left: 5px solid #4CAF50;
        padding: 10px;
        margin-bottom: 10px;
        border-radius: 5px;
    }
    .evidence-header {
        font-weight: bold;
        color: #1E88E5;
        display: flex;
        justify-content: space-between;
    }
    .evidence-context {
        font-size: 0.9em;
        color: #666;
        margin-top: 5px;
        font-style: italic;
    }
    .evidence-confidence {
        font-size: 0.8em;
        background: #e3f2fd;
        padding: 2px 6px;
        border-radius: 10px;
        color: #1565c0;
    }
    .stChatMessage {
        background-color: transparent !important;
    }
    </style>
    """, unsafe_allow_html=True)

    # --- Header ---
    col1, col2 = st.columns([1, 5])
    with col1:
        # Placeholder cho Logo
        st.markdown("ü§ñ **KTC-Bot**") 
    with col2:
        st.title("Tr·ª£ l√Ω H·ªçc t·∫≠p Th√¥ng minh - Ph·∫°m Ki·ªát School")
        st.caption("üöÄ Phi√™n b·∫£n KHKT Qu·ªëc gia: T√≠ch h·ª£p Context-Aware RAG Engine")

    # --- Sidebar ---
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        # API Key
        api_key = st.text_input("Groq API Key", type="password", placeholder="gsk_...")
        if api_key:
            st.session_state.GROQ_API_KEY = api_key
            
        # Model Selection
        selected_model = st.selectbox("M√¥ h√¨nh AI", list(AppConfig.MODELS.keys()))
        
        st.divider()
        
        # File Uploader
        st.subheader("üìö N·∫°p T√†i Li·ªáu (SGK, B√†i gi·∫£ng)")
        uploaded_files = st.file_uploader(
            "Ch·ªçn file PDF (Tin 10_KNTT.pdf)", 
            type=['pdf'], 
            accept_multiple_files=True
        )
        
        if st.button("üöÄ Kh·ªüi t·∫°o H·ªá th·ªëng Tri th·ª©c", type="primary"):
            if not uploaded_files:
                st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 file!")
            elif not api_key and "GROQ_API_KEY" not in st.session_state:
                st.warning("Vui l√≤ng nh·∫≠p API Key!")
            else:
                engine = RAGEngine()
                if engine.build_vector_store(uploaded_files):
                    st.session_state.engine_ready = True
                    st.rerun()

        st.divider()
        st.info("üí° M·∫πo: H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c 'Ch·ªß ƒë·ªÅ' v√† 'B√†i h·ªçc' trong SGK.")

    # --- Main Chat Area ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            # Render Evidence n·∫øu c√≥
            if "evidence" in msg and msg["evidence"]:
                with st.expander("üìö Ki·ªÉm ch·ª©ng ngu·ªìn g·ªëc (Evidence)", expanded=False):
                    seen = set()
                    for item in msg["evidence"]:
                        # Deduplicate ƒë∆°n gi·∫£n
                        key = f"{item['chapter']}-{item['lesson']}"
                        if key in seen: continue
                        seen.add(key)
                        
                        src = item["source"].replace('.pdf', '')
                        topic = item["chapter"]
                        lesson = item["lesson"]
                        confidence_pct = int(item.get("max_score", 0.9) * 100)
                        
                        st.markdown(f"""
                        <div class="evidence-card">
                            <div class="evidence-header">
                                üìñ {src}
                                <span class="evidence-confidence">ƒê·ªô tin c·∫≠y: {confidence_pct}%</span>
                            </div>
                            <div class="evidence-context">‚ûú {topic} <br>‚ûú {lesson}</div>
                        </div>
                        """, unsafe_allow_html=True)

    # Input User
    if prompt := st.chat_input("H·ªèi g√¨ ƒëi n√†o... (VD: Tin h·ªçc l√† g√¨?)"):
        if "engine_ready" not in st.session_state or not st.session_state.engine_ready:
            st.error("‚ö†Ô∏è Vui l√≤ng n·∫°p t√†i li·ªáu ·ªü menu b√™n tr√°i tr∆∞·ªõc!")
        else:
            # Hi·ªÉn th·ªã c√¢u h·ªèi user
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # AI Tr·∫£ l·ªùi
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("‚è≥ AI ƒëang suy nghƒ© & tra c·ª©u SGK...")
                
                engine = RAGEngine()
                engine.load_vector_store()
                
                response_text, evidence_docs = engine.query(prompt, selected_model)
                
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
                message_placeholder.markdown(response_text)
                
                # Hi·ªÉn th·ªã Evidence
                if evidence_docs:
                    with st.expander("üìö Ki·ªÉm ch·ª©ng ngu·ªìn g·ªëc (Evidence)", expanded=True):
                        seen = set()
                        for item in evidence_docs:
                            key = f"{item['chapter']}-{item['lesson']}"
                            if key in seen: continue
                            seen.add(key)
                            
                            src = item["source"].replace('.pdf', '')
                            topic = item["chapter"]
                            lesson = item["lesson"]
                            
                            st.markdown(f"""
                            <div class="evidence-card">
                                <div class="evidence-header">
                                    üìñ {src}
                                </div>
                                <div class="evidence-context">‚ûú {topic} <br>‚ûú {lesson}</div>
                            </div>
                            """, unsafe_allow_html=True)

            # L∆∞u l·ªãch s·ª≠
            st.session_state.messages.append({
                "role": "assistant", 
                "content": response_text,
                "evidence": evidence_docs
            })

def deduplicate_evidence(evidence_list):
    """H√†m ph·ª• tr·ª£ l·ªçc tr√πng l·∫∑p"""
    unique = []
    seen = set()
    for item in evidence_list:
        key = f"{item['chapter']}_{item['lesson']}"
        if key not in seen:
            seen.add(key)
            unique.append(item)
    return unique

if __name__ == "__main__":
    main()