import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import uuid
import unicodedata 
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator

# =========================================================
# 1. IMPORT & KIá»‚M TRA THÆ¯ VIá»†N (GIá»® NGUYÃŠN)
# =========================================================
try:
    import nest_asyncio
    nest_asyncio.apply() 
    from llama_parse import LlamaParse 
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    
    # Rerank optimization (CÃ³ kiá»ƒm tra lá»—i náº¿u chÆ°a cÃ i)
    try:
        from flashrank import Ranker, RerankRequest
        HAS_FLASHRANK = True
    except ImportError:
        HAS_FLASHRANK = False
        
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# =========================================================
# 2. Cáº¤U HÃŒNH Há»† THá»NG (APP CONFIG)
# =========================================================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Pháº¡m Kiá»‡t",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # --- API KEY (Tháº§y Ä‘iá»n vÃ o Ä‘Ã¢y) ---
    GROQ_API_KEY = "gsk_..." 
    LLAMA_CLOUD_API_KEY = "llx-..." # Key LlamaParse náº¿u dÃ¹ng
    
    # Model Config
    LLM_MODEL = "llama-3.3-70b-versatile" 
    EMBEDDING_MODEL = "dangvantuan/vietnamese-embedding"
    
    # Paths
    UPLOAD_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_DIR = "FAISS_DB"
    BM25_PATH = os.path.join(VECTOR_DB_DIR, "bm25_docs.pkl")
    LOGO_PROJECT = "LOGO.jpg" 
    
    # Prompt
    SYSTEM_PROMPT = """Báº¡n lÃ  Trá»£ lÃ½ há»c táº­p mÃ´n Tin há»c, há»— trá»£ giÃ¡o viÃªn vÃ  há»c sinh trÆ°á»ng Pháº¡m Kiá»‡t theo SGK Káº¿t ná»‘i tri thá»©c (KNTT).
    
    NHIá»†M Vá»¤:
    - Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ngá»¯ cáº£nh (Context) Ä‘Æ°á»£c cung cáº¥p.
    - TUYá»†T Äá»I KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin.
    
    YÃŠU Cáº¦U Äáº¦U RA:
    1. Ná»™i dung: Giáº£i thÃ­ch rÃµ rÃ ng, sÆ° pháº¡m, phÃ¹ há»£p lá»©a tuá»•i há»c sinh.
    2. TrÃ­ch dáº«n nguá»“n Báº®T BUá»˜C: Cuá»‘i cÃ¢u tráº£ lá»i pháº£i ghi rÃµ nguá»“n theo Ä‘á»‹nh dáº¡ng: 
       (Nguá»“n: TÃªn sÃ¡ch > Chá»§ Ä‘á»... > BÃ i...)
    3. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong Context: Tráº£ lá»i "Dá»±a trÃªn tÃ i liá»‡u SGK hiá»‡n cÃ³, tÃ´i chÆ°a tÃ¬m tháº¥y thÃ´ng tin nÃ y."
    """

# =========================================================
# 3. Xá»¬ LÃ Dá»® LIá»†U & RAG (PHáº¦N ÄIá»€U CHá»ˆNH Ká»¸ THUáº¬T)
# =========================================================

class VectorStoreManager:
    def __init__(self):
        # Embeddings cho tiáº¿ng Viá»‡t
        self.embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
        # Splitter cáº¯t nhá» chunk (dÃ¹ng sau khi Ä‘Ã£ parse cáº¥u trÃºc)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            separators=["\n\n", "\n", ".", " ", ""]
        )

    def _normalize_text(self, text: str) -> str:
        """LÃ m sáº¡ch vÄƒn báº£n cÆ¡ báº£n"""
        if not text: return ""
        text = unicodedata.normalize("NFC", text)
        return re.sub(r'\s+', ' ', text).strip()

    def _parse_kntt_logic(self, raw_text: str, filename: str) -> List[Document]:
        """
        LOGIC Má»šI: TÃ¡ch Chá»§ Ä‘á» -> BÃ i.
        Sá»­ dá»¥ng Regex linh hoáº¡t Ä‘á»ƒ báº¯t tiÃªu Ä‘á» trong PDF/Markdown.
        """
        lines = raw_text.split('\n')
        structured_docs = []
        
        # Regex báº¯t tiÃªu Ä‘á» (Cháº¥p nháº­n cáº£ Markdown ##, **, vÃ  chá»¯ thÆ°á»ng/hoa)
        # Báº¯t: "Chá»§ Ä‘á» 1:", "## CHá»¦ Äá»€ A", "Chá»§ Ä‘á» 3. MÃ¡y tÃ­nh"
        topic_pattern = re.compile(r'^[\#\*\s]*(?:Chá»§ Ä‘á»|CHá»¦ Äá»€)\s+([0-9A-Za-z]+)(?:[:\.]|\s+)(.+?)(?:[\#\*]*)$', re.IGNORECASE)
        
        # Báº¯t: "BÃ i 1:", "### BÃ€I 5.", "BÃ i 17:"
        lesson_pattern = re.compile(r'^[\#\*\s]*(?:BÃ i|BÃ€I)\s+([0-9]+)(?:[:\.]|\s+)(.+?)(?:[\#\*]*)$', re.IGNORECASE)

        # Tráº¡ng thÃ¡i
        current_topic = None
        current_lesson = None
        buffer = []
        
        # TÃªn nguá»“n (Bá» Ä‘uÃ´i .pdf)
        source_name = os.path.splitext(filename)[0]

        def commit_buffer():
            """LÆ°u Ä‘oáº¡n vÄƒn hiá»‡n táº¡i náº¿u Ä‘á»§ thÃ´ng tin nguá»“n"""
            if buffer and current_topic and current_lesson:
                content = "\n".join(buffer).strip()
                if len(content) > 20: # Bá» qua Ä‘oáº¡n quÃ¡ ngáº¯n
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": source_name,
                            "topic": current_topic,
                            "lesson": current_lesson,
                            "chunk_uid": str(uuid.uuid4())
                        }
                    )
                    structured_docs.append(doc)

        for line in lines:
            line_clean = self._normalize_text(line)
            if not line_clean: continue

            # 1. PhÃ¡t hiá»‡n Chá»§ Ä‘á»
            topic_match = topic_pattern.match(line_clean)
            if topic_match:
                commit_buffer() # LÆ°u ná»™i dung cÅ©
                t_id = topic_match.group(1).strip()
                t_name = topic_match.group(2).strip()
                current_topic = f"Chá»§ Ä‘á» {t_id}: {t_name}"
                current_lesson = None # Reset bÃ i khi sang chá»§ Ä‘á» má»›i
                buffer = []
                continue

            # 2. PhÃ¡t hiá»‡n BÃ i
            lesson_match = lesson_pattern.match(line_clean)
            if lesson_match:
                commit_buffer()
                l_id = lesson_match.group(1).strip()
                l_name = lesson_match.group(2).strip()
                current_lesson = f"BÃ i {l_id}: {l_name}"
                buffer = []
                continue

            # 3. Thu tháº­p ná»™i dung (CHá»ˆ KHI ÄÃƒ CÃ“ CHá»¦ Äá»€ VÃ€ BÃ€I)
            if current_topic and current_lesson:
                buffer.append(line_clean)
        
        # Commit Ä‘oáº¡n cuá»‘i cÃ¹ng
        commit_buffer()
        return structured_docs

    def build_db(self, uploaded_files):
        """XÃ¢y dá»±ng láº¡i Vector DB tá»« file PDF"""
        if not os.path.exists(AppConfig.UPLOAD_DIR):
            os.makedirs(AppConfig.UPLOAD_DIR)

        all_docs = []
        # Thanh tiáº¿n trÃ¬nh UI
        progress_text = "Äang khá»Ÿi Ä‘á»™ng tiáº¿n trÃ¬nh há»c..."
        my_bar = st.progress(0, text=progress_text)

        for i, uploaded_file in enumerate(uploaded_files):
            # LÆ°u file
            file_path = os.path.join(AppConfig.UPLOAD_DIR, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Cáº­p nháº­t UI
            my_bar.progress((i / len(uploaded_files)), text=f"Äang Ä‘á»c tÃ i liá»‡u: {uploaded_file.name}")
            
            # 1. Parse PDF (LlamaParse -> Markdown)
            try:
                # Set API Key mÃ´i trÆ°á»ng náº¿u cáº§n
                if AppConfig.LLAMA_CLOUD_API_KEY.startswith("llx-"):
                    os.environ["LLAMA_CLOUD_API_KEY"] = AppConfig.LLAMA_CLOUD_API_KEY
                
                parser = LlamaParse(result_type="markdown", language="vi")
                parsed_docs = parser.load_data(file_path)
                
                if parsed_docs:
                    raw_text = parsed_docs[0].text
                    # 2. ÃP Dá»¤NG LOGIC KNTT (FIX Má»šI)
                    kntt_docs = self._parse_kntt_logic(raw_text, uploaded_file.name)
                    
                    if kntt_docs:
                        # 3. Split chunk (Giá»¯ metadata)
                        chunks = self.text_splitter.split_documents(kntt_docs)
                        all_docs.extend(chunks)
                    else:
                        st.warning(f"âš ï¸ File {uploaded_file.name}: KhÃ´ng tÃ¬m tháº¥y cáº¥u trÃºc 'Chá»§ Ä‘á» -> BÃ i'.")
            except Exception as e:
                st.error(f"Lá»—i khi Ä‘á»c file {uploaded_file.name}: {e}")

        my_bar.progress(100, text="Äang mÃ£ hÃ³a dá»¯ liá»‡u vÃ o bá»™ nhá»› AI...")
        
        if not all_docs:
            st.error("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡ Ä‘á»ƒ táº¡o Database.")
            return None

        # 4. LÆ°u FAISS DB
        vector_db = FAISS.from_documents(all_docs, self.embeddings)
        vector_db.save_local(AppConfig.VECTOR_DB_DIR)
        
        # 5. LÆ°u BM25 Cache (Cho Hybrid Search)
        with open(AppConfig.BM25_PATH, "wb") as f:
            pickle.dump(all_docs, f)
            
        my_bar.empty()
        return vector_db

    def load_db(self):
        """Load DB tá»« á»• cá»©ng"""
        if os.path.exists(AppConfig.VECTOR_DB_DIR) and os.path.exists(os.path.join(AppConfig.VECTOR_DB_DIR, "index.faiss")):
            return FAISS.load_local(
                AppConfig.VECTOR_DB_DIR, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
        return None

# =========================================================
# 4. ENGINE TÃŒM KIáº¾M VÃ€ TRáº¢ Lá»œI (RAG ENGINE)
# =========================================================

class RAGEngine:
    @staticmethod
    def get_retriever(vector_db):
        # 1. FAISS Retriever (Semantic)
        faiss_retriever = vector_db.as_retriever(search_kwargs={"k": 5})
        
        # 2. BM25 Retriever (Keyword) - Load tá»« cache pickle
        bm25_retriever = None
        if os.path.exists(AppConfig.BM25_PATH):
            try:
                with open(AppConfig.BM25_PATH, "rb") as f:
                    docs = pickle.load(f)
                bm25_retriever = BM25Retriever.from_documents(docs)
                bm25_retriever.k = 5
            except:
                pass
        
        # 3. Hybrid (Ensemble)
        if bm25_retriever:
            return EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[0.4, 0.6]
            )
        return faiss_retriever

    @staticmethod
    def generate_response(client, retriever, query):
        # BÆ°á»›c 1: Retrieve
        docs = retriever.invoke(query)
        
        # BÆ°á»›c 2: Rerank (Náº¿u cÃ³ thÆ° viá»‡n Flashrank)
        final_docs = docs
        if HAS_FLASHRANK and docs:
            try:
                ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="./opt")
                rerank_request = RerankRequest(query=query, passages=[
                    {"id": d.metadata.get("chunk_uid", "0"), "text": d.page_content, "meta": d.metadata} 
                    for d in docs
                ])
                results = ranker.rank(rerank_request)
                # Chuyá»ƒn Ä‘á»•i láº¡i format
                final_docs = []
                for r in results[:3]: # Láº¥y top 3 tá»‘t nháº¥t
                    final_docs.append(Document(page_content=r['text'], metadata=r['meta']))
            except Exception:
                final_docs = docs[:3] # Fallback
        else:
            final_docs = docs[:3]

        # BÆ°á»›c 3: Táº¡o Context string vá»›i Metadata chuáº©n
        context_text = ""
        for d in final_docs:
            source = d.metadata.get('source', 'N/A')
            topic = d.metadata.get('topic', 'N/A')
            lesson = d.metadata.get('lesson', 'N/A')
            
            context_text += f"\n---\n[NGUá»’N: {source} > {topic} > {lesson}]\nNá»™i dung: {d.page_content}\n"

        # BÆ°á»›c 4: Táº¡o Prompt
        full_prompt = f"""{AppConfig.SYSTEM_PROMPT}
        
        THÃ”NG TIN NGá»® Cáº¢NH (CONTEXT):
        {context_text}
        
        CÃ‚U Há»I Cá»¦A Há»ŒC SINH: {query}
        
        TRáº¢ Lá»œI:"""

        # BÆ°á»›c 5: Gá»i LLM (Stream)
        try:
            chat_completion = client.chat.completions.create(
                messages=[{"role": "user", "content": full_prompt}],
                model=AppConfig.LLM_MODEL,
                stream=True,
            )
            for chunk in chat_completion:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"Lá»—i káº¿t ná»‘i AI: {str(e)}"

# =========================================================
# 5. GIAO DIá»†N CHÃNH (MAIN UI) - GIá»® NGUYÃŠN
# =========================================================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"âŒ Thiáº¿u thÆ° viá»‡n: {IMPORT_ERROR}")
        return

    # --- Sidebar ---
    with st.sidebar:
        # Logo project
        if os.path.exists(AppConfig.LOGO_PROJECT):
            st.image(AppConfig.LOGO_PROJECT, width=120)
        else:
            st.image("https://via.placeholder.com/150", width=100)
            
        st.title("ğŸ—‚ï¸ KHO TRI THá»¨C")
        st.markdown("---")
        
        uploaded_files = st.file_uploader(
            "Náº¡p SGK (PDF)", 
            type=["pdf"], 
            accept_multiple_files=True
        )
        
        if st.button("ğŸ”„ Cáº­p nháº­t Tri thá»©c", use_container_width=True):
            if uploaded_files:
                if not AppConfig.GROQ_API_KEY.startswith("gsk_"):
                     st.error("âš ï¸ Vui lÃ²ng Ä‘iá»n API KEY vÃ o code!")
                else:
                    manager = VectorStoreManager()
                    with st.spinner("Äang cáº¥u trÃºc hÃ³a dá»¯ liá»‡u..."):
                        db = manager.build_db(uploaded_files)
                        if db:
                            st.success("âœ… ÄÃ£ há»c xong!")
                            st.session_state.vector_db = db
                            # XÃ³a cache retriever cÅ©
                            if "retriever_engine" in st.session_state:
                                del st.session_state.retriever_engine
                            st.rerun()
            else:
                st.warning("âš ï¸ Vui lÃ²ng chá»n file PDF!")

        st.markdown("---")
        st.info("Há»‡ thá»‘ng RAG há»— trá»£ tra cá»©u SGK KNTT theo chuáº©n: \nChá»§ Ä‘á» -> BÃ i.")

    # --- Main Chat ---
    st.title("ğŸ¤– TRá»¢ LÃ Há»ŒC Táº¬P TIN Há»ŒC")
    
    # Init Chat History
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Init Retriever Engine (Load DB)
    if "retriever_engine" not in st.session_state:
        manager = VectorStoreManager()
        db = manager.load_db()
        if db:
            st.session_state.retriever_engine = RAGEngine.get_retriever(db)
            st.toast("âœ… Dá»¯ liá»‡u SGK Ä‘Ã£ sáºµn sÃ ng!", icon="ğŸ“š")

    # Display Chat
    for msg in st.session_state.messages:
        bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ğŸ¤–"
        avatar = "ğŸ§‘â€ğŸ“" if msg["role"] == "user" else bot_avatar
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True) 

    # Input Area
    user_input = st.chat_input("Nháº­p cÃ¢u há»i há»c táº­p...")
    
    if user_input:
        # Hiá»ƒn thá»‹ cÃ¢u há»i User
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="ğŸ§‘â€ğŸ“"):
            st.markdown(user_input)

        # Hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i AI
        with st.chat_message("assistant", avatar=AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ğŸ¤–"):
            if "retriever_engine" not in st.session_state:
                 st.warning("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u! Vui lÃ²ng náº¡p SGK á»Ÿ cá»™t trÃ¡i.")
            else:
                response_placeholder = st.empty()
                full_response = ""
                
                # Gá»i Engine
                try:
                    groq_client = Groq(api_key=AppConfig.GROQ_API_KEY)
                    response_gen = RAGEngine.generate_response(
                        groq_client,
                        st.session_state.retriever_engine,
                        user_input
                    )

                    for chunk in response_gen:
                        full_response += chunk
                        response_placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
                    
                    response_placeholder.markdown(full_response, unsafe_allow_html=True)
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    
                except Exception as e:
                    st.error(f"Lá»—i há»‡ thá»‘ng: {e}")

if __name__ == "__main__":
    main()