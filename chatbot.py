import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import uuid
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply() # B·∫Øt bu·ªôc cho LlamaParse ch·∫°y trong Streamlit
    from llama_parse import LlamaParse 
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    # Rerank optimization
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config 
    LLM_MODEL = 'llama-3.1-8b-instant'

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD" 

    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Parameters (Updated for Semantic Chunking logic)
    RETRIEVAL_K = 30       
    FINAL_K = 5            
    
    # Hybrid Search Weights
    BM25_WEIGHT = 0.4      
    FAISS_WEIGHT = 0.6     

    LLM_TEMPERATURE = 0.0  # Temperature = 0 ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh x√°c th·ª±c khoa h·ªçc

# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER ) 
# GI·ªÆ NGUY√äN 100% THEO Y√äU C·∫¶U
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .header-left p {
                color: #e0fbfc; margin: 5px 0 0 0; font-size: 1rem; opacity: 0.9;
            }
            .header-right img {
                border-radius: 50%; border: 3px solid rgba(255,255,255,0.3);
                box-shadow: 0 4px 10px rgba(0,0,0,0.2); width: 100px; height: 100px;
                object-fit: cover;
            }
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
            [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
                background: #e3f2fd; color: #0d47a1;
            }
            [data-testid="stChatMessageContent"]:not(:has(+ [data-testid="stChatMessageAvatar"])) {
                background: white; border: 1px solid #e9ecef;
                border-left: 5px solid #00b4d8;
            }
            div.stButton > button {
                border-radius: 8px; background-color: white; color: #0077b6;
                border: 1px solid #90e0ef; transition: all 0.2s;
            }
            div.stButton > button:hover {
                background-color: #0077b6; color: white;
                border-color: #0077b6; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p T·ªânh</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">NƒÉm h·ªçc:</span>
                        <span style="text-align: right; color: #222;"><b>2025 - 2026</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                st.session_state.pop('retriever_engine', None)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" alt="Logo">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p style="font-size: 1.1rem; margin-top: 5px;">H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)

# =========================================================
# 3. LOGIC BACKEND - VERIFIABLE HYBRID RAG (KHKT QU·ªêC GIA)
# =========================================================

class VerifiableRAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key:
                return None
            return Groq(api_key=api_key)
        except Exception:
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            st.error(f"L·ªói t·∫£i m√¥ h√¨nh nh√∫ng (Embedding): {e}")
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception:
            return None

    @staticmethod
    def _detect_grade_and_topic(filename: str, text: str) -> dict:
        """
        Nh·∫≠n di·ªán kh·ªëi l·ªõp (Curriculum-Aware) v√† ch·ªß ƒë·ªÅ.
        """
        meta = {"grade": "General", "topic": "general"}
        
        # Detect Grade
        fname = filename.lower()
        if "10" in fname: meta["grade"] = "10"
        elif "11" in fname: meta["grade"] = "11"
        elif "12" in fname: meta["grade"] = "12"
        
        # Detect Topic
        tx = text.lower()
        if any(t in tx for t in ["<html", "css", "javascript", "th·∫ª"]): meta["topic"] = "html_web"
        elif any(t in tx for t in ["def ", "import ", "python", "bi·∫øn", "h√†m"]): meta["topic"] = "python"
        elif any(t in tx for t in ["sql", "primary key", "csdl", "b·∫£ng", "truy v·∫•n"]): meta["topic"] = "database"
        
        return meta

    @staticmethod
    def _parse_pdf_with_llama(file_path: str) -> str:
        """
        S·ª≠ d·ª•ng LlamaParse ƒë·ªÉ chuy·ªÉn PDF sang Markdown c·∫•u tr√∫c (Header-aware).
        """
        os.makedirs(AppConfig.PROCESSED_MD_DIR, exist_ok=True)
        file_name = os.path.basename(file_path)
        md_file_path = os.path.join(AppConfig.PROCESSED_MD_DIR, f"{file_name}.md")
        
        if os.path.exists(md_file_path):
            with open(md_file_path, "r", encoding="utf-8") as f:
                return f.read()
        
        llama_api_key = st.secrets.get("LLAMA_CLOUD_API_KEY")
        if not llama_api_key:
            return "ERROR: Missing LLAMA_CLOUD_API_KEY"

        try:
            # Instruction t·ªëi ∆∞u cho SGK Vi·ªát Nam
            parser = LlamaParse(
                api_key=llama_api_key,
                result_type="markdown",
                language="vi",
                verbose=True,
                parsing_instruction="H√£y ph√¢n t√≠ch t√†i li·ªáu SGK Tin h·ªçc. Gi·ªØ nguy√™n c√°c ti√™u ƒë·ªÅ ch∆∞∆°ng, b√†i, m·ª•c b·∫±ng Markdown (#, ##, ###). Gi·ªØ nguy√™n b·∫£ng bi·ªÉu v√† code block."
            )
            documents = parser.load_data(file_path)
            markdown_text = documents[0].text
            
            with open(md_file_path, "w", encoding="utf-8") as f:
                f.write(markdown_text)
            
            return markdown_text
        except Exception as e:
            return f"Error parsing {file_name}: {str(e)}"

    @staticmethod
    def _semantic_chunking(text: str, source_filename: str) -> List[Document]:
        """
        K·ª∏ THU·∫¨T: STRUCTURAL / SEMANTIC CHUNKING
        Thay v√¨ c·∫Øt theo k√Ω t·ª±, h√†m n√†y c·∫Øt theo c·∫•u tr√∫c logic c·ªßa SGK (Ch∆∞∆°ng -> B√†i -> M·ª•c).
        ƒê·∫£m b·∫£o m·ªói chunk l√† m·ªôt ƒë∆°n v·ªã tri th·ª©c ho√†n ch·ªânh.
        """
        chunks = []
        lines = text.split('\n')
        
        current_chapter = "Ch∆∞∆°ng m·ªü ƒë·∫ßu/T·ªïng quan"
        current_lesson = "N·ªôi dung chung"
        current_section = "Chi ti·∫øt"
        buffer_content = []
        
        base_meta = VerifiableRAGEngine._detect_grade_and_topic(source_filename, text)
        base_meta["source"] = source_filename

        def flush_buffer():
            if buffer_content:
                content_str = "\n".join(buffer_content).strip()
                if len(content_str) > 50: # B·ªè qua c√°c ƒëo·∫°n qu√° ng·∫Øn (nhi·ªÖu)
                    # T·∫°o ID ƒë·ªãnh danh duy nh·∫•t cho chunk (Verifiable ID)
                    chunk_uid = uuid.uuid4().hex[:8].upper()
                    
                    meta = base_meta.copy()
                    meta.update({
                        "chapter": current_chapter,
                        "lesson": current_lesson,
                        "section": current_section,
                        "chunk_uid": chunk_uid
                    })
                    
                    chunks.append(Document(page_content=content_str, metadata=meta))
                buffer_content.clear()

        for line in lines:
            # Nh·∫≠n di·ªán Header Markdown t·ª´ LlamaParse
            header_match = re.match(r'^(#{1,3})\s+(.*)', line)
            
            if header_match:
                flush_buffer() # L∆∞u n·ªôi dung c·ªßa m·ª•c tr∆∞·ªõc ƒë√≥
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                
                if level == 1: # Chapter
                    current_chapter = title
                    current_lesson = "T·ªïng quan ch∆∞∆°ng"
                    current_section = "M·ªü ƒë·∫ßu"
                elif level == 2: # Lesson
                    current_lesson = title
                    current_section = "N·ªôi dung b√†i"
                elif level == 3: # Section
                    current_section = title
            else:
                buffer_content.append(line)
        
        flush_buffer() # L∆∞u ƒëo·∫°n cu·ªëi c√πng
        return chunks

    @staticmethod
    def _read_and_process_files(pdf_dir: str) -> List[Document]:
        if not os.path.exists(pdf_dir):
            return []
        
        pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
        all_chunks: List[Document] = []
        status_text = st.empty()

        for file_path in pdf_files:
            source_file = os.path.basename(file_path)
            status_text.text(f"ƒêang ph√¢n t√≠ch ng·ªØ nghƒ©a: {source_file}...")
            
            markdown_content = VerifiableRAGEngine._parse_pdf_with_llama(file_path)
            
            if "ERROR" not in markdown_content:
                # √Åp d·ª•ng Semantic Chunking
                file_chunks = VerifiableRAGEngine._semantic_chunking(markdown_content, source_file)
                all_chunks.extend(file_chunks)
            else:
                # Fallback n·∫øu LlamaParse l·ªói (√≠t d√πng, nh∆∞ng c·∫ßn ƒë·ªÉ an to√†n h·ªá th·ªëng)
                try:
                    from pypdf import PdfReader
                    reader = PdfReader(file_path)
                    text = ""
                    for page in reader.pages:
                        text += page.extract_text() or ""
                    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                    raw_docs = [Document(page_content=text, metadata={"source": source_file})]
                    all_chunks.extend(splitter.split_documents(raw_docs))
                except: pass
                
        status_text.empty()
        return all_chunks

    @staticmethod
    def build_hybrid_retriever(embeddings):
        if not embeddings: return None

        vector_db = None
        # Ki·ªÉm tra DB ƒë√£ t·ªìn t·∫°i ch∆∞a
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            except Exception: pass

        if not vector_db:
            # N·∫øu ch∆∞a, ti·∫øn h√†nh x·ª≠ l√Ω d·ªØ li·ªáu m·ªõi
            chunk_docs = VerifiableRAGEngine._read_and_process_files(AppConfig.PDF_DIR)
            if not chunk_docs:
                st.error(f"Kh√¥ng t√¨m th·∫•y t√†i li·ªáu trong {AppConfig.PDF_DIR}")
                return None
            
            # X√¢y d·ª±ng Index
            vector_db = FAISS.from_documents(chunk_docs, embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)

        try:
            docstore_docs = list(vector_db.docstore._dict.values())
            # BM25 cho t√¨m ki·∫øm t·ª´ kh√≥a ch√≠nh x√°c
            bm25_retriever = BM25Retriever.from_documents(docstore_docs)
            bm25_retriever.k = AppConfig.RETRIEVAL_K

            # FAISS cho t√¨m ki·∫øm ng·ªØ nghƒ©a (Semantic Search)
            faiss_retriever = vector_db.as_retriever(
                search_type="mmr",
                search_kwargs={"k": AppConfig.RETRIEVAL_K, "lambda_mult": 0.5}
            )

            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[AppConfig.BM25_WEIGHT, AppConfig.FAISS_WEIGHT]
            )
            return ensemble_retriever
        except Exception:
            return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})

    @staticmethod
    def _validate_answer_grounding(response_text: str, context_docs: List[Document]) -> bool:
        """
        K·ª∏ THU·∫¨T: POST-GENERATION VALIDATION LAYER
        Ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c·ªßa AI c√≥ th·ª±c s·ª± d·ª±a tr√™n Context hay kh√¥ng.
        NgƒÉn ch·∫∑n ·∫£o gi√°c (Hallucination).
        """
        # 1. Tr√≠ch xu·∫•t t·ª´ kh√≥a ƒë∆°n gi·∫£n t·ª´ context
        context_text = " ".join([d.page_content.lower() for d in context_docs])
        
        # 2. Ki·ªÉm tra ch·ªìng l·∫Øp (Overlap Check) - Simplified for Speed
        # N·∫øu c√¢u tr·∫£ l·ªùi qu√° ng·∫Øn (v√≠ d·ª•: ch√†o h·ªèi), b·ªè qua check
        if len(response_text.split()) < 10:
            return True
            
        # Ki·ªÉm tra n·∫øu AI th·ª´a nh·∫≠n kh√¥ng bi·∫øt
        if "kh√¥ng t√¨m th·∫•y" in response_text.lower() or "kh√¥ng c√≥ th√¥ng tin" in response_text.lower():
            return True

        # T√≠nh t·ª∑ l·ªá xu·∫•t hi·ªán c·ªßa c√°c t·ª´ quan tr·ªçng trong c√¢u tr·∫£ l·ªùi so v·ªõi context
        # ƒê√¢y l√† m·ªôt b·ªô l·ªçc ƒë∆°n gi·∫£n. Trong th·ª±c t·∫ø KHKT c√≥ th·ªÉ d√πng NLI models.
        response_words = set(response_text.lower().split())
        context_words = set(context_text.split())
        
        common = response_words.intersection(context_words)
        
        # Ng∆∞·ª°ng ch·∫•p nh·∫≠n: √çt nh·∫•t 30% t·ª´ v·ª±ng (tr·ª´ stopword) ph·∫£i n·∫±m trong context
        # (·ªû ƒë√¢y c√†i ƒë·∫∑t ƒë∆°n gi·∫£n: n·∫øu overlap > 5 t·ª´ l√† pass ƒë·ªÉ tr√°nh ch·∫∑n qu√° ch·∫∑t)
        if len(common) > 5:
            return True
            
        return False

    @staticmethod
    def generate_response(client, retriever, query) -> Tuple[Generator, List[str]]:
        if not retriever:
            return (x for x in ["H·ªá th·ªëng ƒëang kh·ªüi t·∫°o... vui l√≤ng ch·ªù gi√¢y l√°t."]), []
        
        # 1. Hybrid Retrieval
        initial_docs = retriever.invoke(query)
        
        # 2. Reranking (L·ªçc tinh)
        final_docs = []
        try:
            ranker = VerifiableRAGEngine.load_reranker()
            if ranker and initial_docs:
                passages = [
                    {"id": str(i), "text": d.page_content, "meta": d.metadata} 
                    for i, d in enumerate(initial_docs)
                ]
                rerank_req = RerankRequest(query=query, passages=passages)
                results = ranker.rank(rerank_req)
                
                for res in results[:AppConfig.FINAL_K]:
                    final_docs.append(Document(page_content=res["text"], metadata=res["meta"]))
            else:
                final_docs = initial_docs[:AppConfig.FINAL_K]
        except Exception:
            final_docs = initial_docs[:AppConfig.FINAL_K]

        if not final_docs:
            return (x for x in ["Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong SGK ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."]), []

        # 3. Build Verifiable Context (K√®m ID ƒë·ªÉ tr√≠ch d·∫´n)
        context_parts = []
        source_display = []
        
        for doc in final_docs:
            meta = doc.metadata
            chunk_uid = meta.get('chunk_uid', 'N/A')
            source_name = meta.get('source', 'SGK')
            chapter = meta.get('chapter', '')
            lesson = meta.get('lesson', '')
            
            # Format hi·ªÉn th·ªã ngu·ªìn cho ng∆∞·ªùi d√πng
            source_label = f"{source_name} - {chapter}"
            source_display.append(source_label)
            
            # Format Context cho AI (B·∫Øt bu·ªôc tr√≠ch d·∫´n ID)
            context_parts.append(f"""
--- CHUNK ID: {chunk_uid} ---
Ngu·ªìn: {source_name} > {chapter} > {lesson}
N·ªôi dung: {doc.page_content}
""")
        
        full_context = "\n".join(context_parts)

        # 4. Strict Scientific Prompting
        system_prompt = f"""B·∫°n l√† Tr·ª£ l√Ω AI Gi√°o d·ª•c trong h·ªá th·ªëng RAG (Retrieval-Augmented Generation).
NHI·ªÜM V·ª§: Sinh c√¢u tr·∫£ l·ªùi d·ª±a tr√™n tri th·ª©c SGK ƒë√£ truy xu·∫•t d∆∞·ªõi ƒë√¢y.

QUY T·∫ÆC C·ªêT L√ïI (VERIFIABLE GROUNDING):
1. **D·ª±a tr√™n b·∫±ng ch·ª©ng:** Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong [CONTEXT]. Tuy·ªát ƒë·ªëi kh√¥ng t·ª± b·ªãa ƒë·∫∑t ki·∫øn th·ª©c ngo√†i.
2. **Tr√≠ch d·∫´n b·∫Øt bu·ªôc:** M·ªçi √Ω ch√≠nh ph·∫£i ƒëi k√®m ngu·ªìn g·ªëc. C√∫ ph√°p: `[Ngu·ªìn: ID_C·ª¶A_CHUNK]`.
   - V√≠ d·ª•: "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh b·∫≠c cao [Ngu·ªìn: A1B2C3D4]."
3. **Trung th·ª±c:** N·∫øu [CONTEXT] kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i: "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong SGK hi·ªán c√≥."
4. **Phong c√°ch:** H·ªçc thu·∫≠t, s∆∞ ph·∫°m, khuy·∫øn kh√≠ch t∆∞ duy. Tr√¨nh b√†y Markdown r√µ r√†ng.

[CONTEXT B·∫ÆT ƒê·∫¶U]
{full_context}
[CONTEXT K·∫æT TH√öC]
"""

        try:
            # 5. Generation (Non-stream internal to allow validation, simulated stream output)
            # L∆∞u √Ω: ƒê·ªÉ t·ªëi ∆∞u tr·∫£i nghi·ªám UI stream nh∆∞ng v·∫´n validate, ta s·∫Ω d√πng k·ªπ thu·∫≠t
            # "Speculative Streaming" ho·∫∑c ƒë∆°n gi·∫£n l√† l·∫•y full response r·ªìi stream gi·∫£ l·∫≠p n·∫øu pass validation.
            # ƒê·ªÉ an to√†n cho KHKT, ta l·∫•y full response ƒë·ªÉ validate ch·∫∑t ch·∫Ω.
            
            response = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                stream=False, # T·∫Øt stream ƒë·ªÉ validate to√†n v·∫πn
                temperature=AppConfig.LLM_TEMPERATURE,
                max_tokens=1500
            )
            
            full_response_text = response.choices[0].message.content

            # 6. Post-Generation Validation Check
            is_valid = VerifiableRAGEngine._validate_answer_grounding(full_response_text, final_docs)
            
            if not is_valid:
                # N·∫øu ph√°t hi·ªán ·∫£o gi√°c ho·∫∑c kh√¥ng li√™n quan
                final_response = "H·ªá th·ªëng ph√°t hi·ªán c√¢u tr·∫£ l·ªùi kh√¥ng b√°m s√°t t√†i li·ªáu SGK g·ªëc. Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi c·ª• th·ªÉ h∆°n."
                return (x for x in [final_response]), []
            
            # N·∫øu h·ª£p l·ªá, gi·∫£ l·∫≠p stream tr·∫£ v·ªÅ cho UI
            # T√°ch th√†nh c√°c t·ª´ ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng g√µ
            def simulated_stream():
                words = full_response_text.split(' ') # T√°ch theo kho·∫£ng tr·∫Øng ƒë·ªÉ gi·ªØ format t·ªët h∆°n
                for i, word in enumerate(words):
                    yield word + " " if i < len(words)-1 else word
                    
            return simulated_stream(), list(set(source_display))

        except Exception as e:
            return (x for x in [f"L·ªói h·ªá th·ªëng RAG: {str(e)}"]), []

# ===================
# 4. MAIN APPLICATION
# ===================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! KTC Chatbot s·∫µn s√†ng h·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c."}]

    groq_client = VerifiableRAGEngine.load_groq_client()

    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c s·ªë (LlamaParse + Semantic RAG)..."):
            embeddings = VerifiableRAGEngine.load_embedding_model()
            st.session_state.retriever_engine = VerifiableRAGEngine.build_hybrid_retriever(embeddings)
            if st.session_state.retriever_engine:
                st.toast("‚úÖ D·ªØ li·ªáu SGK ƒë√£ s·∫µn s√†ng!", icon="üìö")

    for msg in st.session_state.messages:
        bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else bot_avatar
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y...")
    
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"):
            response_placeholder = st.empty()
            
            stream_generator, sources = VerifiableRAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                user_input
            )

            full_response = ""
            # X·ª≠ l√Ω generator tr·∫£ v·ªÅ (d√π l√† stream th·∫≠t hay gi·∫£ l·∫≠p)
            for chunk in stream_generator:
                # X·ª≠ l√Ω kh√°c bi·ªát gi·ªØa object chunk c·ªßa OpenAI v√† string thu·∫ßn
                content = chunk if isinstance(chunk, str) else (chunk.choices[0].delta.content or "")
                full_response += content
                response_placeholder.markdown(full_response + "‚ñå")
                
            response_placeholder.markdown(full_response)

            if sources:
                with st.expander("üìö Ngu·ªìn SGK x√°c th·ª±c (Verifiable Source)"):
                    for src in sources:
                        st.markdown(f"- {src}")

            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()