import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import uuid
import unicodedata 
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply() 
    # ∆Øu ti√™n LlamaParse, n·∫øu kh√¥ng c√≥ s·∫Ω d√πng PyPDFLoader l√†m fallback
    try:
        from llama_parse import LlamaParse 
    except ImportError:
        LlamaParse = None
        
    from langchain_community.document_loaders import PyPDFLoader # Fallback loader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    # Rerank optimization
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant'

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD" 

    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Pipeline Parameters (Strict Mode for KHKT)
    BM25_TOP_K = 50        # L·ªçc th√¥: L·∫•y r·ªông ƒë·ªÉ b·∫Øt t·ª´ kh√≥a SGK ch√≠nh x√°c
    SEMANTIC_TOP_K = 10    # L·ªçc tinh: L·∫•y theo ng·ªØ nghƒ©a t·ª´ t·∫≠p th√¥
    FINAL_K = 5            # Output: ƒê∆∞a v√†o LLM l√†m b·∫±ng ch·ª©ng
    
    LLM_TEMPERATURE = 0.0  # Nhi·ªát ƒë·ªô 0 ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n khoa h·ªçc

# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER ) 
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .header-left p {
                color: #e0fbfc; margin: 5px 0 0 0; font-size: 1rem; opacity: 0.9;
            }
            .header-right img {
                border-radius: 50%; border: 3px solid rgba(255,255,255,0.3);
                box-shadow: 0 4px 10px rgba(0,0,0,0.2); width: 100px; height: 100px;
                object-fit: cover;
            }
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
            [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
                background: #e3f2fd; color: #0d47a1;
            }
            [data-testid="stChatMessageContent"]:not(:has(+ [data-testid="stChatMessageAvatar"])) {
                background: white; border: 1px solid #e9ecef;
                border-left: 5px solid #00b4d8;
            }
            
            /* Style cho ph·∫ßn Ngu·ªìn tham kh·∫£o footer */
            .citation-footer {
                margin-top: 15px;
                padding-top: 10px;
                border-top: 1px dashed #ced4da;
                font-size: 0.9rem;
                color: #495057;
            }
            .citation-header {
                font-weight: 700;
                color: #d63384; 
                margin-bottom: 5px;
                display: flex;
                align-items: center;
                gap: 5px;
            }
            .citation-item {
                margin-left: 5px;
                margin-bottom: 3px;
                display: block;
            }
            
            div.stButton > button {
                border-radius: 8px; background-color: white; color: #0077b6;
                border: 1px solid #90e0ef; transition: all 0.2s;
            }
            div.stButton > button:hover {
                background-color: #0077b6; color: white;
                border-color: #0077b6; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p T·ªânh</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">NƒÉm h·ªçc:</span>
                        <span style="text-align: right; color: #222;"><b>2025 - 2026</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                if os.path.exists(AppConfig.PROCESSED_MD_DIR):
                    shutil.rmtree(AppConfig.PROCESSED_MD_DIR)
                st.session_state.pop('retriever_engine', None)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" alt="Logo">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p style="font-size: 1.1rem; margin-top: 5px;">H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)

# ==================================
# 3. LOGIC BACKEND - VERIFIABLE CASCADING RAG
# ==================================

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key:
                return None
            return Groq(api_key=api_key)
        except Exception:
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            st.error(f"L·ªói t·∫£i Embedding: {e}")
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception as e:
            return None

    @staticmethod
    def _detect_grade(filename: str) -> str:
        filename = filename.lower()
        if "10" in filename: return "10"
        if "11" in filename: return "11"
        if "12" in filename: return "12"
        return "THCS"

    # --- [STRICT] CHUNKING: T√ÅCH BI·ªÜT DATA & METADATA ---
    @staticmethod
    def _structural_chunking(text: str, source_meta: dict) -> List[Document]:
        text = unicodedata.normalize('NFC', text)
        text = text.replace('\xa0', ' ').replace('\u200b', '')
        
        lines = text.split('\n')
        chunks = []
        
        # State tracking
        current_topic = "Ki·∫øn th·ª©c chung"
        current_lesson = "T·ªïng quan"
        current_section = "N·ªôi dung"
        
        buffer = []

        # Regex ƒê·∫∂C TH√ô CHO SGK KNTT
        p_topic = re.compile(r'(?:^|[\#\*\s]+)(CH·ª¶\s*ƒê·ªÄ)\s+([0-9A-Z]+)(.*)', re.IGNORECASE)
        p_lesson = re.compile(r'(?:^|[\#\*\s]+)(B√ÄI)\s+([0-9]+)(.*)', re.IGNORECASE)
        
        def commit_chunk(buf, topic, lesson, section):
            content = "\n".join(buf).strip()
            if len(content) < 30: return 
            
            # QUY T·∫ÆC 1: Content ch·ªâ ch·ª©a ki·∫øn th·ª©c thu·∫ßn t√∫y ƒë·ªÉ Embedding kh√¥ng b·ªã nhi·ªÖu
            clean_content = content 
            
            # QUY T·∫ÆC 2: Metadata ch·ª©a ng·ªØ c·∫£nh ƒë·ªÉ tr√≠ch ngu·ªìn
            meta = source_meta.copy()
            meta.update({
                "subject": "Tin h·ªçc",
                "book": "K·∫øt n·ªëi tri th·ª©c",
                "chapter": topic,
                "lesson": lesson,
                "section": section,
                # Context String d√πng cho hi·ªÉn th·ªã n·∫øu c·∫ßn
                "full_source_str": f"SGK Tin h·ªçc {meta.get('grade')} - {topic} - {lesson}"
            })
            
            chunks.append(Document(page_content=clean_content, metadata=meta))

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped: continue
            
            # 4. LOGIC PH√ÅT HI·ªÜN CH·ª¶ ƒê·ªÄ (TOPIC)
            match_topic = p_topic.search(line_stripped)
            if match_topic:
                commit_chunk(buffer, current_topic, current_lesson, current_section)
                buffer = []
                current_topic = f"Ch·ªß ƒë·ªÅ {match_topic.group(2)}: {match_topic.group(3).strip(' :.-')}"
                current_lesson = "Gi·ªõi thi·ªáu ch·ªß ƒë·ªÅ"
                continue
            
            # 5. LOGIC PH√ÅT HI·ªÜN B√ÄI (LESSON)
            match_lesson = p_lesson.search(line_stripped)
            if match_lesson:
                commit_chunk(buffer, current_topic, current_lesson, current_section)
                buffer = []
                current_lesson = f"B√†i {match_lesson.group(2)}: {match_lesson.group(3).strip(' :.-')}"
                current_section = "N·ªôi dung b√†i"
                continue
                
            buffer.append(line)
        
        commit_chunk(buffer, current_topic, current_lesson, current_section)
        return chunks

    @staticmethod
    def _parse_pdf_smart(file_path: str) -> str:
        """
        H√†m ƒë·ªçc PDF th√¥ng minh: Th·ª≠ LlamaParse tr∆∞·ªõc, n·∫øu l·ªói th√¨ d√πng PyPDFLoader (mi·ªÖn ph√≠, offline)
        """
        os.makedirs(AppConfig.PROCESSED_MD_DIR, exist_ok=True)
        file_name = os.path.basename(file_path)
        md_file_path = os.path.join(AppConfig.PROCESSED_MD_DIR, f"{file_name}.md")
        
        # 1. Ki·ªÉm tra Cache
        if os.path.exists(md_file_path):
            with open(md_file_path, "r", encoding="utf-8") as f:
                return f.read()
        
        markdown_text = ""
        
        # 2. Th·ª≠ d√πng LlamaParse (∆Øu ti√™n)
        llama_api_key = st.secrets.get("LLAMA_CLOUD_API_KEY")
        used_llama = False
        
        if llama_api_key and LlamaParse:
            try:
                parser = LlamaParse(
                    api_key=llama_api_key,
                    result_type="markdown",
                    language="vi",
                    verbose=True
                )
                documents = parser.load_data(file_path)
                markdown_text = documents[0].text
                used_llama = True
            except Exception as e:
                print(f"‚ö†Ô∏è LlamaParse failed cho {file_name}: {e}. Chuy·ªÉn sang PyPDFLoader.")
        
        # 3. Fallback: D√πng PyPDFLoader (N·∫øu LlamaParse l·ªói ho·∫∑c kh√¥ng c√≥ key)
        if not used_llama or not markdown_text:
            try:
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                # N·ªëi text c√°c trang l·∫°i
                markdown_text = "\n\n".join([d.page_content for d in docs])
            except Exception as e:
                return f"ERROR reading file {file_name}: {str(e)}"

        # 4. L∆∞u Cache
        if markdown_text:
            with open(md_file_path, "w", encoding="utf-8") as f:
                f.write(markdown_text)
            
        return markdown_text

    @staticmethod
    def _read_and_process_files(pdf_dir: str) -> List[Document]:
        if not os.path.exists(pdf_dir):
            os.makedirs(pdf_dir, exist_ok=True)
            return []
        
        pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
        all_chunks: List[Document] = []
        status_text = st.empty()

        if not pdf_files:
            st.warning(f"‚ö†Ô∏è Th∆∞ m·ª•c {pdf_dir} ƒëang tr·ªëng. Vui l√≤ng b·ªè file PDF SGK v√†o.")
            return []

        for file_path in pdf_files:
            source_file = os.path.basename(file_path)
            status_text.text(f"ƒêang x·ª≠ l√Ω c·∫•u tr√∫c tri th·ª©c: {source_file}...")
            
            content = RAGEngine._parse_pdf_smart(file_path)
            
            if content and not content.startswith("ERROR"):
                 meta = {
                     "source": source_file, 
                     "grade": RAGEngine._detect_grade(source_file)
                 }
                 file_chunks = RAGEngine._structural_chunking(content, meta)
                 if file_chunks:
                    all_chunks.extend(file_chunks)
                 else:
                    print(f"‚ö†Ô∏è File {source_file} ƒë·ªçc ƒë∆∞·ª£c text nh∆∞ng kh√¥ng t·∫°o ƒë∆∞·ª£c chunk n√†o.")
            else:
                st.error(f"L·ªói ƒë·ªçc file {source_file}: {content}")
                
        status_text.empty()
        return all_chunks

    # --- [STRICT] BUILD COMPONENTS: KH√îNG D√ôNG ENSEMBLE ---
    @staticmethod
    def build_pipeline_components(embeddings):
        """
        Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn r·ªùi r·∫°c cho Pipeline th·ªß c√¥ng.
        """
        if not embeddings: return None

        # 1. Load/Create Vector DB
        vector_db = None
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            except Exception: pass

        docs_for_bm25 = []
        if not vector_db:
            chunk_docs = RAGEngine._read_and_process_files(AppConfig.PDF_DIR)
            if not chunk_docs: 
                st.error(f"Kh√¥ng t·∫°o ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ {AppConfig.PDF_DIR}. H√£y ki·ªÉm tra: 1. C√≥ file PDF kh√¥ng? 2. File c√≥ text kh√¥ng?")
                return None
            vector_db = FAISS.from_documents(chunk_docs, embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)
            docs_for_bm25 = chunk_docs
        else:
            docs_for_bm25 = list(vector_db.docstore._dict.values())

        # 2. Build BM25 Retriever (Independent)
        if not docs_for_bm25: return None
        bm25_retriever = BM25Retriever.from_documents(docs_for_bm25)
        bm25_retriever.k = AppConfig.BM25_TOP_K 

        return {
            "vector_db": vector_db,
            "bm25": bm25_retriever
        }

    # --- [STRICT] CASCADING RETRIEVAL & GENERATION ---
    @staticmethod
    def generate_response(client, components, query) -> Generator[str, None, None]:
        if not components:
            yield "H·ªá th·ªëng ƒëang kh·ªüi t·∫°o d·ªØ li·ªáu..."
            return
        
        bm25 = components['bm25']
        vector_db = components['vector_db']
        
        # === B∆Ø·ªöC 1: BM25 KEYWORD FILTER (L·∫•y 50 ·ª©ng vi√™n) ===
        # M·ª•c ti√™u: ƒê·∫£m b·∫£o kh√¥ng b·ªè s√≥t t·ª´ kh√≥a ch√≠nh x√°c (VD: "bi·∫øn c·ª•c b·ªô", "c·∫•u tr√∫c r·∫Ω nh√°nh")
        try:
            initial_candidates = bm25.invoke(query)
        except Exception:
            yield "L·ªói truy v·∫•n d·ªØ li·ªáu."
            return

        if not initial_candidates:
            yield "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong SGK."
            return

        # === B∆Ø·ªöC 2: FAISS SEMANTIC SCORING (Tr√™n t·∫≠p 50 ·ª©ng vi√™n) ===
        # M·ª•c ti√™u: L·ªçc t·ª´ 50 xu·ªëng 10 d·ª±a tr√™n ƒë·ªô hi·ªÉu ng·ªØ c·∫£nh c·ªßa c√¢u h·ªèi
        final_docs = []
        try:
            embeddings = RAGEngine.load_embedding_model()
            # T·∫°o vector store t·∫°m th·ªùi c·ª±c nhanh t·ª´ 50 k·∫øt qu·∫£ BM25
            temp_db = FAISS.from_documents(initial_candidates, embeddings)
            # Semantic Search tr√™n t·∫≠p nh·ªè n√†y
            semantic_docs = temp_db.similarity_search(query, k=AppConfig.SEMANTIC_TOP_K)
            
            # === B∆Ø·ªöC 3: RERANKER (FlashRank) ===
            # M·ª•c ti√™u: S·∫Øp x·∫øp l·∫°i top 10 ƒë·ªÉ ch·ªçn ra Top 5 chu·∫©n x√°c nh·∫•t
            ranker = RAGEngine.load_reranker()
            if ranker:
                passages = [
                    {"id": str(i), "text": d.page_content, "meta": d.metadata} 
                    for i, d in enumerate(semantic_docs)
                ]
                rerank_req = RerankRequest(query=query, passages=passages)
                results = ranker.rank(rerank_req)
                for res in results[:AppConfig.FINAL_K]:
                    final_docs.append(Document(page_content=res["text"], metadata=res["meta"]))
            else:
                final_docs = semantic_docs[:AppConfig.FINAL_K]
                
        except Exception as e:
            # Fallback n·∫øu l·ªói Embedding/Rerank th√¨ d√πng k·∫øt qu·∫£ BM25
            print(f"L·ªói Pipeline Semantics: {e}")
            final_docs = initial_candidates[:AppConfig.FINAL_K]

        # === B∆Ø·ªöC 4: CONTEXT CONSTRUCTION ===
        context_text = ""
        used_sources = []
        
        for i, doc in enumerate(final_docs):
            # Ch·ªâ l·∫•y n·ªôi dung s·∫°ch, kh√¥ng tr·ªôn metadata v√†o context LLM ƒë·ªÉ tr√°nh nhi·ªÖu
            context_text += f"\n[ƒêo·∫°n {i+1}]: {doc.page_content}\n"
            used_sources.append(doc.metadata)

        # === B∆Ø·ªöC 5: STRICT PROMPT ===
        system_prompt = f"""B·∫°n l√† Tr·ª£ l√Ω ·∫£o Tin h·ªçc KTC.
Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n [CONTEXT] b√™n d∆∞·ªõi.

Y√äU C·∫¶U NGHI√äM NG·∫∂T:
1. N·ªôi dung ph·∫£i l·∫•y T·ª™NG CH·ªÆ t·ª´ [CONTEXT]. Kh√¥ng t·ª± b·ªãa ki·∫øn th·ª©c.
2. N·∫øu [CONTEXT] kh√¥ng c√≥ th√¥ng tin, tr·∫£ l·ªùi "SGK hi·ªán t·∫°i ch∆∞a c·∫≠p nh·∫≠t th√¥ng tin n√†y".
3. Tr·∫£ l·ªùi vƒÉn phong s∆∞ ph·∫°m, h∆∞·ªõng d·∫´n h·ªçc sinh.
4. Tr√¨nh b√†y r√µ r√†ng, d√πng g·∫°ch ƒë·∫ßu d√≤ng n·∫øu li·ªát k√™.

[CONTEXT]
{context_text}
"""

        try:
            completion = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                stream=False,
                temperature=AppConfig.LLM_TEMPERATURE
            )
            response_content = completion.choices[0].message.content

            # === B∆Ø·ªöC 6: CITATION (Ngu·ªìn tham kh·∫£o chu·∫©n SGK) ===
            citation_html = "\n\n<div class='citation-footer'><div class='citation-header'>üìö CƒÉn c·ª© SGK Tin h·ªçc (K·∫øt n·ªëi tri th·ª©c):</div>"
            
            seen_citations = set()
            has_citation = False
            
            for meta in used_sources:
                # Format: SGK Tin h·ªçc 10 - Ch·ªß ƒë·ªÅ 1 - B√†i 2
                grade = meta.get('grade', '')
                topic = meta.get('chapter', 'Ch∆∞∆°ng ?')
                lesson = meta.get('lesson', 'B√†i ?')
                
                # T·∫°o chu·ªói citation duy nh·∫•t
                cite_str = f"L·ªõp {grade} ‚ûú {topic} ‚ûú {lesson}"
                
                if cite_str not in seen_citations:
                    citation_html += f"<span class='citation-item'>‚Ä¢ {cite_str}</span>"
                    seen_citations.add(cite_str)
                    has_citation = True
            
            citation_html += "</div>"
            
            final_output = response_content + (citation_html if has_citation else "")
            yield final_output

        except Exception as e:
            yield f"L·ªói sinh c√¢u tr·∫£ l·ªùi: {str(e)}"

# ===================
# 4. MAIN APPLICATION
# ===================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! KTC Chatbot s·∫µn s√†ng h·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c SGK Tin h·ªçc."}]

    groq_client = RAGEngine.load_groq_client()

    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c s·ªë (Cascading Filter RAG)..."):
            embeddings = RAGEngine.load_embedding_model()
            # S·ª¨ D·ª§NG H√ÄM M·ªöI: build_pipeline_components
            st.session_state.retriever_engine = RAGEngine.build_pipeline_components(embeddings)
            
            if st.session_state.retriever_engine:
                st.toast("‚úÖ D·ªØ li·ªáu SGK ƒë√£ s·∫µn s√†ng!", icon="üìö")

    for msg in st.session_state.messages:
        bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else bot_avatar
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True) 

    user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi h·ªçc t·∫≠p...")
    
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"):
            response_placeholder = st.empty()
            
            # S·ª¨ D·ª§NG H√ÄM M·ªöI: generate_response v·ªõi tham s·ªë components
            response_gen = RAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                user_input
            )

            full_response = ""
            for chunk in response_gen:
                full_response += chunk
                response_placeholder.markdown(full_response + "‚ñå", unsafe_allow_html=True)
            
            response_placeholder.markdown(full_response, unsafe_allow_html=True)

            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()