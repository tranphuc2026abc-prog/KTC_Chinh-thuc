import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import uuid
import unicodedata 
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator
from collections import defaultdict

# --- [NEW] IMPORT MODULE X·ª¨ L√ù N√ÇNG CAO CHO KHKT ---
# ƒê√¢y l√† d√≤ng k·∫øt n·ªëi v·ªõi file advanced_pdf_processor.py th·∫ßy v·ª´a t·∫°o
try:
    from advanced_pdf_processor import process_pdf_advanced
    ADVANCED_MODE = True
except ImportError:
    ADVANCED_MODE = False
    st.error("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file 'advanced_pdf_processor.py'. H√£y ƒë·∫£m b·∫£o file n√†y n·∫±m c√πng th∆∞ m·ª•c.")

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply() 
    try:
        from llama_parse import LlamaParse 
    except ImportError:
        LlamaParse = None
        
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================================================================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    MODELS = {
        "Llama 3 70B": "llama3-70b-8192",
        "Mixtral 8x7B": "mixtral-8x7b-32768",
        "Gemma 7B": "gemma-7b-it"
    }
    
    # Vector DB Config
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    # Paths
    VECTOR_DB_PATH = "faiss_index"
    UPLOAD_DIR = "uploaded_docs"

# ==============================================================================
# 2. RAG ENGINE (CORE LOGIC)
# ==============================================================================

class RAGEngine:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
        self.vector_store = None
        self.ensure_directories()
        
    def ensure_directories(self):
        os.makedirs(AppConfig.UPLOAD_DIR, exist_ok=True)
        
    def get_groq_client(self):
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            if "GROQ_API_KEY" in st.session_state:
                api_key = st.session_state.GROQ_API_KEY
            else:
                return None
        return Groq(api_key=api_key)

    def _read_and_process_files(self, files) -> List[Document]:
        """
        ƒê·ªçc v√† x·ª≠ l√Ω file upload.
        ƒê√£ n√¢ng c·∫•p ƒë·ªÉ s·ª≠ d·ª•ng 'advanced_pdf_processor' cho file PDF.
        """
        documents = []
        progress_text = "ƒêang ph√¢n t√≠ch c·∫•u tr√∫c t√†i li·ªáu..."
        my_bar = st.progress(0, text=progress_text)
        
        for idx, file in enumerate(files):
            temp_path = os.path.join(AppConfig.UPLOAD_DIR, file.name)
            
            # L∆∞u file t·∫°m
            with open(temp_path, "wb") as f:
                f.write(file.getbuffer())
            
            try:
                # --- [KHKT UPGRADE] X·ª¨ L√ù PDF TH√îNG MINH ---
                if file.name.endswith('.pdf') and ADVANCED_MODE:
                    st.toast(f"üöÄ ƒêang k√≠ch ho·∫°t ch·∫ø ƒë·ªô ƒë·ªçc hi·ªÉu c·∫•u tr√∫c cho: {file.name}")
                    # G·ªçi h√†m t·ª´ file advanced_pdf_processor.py
                    # H√†m n√†y tr·∫£ v·ªÅ list Document ƒë√£ c√≥ s·∫µn Metadata (Chapter/Lesson)
                    file_docs = process_pdf_advanced(temp_path)
                    
                    if file_docs:
                        documents.extend(file_docs)
                        st.info(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t {len(file_docs)} ph√¢n ƒëo·∫°n ki·∫øn th·ª©c t·ª´ {file.name}")
                    else:
                        st.warning(f"File {file.name} kh√¥ng c√≥ n·ªôi dung text ho·∫∑c b·ªã m√£ h√≥a.")
                
                # --- X·ª¨ L√ù C√ÅC LO·∫†I FILE KH√ÅC (C≈®) ---
                else:
                    # Fallback cho file kh√¥ng ph·∫£i PDF ho·∫∑c n·∫øu ch∆∞a c√≥ module n√¢ng cao
                    loader = PyPDFLoader(temp_path)
                    raw_docs = loader.load()
                    
                    # C·∫Øt nh·ªè vƒÉn b·∫£n (Chunking truy·ªÅn th·ªëng)
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=AppConfig.CHUNK_SIZE,
                        chunk_overlap=AppConfig.CHUNK_OVERLAP
                    )
                    chunks = splitter.split_documents(raw_docs)
                    
                    # B·ªï sung metadata c∆° b·∫£n ƒë·ªÉ tr√°nh l·ªói UI
                    for doc in chunks:
                        if "chapter" not in doc.metadata:
                            doc.metadata["chapter"] = "T√†i li·ªáu b·ªï sung"
                        if "lesson" not in doc.metadata:
                            doc.metadata["lesson"] = "N·ªôi dung chi ti·∫øt"
                            
                    documents.extend(chunks)
                    
            except Exception as e:
                st.error(f"L·ªói khi x·ª≠ l√Ω {file.name}: {str(e)}")
            finally:
                # D·ªçn d·∫πp file t·∫°m (T√πy ch·ªçn: c√≥ th·ªÉ gi·ªØ l·∫°i n·∫øu c·∫ßn debug)
                if os.path.exists(temp_path):
                    os.remove(temp_path)
            
            my_bar.progress((idx + 1) / len(files), text=progress_text)
            
        my_bar.empty()
        return documents

    def build_vector_store(self, uploaded_files):
        """X√¢y d·ª±ng vector store t·ª´ file upload"""
        if not uploaded_files:
            return False

        with st.spinner("üîÑ ƒêang c·∫•u tr√∫c h√≥a d·ªØ li·ªáu (Semantic Segmentation)..."):
            # 1. X·ª≠ l√Ω file
            docs = self._read_and_process_files(uploaded_files)
            
            if not docs:
                st.error("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c d·ªØ li·ªáu kh·∫£ d·ª•ng.")
                return False
            
            # 2. T·∫°o Vector Store
            try:
                self.vector_store = FAISS.from_documents(docs, self.embeddings)
                self.vector_store.save_local(AppConfig.VECTOR_DB_PATH)
                st.success(f"‚úÖ ƒê√£ n·∫°p th√†nh c√¥ng {len(docs)} ph√¢n ƒëo·∫°n ki·∫øn th·ª©c!")
                return True
            except Exception as e:
                st.error(f"L·ªói kh·ªüi t·∫°o Vector Store: {str(e)}")
                return False

    def load_vector_store(self):
        """Load vector store ƒë√£ l∆∞u"""
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                self.vector_store = FAISS.load_local(
                    AppConfig.VECTOR_DB_PATH, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                return True
            except Exception:
                return False
        return False

    def query(self, user_question: str, model_name: str, k: int = 4):
        """Truy v·∫•n v√† tr·∫£ l·ªùi"""
        client = self.get_groq_client()
        if not client or not self.vector_store:
            return "Vui l√≤ng nh·∫≠p API Key v√† n·∫°p d·ªØ li·ªáu.", []

        # 1. Retrieve
        retriever = self.vector_store.as_retriever(search_kwargs={"k": k*2})
        docs = retriever.invoke(user_question)
        
        # 2. Rerank (Simple slice for speed)
        final_docs = docs[:k]

        # 3. Context Construction
        context_parts = []
        evidence_list = []
        
        for doc in final_docs:
            # L·∫•y metadata (Code m·ªõi ƒë·∫£m b·∫£o c√°c tr∆∞·ªùng n√†y lu√¥n c√≥ d·ªØ li·ªáu)
            chapter = doc.metadata.get("chapter", "Ch∆∞∆°ng ch∆∞a x√°c ƒë·ªãnh")
            lesson = doc.metadata.get("lesson", "B√†i ch∆∞a x√°c ƒë·ªãnh")
            page = doc.metadata.get("page", "?")
            source = doc.metadata.get("source", "T√†i li·ªáu")
            
            context_parts.append(f"""
            [Ngu·ªìn: {source} | Trang: {page}]
            [V·ªã tr√≠: {chapter} > {lesson}]
            N·ªôi dung: {doc.page_content}
            """)
            
            evidence_list.append({
                "source": source,
                "chapter": chapter,
                "lesson": lesson,
                "page": page,
                "content": doc.page_content,
                "max_score": 0.9, # Score gi·∫£ l·∫≠p cho UI
                "count": 1
            })

        context_str = "\n---\n".join(context_parts)
        
        # 4. Generate Answer
        system_prompt = f"""B·∫°n l√† Tr·ª£ l√Ω AI gi√°o d·ª•c c·ªßa tr∆∞·ªùng THCS & THPT Ph·∫°m Ki·ªát.
        Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
        
        Y√äU C·∫¶U:
        1. Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn, s∆∞ ph·∫°m.
        2. B·∫ÆT BU·ªòC tr√≠ch d·∫´n ngu·ªìn (B√†i n√†o, trang n√†o) n·∫øu th√¥ng tin c√≥ trong ng·ªØ c·∫£nh.
        3. N·∫øu kh√¥ng c√≥ th√¥ng tin trong ng·ªØ c·∫£nh, h√£y n√≥i "Xin l·ªói, t√†i li·ªáu hi·ªán t·∫°i ch∆∞a ƒë·ªÅ c·∫≠p v·∫•n ƒë·ªÅ n√†y."
        
        NG·ªÆ C·∫¢NH H·ªåC LI·ªÜU:
        {context_str}
        """

        try:
            chat_completion = client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_question}
                ],
                model=AppConfig.MODELS.get(model_name, "llama3-70b-8192"),
                temperature=0.3,
                max_tokens=2048,
            )
            return chat_completion.choices[0].message.content, evidence_list
        except Exception as e:
            return f"L·ªói khi g·ªçi API: {str(e)}", []

# ==============================================================================
# 3. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (STREAMLIT UI)
# ==============================================================================

def main():
    # --- CSS T√πy ch·ªânh ---
    st.markdown("""
    <style>
    .evidence-card {
        background-color: #f0f2f6;
        border-left: 5px solid #4CAF50;
        padding: 10px;
        margin-bottom: 10px;
        border-radius: 5px;
    }
    .evidence-header {
        font-weight: bold;
        color: #1E88E5;
        display: flex;
        justify-content: space-between;
    }
    .evidence-context {
        font-size: 0.9em;
        color: #666;
        margin-top: 5px;
        font-style: italic;
    }
    .evidence-confidence {
        font-size: 0.8em;
        background: #e3f2fd;
        padding: 2px 6px;
        border-radius: 10px;
        color: #1565c0;
    }
    .stChatMessage {
        background-color: transparent !important;
    }
    </style>
    """, unsafe_allow_html=True)

    # --- Header ---
    col1, col2 = st.columns([1, 5])
    with col1:
        st.markdown("ü§ñ **KTC-Bot**") 
    with col2:
        st.title("Tr·ª£ l√Ω H·ªçc t·∫≠p Th√¥ng minh - Ph·∫°m Ki·ªát School")
        st.caption("üöÄ Phi√™n b·∫£n KHKT Qu·ªëc gia: T√≠ch h·ª£p Context-Aware RAG Engine")

    # --- Sidebar ---
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        # API Key
        api_key = st.text_input("Groq API Key", type="password", placeholder="gsk_...")
        if api_key:
            st.session_state.GROQ_API_KEY = api_key
            
        # Model Selection
        selected_model = st.selectbox("M√¥ h√¨nh AI", list(AppConfig.MODELS.keys()))
        
        st.divider()
        
        # File Uploader
        st.subheader("üìö N·∫°p T√†i Li·ªáu (SGK, B√†i gi·∫£ng)")
        uploaded_files = st.file_uploader(
            "Ch·ªçn file PDF (Tin 10_KNTT.pdf)", 
            type=['pdf'], 
            accept_multiple_files=True
        )
        
        if st.button("üöÄ Kh·ªüi t·∫°o H·ªá th·ªëng Tri th·ª©c", type="primary"):
            if not uploaded_files:
                st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 file!")
            elif not api_key and "GROQ_API_KEY" not in st.session_state:
                st.warning("Vui l√≤ng nh·∫≠p API Key!")
            else:
                engine = RAGEngine()
                if engine.build_vector_store(uploaded_files):
                    st.session_state.engine_ready = True
                    st.rerun()

        st.divider()
        st.info("üí° M·∫πo: H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c 'Ch·ªß ƒë·ªÅ' v√† 'B√†i h·ªçc' trong SGK.")

    # --- Main Chat Area ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            # Render Evidence n·∫øu c√≥
            if "evidence" in msg and msg["evidence"]:
                # Deduplicate evidence for display
                seen = set()
                unique_evidence = []
                for item in msg["evidence"]:
                    key = f"{item['chapter']}-{item['lesson']}"
                    if key not in seen:
                        seen.add(key)
                        unique_evidence.append(item)
                
                with st.expander("üìö Ki·ªÉm ch·ª©ng ngu·ªìn g·ªëc (Evidence)", expanded=False):
                    for item in unique_evidence:
                        src = item["source"].replace('.pdf', '')
                        topic = item["chapter"]
                        lesson = item["lesson"]
                        confidence_pct = int(item.get("max_score", 0.9) * 100)
                        
                        st.markdown(f"""
                        <div class="evidence-card">
                            <div class="evidence-header">
                                üìñ {src}
                                <span class="evidence-confidence">ƒê·ªô tin c·∫≠y: {confidence_pct}%</span>
                            </div>
                            <div class="evidence-context">‚ûú {topic} <br>‚ûú {lesson}</div>
                        </div>
                        """, unsafe_allow_html=True)

    # Input User
    if prompt := st.chat_input("H·ªèi g√¨ ƒëi n√†o... (VD: Tin h·ªçc l√† g√¨?)"):
        if "engine_ready" not in st.session_state or not st.session_state.engine_ready:
            st.error("‚ö†Ô∏è Vui l√≤ng n·∫°p t√†i li·ªáu ·ªü menu b√™n tr√°i tr∆∞·ªõc!")
        else:
            # Hi·ªÉn th·ªã c√¢u h·ªèi user
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # AI Tr·∫£ l·ªùi
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("‚è≥ AI ƒëang suy nghƒ© & tra c·ª©u SGK...")
                
                engine = RAGEngine()
                engine.load_vector_store()
                
                response_text, evidence_docs = engine.query(prompt, selected_model)
                
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
                message_placeholder.markdown(response_text)
                
                # Hi·ªÉn th·ªã Evidence
                if evidence_docs:
                    seen = set()
                    unique_evidence = []
                    for item in evidence_docs:
                        key = f"{item['chapter']}-{item['lesson']}"
                        if key not in seen:
                            seen.add(key)
                            unique_evidence.append(item)

                    with st.expander("üìö Ki·ªÉm ch·ª©ng ngu·ªìn g·ªëc (Evidence)", expanded=True):
                        for item in unique_evidence:
                            src = item["source"].replace('.pdf', '')
                            topic = item["chapter"]
                            lesson = item["lesson"]
                            
                            st.markdown(f"""
                            <div class="evidence-card">
                                <div class="evidence-header">
                                    üìñ {src}
                                </div>
                                <div class="evidence-context">‚ûú {topic} <br>‚ûú {lesson}</div>
                            </div>
                            """, unsafe_allow_html=True)

            # L∆∞u l·ªãch s·ª≠
            st.session_state.messages.append({
                "role": "assistant", 
                "content": response_text,
                "evidence": evidence_docs
            })

if __name__ == "__main__":
    main()