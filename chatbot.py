import os
import glob
import base64
import streamlit as st
import shutil
import pickle
import re
import uuid
import unicodedata 
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Generator

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói & Th∆∞ vi·ªán RAG ---
try:
    import nest_asyncio
    nest_asyncio.apply() 
    
    # Loaders & Splitters
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document
    
    # Vector Store & Retrievers
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain_huggingface import HuggingFaceEmbeddings
    
    # LLM & Core
    from groq import Groq
    
    # Rerank optimization (Quan tr·ªçng cho KHKT)
    from flashrank import Ranker, RerankRequest
    
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="ü§ñ", # Thay icon n·∫øu kh√¥ng c√≥ file ·∫£nh
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant' # T·ªëc ƒë·ªô cao, context d√†i
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" # H·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët
    
    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    BM25_PATH = "bm25_retriever.pkl" # L∆∞u cache BM25
    
    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    
    # RAG Parameters (Chu·∫©n tinh ch·ªânh)
    BM25_K = 40        # L·∫•y r·ªông theo t·ª´ kh√≥a
    FAISS_K = 10       # L·∫•y s√¢u theo ng·ªØ nghƒ©a
    RERANK_TOP_K = 5   # L·ªçc l·∫°i tinh hoa nh·∫•t ƒë·ªÉ ƒë∆∞a v√†o LLM
    
    LLM_TEMPERATURE = 0.3 # Gi·ªØ ƒë·ªô s√°ng t·∫°o th·∫•p ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c (Academic)

# ===============================
# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU & RAG CORE (RE-ENGINEERED)
# ===============================

class SGKProcessor:
    """
    B·ªô x·ª≠ l√Ω vƒÉn b·∫£n chuy√™n d·ª•ng cho SGK Vi·ªát Nam.
    T·ª± ƒë·ªông ph√°t hi·ªán Kh·ªëi l·ªõp, Ch·ªß ƒë·ªÅ, B√†i h·ªçc ƒë·ªÉ g·∫Øn Metadata.
    """
    @staticmethod
    def normalize_text(text: str) -> str:
        """Chu·∫©n h√≥a Unicode v√† x√≥a k√Ω t·ª± r√°c."""
        text = unicodedata.normalize("NFC", text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    @staticmethod
    def extract_grade_from_filename(filename: str) -> str:
        """L·∫•y kh·ªëi l·ªõp t·ª´ t√™n file (VD: Tin 10_KNTT.pdf -> 10)"""
        match = re.search(r'(?:Tin|L·ªõp)\s*(\d+)', filename, re.IGNORECASE)
        return match.group(1) if match else "THPT"

    @staticmethod
    def parse_sgk_structure(file_path: str) -> List[Document]:
        """
        ƒê·ªçc PDF v√† ph√¢n t√≠ch c·∫•u tr√∫c SGK:
        - Ph√°t hi·ªán 'CH·ª¶ ƒê·ªÄ ...'
        - Ph√°t hi·ªán 'B√ÄI ...'
        G√°n metadata ng·ªØ c·∫£nh cho t·ª´ng trang/ƒëo·∫°n.
        """
        loader = PyPDFLoader(file_path)
        raw_docs = loader.load()
        
        filename = os.path.basename(file_path)
        grade = SGKProcessor.extract_grade_from_filename(filename)
        
        refined_docs = []
        current_topic = "Ch∆∞a ph√¢n lo·∫°i"
        current_lesson = "Gi·ªõi thi·ªáu chung"
        
        # Regex patterns cho SGK KNTT
        topic_pattern = re.compile(r'(?:CH·ª¶ ƒê·ªÄ|Ch·ªß ƒë·ªÅ)\s+([0-9A-Z]+)[:\.]?(.*)', re.IGNORECASE)
        lesson_pattern = re.compile(r'(?:B√ÄI|B√†i)\s+(\d+)[:\.]?(.*)', re.IGNORECASE)

        for doc in raw_docs:
            text = SGKProcessor.normalize_text(doc.page_content)
            
            # Qu√©t ƒë·∫ßu trang ho·∫∑c n·ªôi dung ƒë·ªÉ t√¨m ti√™u ƒë·ªÅ
            # L∆∞u √Ω: Logic n√†y ƒë∆°n gi·∫£n h√≥a, th·ª±c t·∫ø c√≥ th·ªÉ c·∫ßn qu√©t t·ª´ng d√≤ng
            topic_match = topic_pattern.search(text[:200]) # T√¨m trong 200 k√Ω t·ª± ƒë·∫ßu
            if topic_match:
                current_topic = f"Ch·ªß ƒë·ªÅ {topic_match.group(1)}: {topic_match.group(2).strip()}"
            
            lesson_match = lesson_pattern.search(text[:200])
            if lesson_match:
                current_lesson = f"B√†i {lesson_match.group(1)}: {lesson_match.group(2).strip()}"
                
            # C·∫≠p nh·∫≠t metadata
            doc.metadata.update({
                "source": filename,
                "grade": grade,
                "topic": current_topic,
                "lesson": current_lesson,
                "page": doc.metadata.get("page", 0) + 1 # Page trong PDF b·∫Øt ƒë·∫ßu t·ª´ 0
            })
            doc.page_content = text # C·∫≠p nh·∫≠t text ƒë√£ l√†m s·∫°ch
            refined_docs.append(doc)
            
        return refined_docs

class VectorStoreManager:
    """Qu·∫£n l√Ω Vector Database v√† Hybrid Retrieval"""
    
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)

    def create_or_load_vector_store(self):
        """Pipeline Ingestion D·ªØ li·ªáu"""
        if os.path.exists(AppConfig.VECTOR_DB_PATH) and os.path.exists(AppConfig.BM25_PATH):
            # Load FAISS
            vector_db = FAISS.load_local(
                AppConfig.VECTOR_DB_PATH, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
            # Load BM25
            with open(AppConfig.BM25_PATH, "rb") as f:
                bm25_retriever = pickle.load(f)
            return vector_db, bm25_retriever

        # N·∫øu ch∆∞a c√≥ DB, th·ª±c hi·ªán Ingestion m·ªõi
        if not os.path.exists(AppConfig.PDF_DIR):
            os.makedirs(AppConfig.PDF_DIR)
            return None, None

        pdf_files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
        if not pdf_files:
            return None, None

        all_docs = []
        progress_text = "ƒêang s·ªë h√≥a v√† ph√¢n t√≠ch c·∫•u tr√∫c SGK..."
        my_bar = st.progress(0, text=progress_text)
        
        for i, pdf_path in enumerate(pdf_files):
            # B∆∞·ªõc 1: Parse c·∫•u tr√∫c SGK
            structured_docs = SGKProcessor.parse_sgk_structure(pdf_path)
            
            # B∆∞·ªõc 2: Semantic Chunking (chia nh·ªè nh∆∞ng gi·ªØ ng·ªØ c·∫£nh)
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=800, # K√≠ch th∆∞·ªõc v·ª´a ƒë·ªß cho 1 ƒë∆°n v·ªã ki·∫øn th·ª©c
                chunk_overlap=150, # Overlap ƒë·ªÉ gi·ªØ li√™n k·∫øt c√¢u
                separators=["\n\n", "\n", ". ", " ", ""]
            )
            chunks = text_splitter.split_documents(structured_docs)
            all_docs.extend(chunks)
            my_bar.progress((i + 1) / len(pdf_files), text=f"ƒêang x·ª≠ l√Ω: {os.path.basename(pdf_path)}")

        my_bar.empty()

        # B∆∞·ªõc 3: T·∫°o Vector Store (FAISS)
        vector_db = FAISS.from_documents(all_docs, self.embeddings)
        vector_db.save_local(AppConfig.VECTOR_DB_PATH)
        
        # B∆∞·ªõc 4: T·∫°o Keyword Retriever (BM25)
        bm25_retriever = BM25Retriever.from_documents(all_docs)
        bm25_retriever.k = AppConfig.BM25_K
        with open(AppConfig.BM25_PATH, "wb") as f:
            pickle.dump(bm25_retriever, f)
            
        return vector_db, bm25_retriever

class RAGEngine:
    """Core RAG Logic: Hybrid Search + Rerank + Citation"""
    
    @staticmethod
    def generate_response(client, vector_db, bm25_retriever, user_query):
        if not vector_db or not bm25_retriever:
            yield "H·ªá th·ªëng ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng t·∫£i l√™n t√†i li·ªáu SGK."
            return

        # --- 1. RETRIEVAL (HYBRID) ---
        # L·∫•y candidates t·ª´ BM25 (Keyword)
        docs_bm25 = bm25_retriever.invoke(user_query)
        
        # L·∫•y candidates t·ª´ FAISS (Semantic)
        retriever_faiss = vector_db.as_retriever(search_kwargs={"k": AppConfig.FAISS_K})
        docs_faiss = retriever_faiss.invoke(user_query)
        
        # G·ªôp v√† kh·ª≠ tr√πng l·∫∑p (Deduplication)
        all_candidates = {}
        for doc in docs_bm25 + docs_faiss:
            # D√πng content l√†m key ƒë·ªÉ l·ªçc tr√πng
            all_candidates[doc.page_content] = doc
        
        unique_docs = list(all_candidates.values())
        
        # --- 2. RERANKING (FlashRank) ---
        # S·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£ ƒë·ªÉ ch·ªçn ra nh·ªØng ƒëo·∫°n ph√π h·ª£p nh·∫•t
        ranker = Ranker()
        rerank_request = RerankRequest(query=user_query, passages=[
            {"id": str(i), "text": doc.page_content, "meta": doc.metadata} 
            for i, doc in enumerate(unique_docs)
        ])
        results = ranker.rerank(rerank_request)
        
        # L·∫•y Top K t·ªët nh·∫•t sau Rerank
        top_results = results[:AppConfig.RERANK_TOP_K]
        
        # --- 3. CONTEXT PREPARATION ---
        context_text = ""
        sources_list = []
        
        for res in top_results:
            meta = res["meta"]
            source_info = f"[{meta.get('source', 'TL')}, {meta.get('topic', '')}, {meta.get('lesson', '')}, Tr.{meta.get('page', '?')}]"
            content = res["text"]
            context_text += f"N·ªôi dung: {content}\nNgu·ªìn: {source_info}\n\n"
            sources_list.append(source_info)

        # --- 4. GENERATION (PROMPT ENGINEERING) ---
        system_prompt = f"""
        B·∫°n l√† Tr·ª£ l√Ω AI Gi√°o d·ª•c chuy√™n s√¢u v·ªÅ Tin h·ªçc ph·ªï th√¥ng (L·ªõp 10, 11, 12).
        Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi h·ªçc t·∫≠p d·ª±a tr√™n NG·ªÆ C·∫¢NH (CONTEXT) ƒë∆∞·ª£c cung c·∫•p.
        
        QUY T·∫ÆC B·∫ÆT BU·ªòC (TU√ÇN TH·ª¶ NGHI√äM NG·∫∂T):
        1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong ph·∫ßn CONTEXT b√™n d∆∞·ªõi. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y tr·∫£ l·ªùi: "Xin l·ªói, s√°ch gi√°o khoa kh√¥ng ƒë·ªÅ c·∫≠p chi ti·∫øt v·∫•n ƒë·ªÅ n√†y."
        2. TR√çCH D·∫™N: M·ªçi kh·∫≥ng ƒë·ªãnh ph·∫£i ƒëi k√®m ngu·ªìn g·ªëc c·ª• th·ªÉ t·ª´ metadata (B√†i, Ch·ªß ƒë·ªÅ, Trang).
           V√≠ d·ª•: "Th√¥ng tin l√† s·ª± hi·ªÉu bi·∫øt [Tin 10_KNTT, B√†i 1, Tr.5]".
        3. PHONG C√ÅCH: S∆∞ ph·∫°m, d·ªÖ hi·ªÉu, gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc (step-by-step), ph√π h·ª£p h·ªçc sinh.
        4. ƒê·ªäNH D·∫†NG: S·ª≠ d·ª•ng Markdown, in ƒë·∫≠m c√°c thu·∫≠t ng·ªØ quan tr·ªçng.
        
        CONTEXT D·ªÆ LI·ªÜU SGK:
        {context_text}
        """

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]

        try:
            stream = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=messages,
                temperature=AppConfig.LLM_TEMPERATURE,
                max_tokens=2048,
                top_p=1,
                stream=True,
                stop=None,
            )
            
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content
                    
        except Exception as e:
            yield f"‚ùå L·ªói k·∫øt n·ªëi LLM: {str(e)}"

# ===============================
# 3. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (UI) - GI·ªÆ NGUY√äN
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path): return ""
        with open(file_path, "rb") as f: data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        # (CSS Gi·ªØ nguy√™n nh∆∞ c≈© ƒë·ªÉ kh√¥ng ph√° v·ª° giao di·ªán)
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown { font-family: 'Inter', sans-serif !important; }
            .main-header { background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%); padding: 1.5rem; border-radius: 10px; color: white; margin-bottom: 2rem; }
            .stChatMessage { border-radius: 10px; border: 1px solid #e0e0e0; }
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_header():
        logo_base64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        header_html = f"""
        <div class="main-header">
            <div style="display: flex; align-items: center;">
                <img src="data:image/jpeg;base64,{logo_base64}" style="width: 80px; height: 80px; border-radius: 50%; margin-right: 20px;">
                <div>
                    <h1 style="margin:0; font-size: 2rem;">KTC CHATBOT - TR·ª¢ L√ù TIN H·ªåC</h1>
                    <p style="margin:0; opacity: 0.8;">H·ªá th·ªëng h·ªó tr·ª£ h·ªçc t·∫≠p chu·∫©n KHKT & GDPT 2018</p>
                </div>
            </div>
        </div>
        """
        st.markdown(header_html, unsafe_allow_html=True)

# ===============================
# 4. H√ÄM MAIN
# ===============================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}. Vui l√≤ng c√†i ƒë·∫∑t: pip install langchain-community langchain-huggingface faiss-cpu flashrank groq PyPDF2")
        return

    UIManager.inject_custom_css()
    
    # Sidebar Setup
    with st.sidebar:
        st.image(AppConfig.LOGO_PROJECT, width=100) if os.path.exists(AppConfig.LOGO_PROJECT) else None
        st.title("C·∫•u h√¨nh")
        groq_api_key = st.text_input("Nh·∫≠p Groq API Key:", type="password")
        
        st.divider()
        st.subheader("Qu·∫£n l√Ω d·ªØ li·ªáu")
        uploaded_files = st.file_uploader("T·∫£i l√™n SGK (PDF)", accept_multiple_files=True, type=['pdf'])
        
        if st.button("üîÑ Hu·∫•n luy·ªán l·∫°i H·ªá th·ªëng"):
            if uploaded_files:
                if not os.path.exists(AppConfig.PDF_DIR): os.makedirs(AppConfig.PDF_DIR)
                # X√≥a d·ªØ li·ªáu c≈©
                if os.path.exists(AppConfig.VECTOR_DB_PATH): shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                if os.path.exists(AppConfig.BM25_PATH): os.remove(AppConfig.BM25_PATH)
                
                for file in uploaded_files:
                    with open(os.path.join(AppConfig.PDF_DIR, file.name), "wb") as f:
                        f.write(file.getbuffer())
                
                st.session_state.vector_manager = VectorStoreManager()
                st.session_state.vector_db, st.session_state.bm25 = st.session_state.vector_manager.create_or_load_vector_store()
                st.success("ƒê√£ n·∫°p d·ªØ li·ªáu th√†nh c√¥ng!")
                st.rerun()
            else:
                st.warning("Vui l√≤ng ch·ªçn file PDF!")

    UIManager.render_header()

    if not groq_api_key:
        st.info("üëà Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        return

    client = Groq(api_key=groq_api_key)

    # Init Session State
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! M√¨nh l√† tr·ª£ l√Ω AI Tin h·ªçc. B·∫°n c·∫ßn t√¨m hi·ªÉu ki·∫øn th·ª©c n√†o trong SGK?"}]
    
    if "vector_db" not in st.session_state:
        st.session_state.vector_manager = VectorStoreManager()
        st.session_state.vector_db, st.session_state.bm25 = st.session_state.vector_manager.create_or_load_vector_store()

    # Chat Interface
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        st.chat_message(msg["role"], avatar=avatar).write(msg["content"])

    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user", avatar="üßë‚Äçüéì").write(prompt)

        with st.chat_message("assistant", avatar="ü§ñ"):
            response_placeholder = st.empty()
            full_response = ""
            
            # G·ªçi RAGEngine
            generator = RAGEngine.generate_response(
                client, 
                st.session_state.vector_db, 
                st.session_state.bm25, 
                prompt
            )
            
            for chunk in generator:
                full_response += chunk
                response_placeholder.markdown(full_response + "‚ñå")
            
            response_placeholder.markdown(full_response)
            
        st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()