import os
import glob
import time
import streamlit as st
from pathlib import Path

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói th√¥ng minh ---
try:
    from pypdf import PdfReader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==============================================================================

st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω KHKT",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    """C·∫•u h√¨nh trung t√¢m cho ·ª©ng d·ª•ng - D·ªÖ d√†ng ƒëi·ªÅu ch·ªânh"""
    # Model AI
    LLM_MODEL = 'llama-3.1-8b-instant'
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    # ƒê∆∞·ªùng d·∫´n files
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    LOGO_PATH = "LOGO.jpg"
    
    # Tham s·ªë RAG
    CHUNK_SIZE = 1000 
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 4
    
    # T·ªëi ∆∞u performance
    MAX_CONTEXT_LENGTH = 3000  # Gi·ªõi h·∫°n ƒë·ªô d√†i context ƒë·ªÉ tr√°nh overload
    ENABLE_TRANSLATION = False  # T·∫ÆT d·ªãch thu·∫≠t ƒë·ªÉ gi·∫£m RAM (model multilingual ƒë√£ ƒë·ªß t·ªët)

# ==============================================================================
# 2. UI/UX: GIAO DI·ªÜN HI·ªÜN ƒê·∫†I & ANIMATIONS
# ==============================================================================

def inject_custom_css():
    """CSS t·ªëi ∆∞u cho giao di·ªán thi ƒë·∫•u - Modern & Professional"""
    st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        
        /* Font ch·ªØ hi·ªán ƒë·∫°i */
        html, body, [class*="css"] {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
        }

        /* Header ch√≠nh v·ªõi gradient ƒë·∫πp m·∫Øt */
        .main-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 2rem 1.5rem;
            border-radius: 20px;
            color: white;
            text-align: center;
            margin-bottom: 2rem;
            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
            animation: fadeInDown 0.6s ease-out;
        }
        
        @keyframes fadeInDown {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        .main-header h1 {
            color: white !important;
            font-weight: 700;
            margin: 0;
            font-size: 2.5rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        }
        
        .main-header p {
            margin-top: 0.8rem;
            opacity: 0.95;
            font-size: 1.15rem;
            font-weight: 300;
        }

        /* Sidebar hi·ªán ƒë·∫°i */
        [data-testid="stSidebar"] {
            background: linear-gradient(180deg, #f8f9fa 0%, #e9ecef 100%);
        }
        
        .sidebar-card {
            background: white;
            padding: 20px;
            border-radius: 15px;
            border-left: 5px solid #667eea;
            box-shadow: 0 4px 15px rgba(0,0,0,0.08);
            margin-bottom: 20px;
            transition: transform 0.2s ease;
        }
        
        .sidebar-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.12);
        }
        
        .sidebar-card h4 {
            color: #667eea;
            margin-top: 0;
            font-size: 1.1rem;
            font-weight: 700;
        }
        
        /* Chat bubbles ƒë·∫πp h∆°n */
        .stChatMessage {
            border-radius: 15px;
            padding: 1rem;
            margin-bottom: 0.5rem;
            border: none;
            animation: fadeIn 0.3s ease-in;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        
        /* Tin nh·∫Øn user - m√†u xanh nh·∫°t */
        [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            border-left: 4px solid #2196f3;
        }
        
        /* N√∫t b·∫•m ƒë·∫πp h∆°n */
        .stButton > button {
            border-radius: 10px;
            font-weight: 600;
            transition: all 0.3s ease;
            border: none;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .stButton > button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        
        /* Status container ƒë·∫πp h∆°n */
        [data-testid="stStatusWidget"] {
            border-radius: 10px;
            border: 2px solid #e0e0e0;
        }
        
        /* Input chat ƒë·∫πp h∆°n */
        .stChatInputContainer {
            border-top: 2px solid #e0e0e0;
            padding-top: 1rem;
        }
    </style>
    """, unsafe_allow_html=True)

# ==============================================================================
# 3. QU·∫¢N L√ù T√ÄI NGUY√äN V·ªöI CACHE TH√îNG MINH
# ==============================================================================

@st.cache_resource(show_spinner=False)
def load_groq_client():
    """Load Groq API client v·ªõi x·ª≠ l√Ω l·ªói"""
    try:
        api_key = st.secrets.get("GROQ_API_KEY")
        if not api_key:
            st.error("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh GROQ_API_KEY trong Streamlit secrets!")
            return None
        return Groq(api_key=api_key)
    except Exception as e:
        st.error(f"‚ùå L·ªói k·∫øt n·ªëi Groq API: {e}")
        return None

@st.cache_resource(show_spinner=False)
def load_embedding_model():
    """Load model embedding v·ªõi fallback th√¥ng minh"""
    try:
        with st.spinner("üîÑ ƒêang t·∫£i model embedding (ch·ªâ l·∫ßn ƒë·∫ßu)..."):
            embeddings = HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},  # Force CPU ƒë·ªÉ tr√°nh l·ªói CUDA
                encode_kwargs={'normalize_embeddings': True}  # C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
            )
        return embeddings
    except Exception as e:
        st.error(f"‚ùå Kh√¥ng th·ªÉ load model embedding: {e}")
        st.info("üí° Th·ª≠ kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng ho·∫∑c ki·ªÉm tra k·∫øt n·ªëi m·∫°ng.")
        return None

@st.cache_data(show_spinner=False, ttl=3600)  # Cache 1 gi·ªù
def load_and_process_pdfs(pdf_dir):
    """
    ƒê·ªçc v√† x·ª≠ l√Ω t·∫•t c·∫£ PDF trong th∆∞ m·ª•c
    TTL=3600s ƒë·ªÉ t·ª± ƒë·ªông refresh n·∫øu c√≥ PDF m·ªõi
    """
    docs = []
    
    # Ki·ªÉm tra th∆∞ m·ª•c t·ªìn t·∫°i
    if not os.path.exists(pdf_dir):
        st.warning(f"‚ö†Ô∏è Th∆∞ m·ª•c {pdf_dir} kh√¥ng t·ªìn t·∫°i. T·∫°o th∆∞ m·ª•c r·ªóng.")
        os.makedirs(pdf_dir, exist_ok=True)
        return docs
    
    pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
    
    if not pdf_files:
        st.info(f"üìÅ Ch∆∞a c√≥ file PDF n√†o trong th∆∞ m·ª•c '{pdf_dir}'")
        return docs
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, pdf_path in enumerate(pdf_files):
        try:
            filename = os.path.basename(pdf_path)
            status_text.text(f"üìÑ ƒêang x·ª≠ l√Ω: {filename}")
            
            reader = PdfReader(pdf_path)
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text and len(text.strip()) > 50:  # B·ªè qua trang tr·ªëng
                    docs.append(Document(
                        page_content=text,
                        metadata={"source": filename, "page": page_num + 1}
                    ))
            
            progress_bar.progress((idx + 1) / len(pdf_files))
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è L·ªói ƒë·ªçc file {filename}: {str(e)}")
            continue
    
    progress_bar.empty()
    status_text.empty()
    
    # Split documents th√†nh chunks
    if docs:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        splits = splitter.split_documents(docs)
        st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(pdf_files)} file PDF ‚Üí {len(splits)} chunks")
        return splits
    
    return []

# ==============================================================================
# 4. VECTOR DATABASE V·ªöI QU·∫¢N L√ù TH√îNG MINH
# ==============================================================================

class KnowledgeBase:
    """Qu·∫£n l√Ω Vector Database v·ªõi c√°c ch·ª©c nƒÉng n√¢ng cao"""
    
    def __init__(self):
        self.embeddings = load_embedding_model()
        self.db_path = AppConfig.VECTOR_DB_PATH

    def get_vector_store(self, force_rebuild=False):
        """
        L·∫•y ho·∫∑c t·∫°o Vector Store
        force_rebuild=True: X√¢y d·ª±ng l·∫°i t·ª´ ƒë·∫ßu (khi th√™m PDF m·ªõi)
        """
        if not self.embeddings:
            st.error("‚ùå Model embedding ch∆∞a s·∫µn s√†ng!")
            return None

        # Ki·ªÉm tra DB c√≥ t·ªìn t·∫°i kh√¥ng
        db_exists = os.path.exists(self.db_path)
        
        if db_exists and not force_rebuild:
            try:
                with st.spinner("üîç ƒêang t·∫£i c∆° s·ªü d·ªØ li·ªáu vector..."):
                    vector_db = FAISS.load_local(
                        self.db_path, 
                        self.embeddings, 
                        allow_dangerous_deserialization=True
                    )
                st.success("‚úÖ ƒê√£ t·∫£i Vector Database t·ª´ cache")
                return vector_db
            except Exception as e:
                st.warning(f"‚ö†Ô∏è L·ªói t·∫£i DB c≈©: {e}. ƒêang t·∫°o m·ªõi...")
        
        # T·∫°o DB m·ªõi
        return self._create_new_db()

    def _create_new_db(self):
        """T·∫°o Vector Database m·ªõi t·ª´ PDF"""
        splits = load_and_process_pdfs(AppConfig.PDF_DIR)
        
        if not splits:
            st.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o Vector Database")
            return None
        
        try:
            with st.spinner(f"üî® ƒêang x√¢y d·ª±ng Vector Database ({len(splits)} chunks)..."):
                vector_db = FAISS.from_documents(splits, self.embeddings)
                vector_db.save_local(self.db_path)
            
            st.success("‚úÖ ƒê√£ t·∫°o v√† l∆∞u Vector Database m·ªõi!")
            return vector_db
            
        except Exception as e:
            st.error(f"‚ùå L·ªói t·∫°o Vector DB: {e}")
            return None
    
    def rebuild_database(self):
        """Reset v√† x√¢y d·ª±ng l·∫°i to√†n b·ªô Database"""
        # X√≥a DB c≈©
        if os.path.exists(self.db_path):
            import shutil
            shutil.rmtree(self.db_path)
            st.info("üóëÔ∏è ƒê√£ x√≥a Database c≈©")
        
        # X√≥a cache
        load_and_process_pdfs.clear()
        
        # T·∫°o m·ªõi
        return self.get_vector_store(force_rebuild=True)

# ==============================================================================
# 5. CORE LOGIC: RAG PROCESSING
# ==============================================================================

def get_context(vector_db, query):
    """
    T√¨m ki·∫øm context t·ª´ Vector DB
    Returns: (context_text, list_sources)
    """
    if not vector_db:
        return "", []
    
    try:
        # Similarity search v·ªõi ƒëi·ªÉm s·ªë
        results = vector_db.similarity_search_with_score(
            query, 
            k=AppConfig.TOP_K_RETRIEVAL
        )
        
        context_parts = []
        sources = []
        total_length = 0
        
        for doc, score in results:
            # L·ªçc k·∫øt qu·∫£ c√≥ score t·ªët (c√†ng th·∫•p c√†ng t·ªët v·ªõi FAISS)
            if score > 1.5:  # Threshold t√πy ch·ªânh
                continue
            
            src = doc.metadata.get('source', 'T√†i li·ªáu')
            page = doc.metadata.get('page', '1')
            content = doc.page_content.replace("\n", " ").strip()
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i context
            if total_length + len(content) > AppConfig.MAX_CONTEXT_LENGTH:
                break
            
            context_parts.append(f"[{src} - Tr.{page}]:\n{content}")
            sources.append(f"{src} (Trang {page})")
            total_length += len(content)
        
        context_text = "\n\n".join(context_parts)
        return context_text, list(set(sources))
        
    except Exception as e:
        st.error(f"‚ùå L·ªói t√¨m ki·∫øm: {e}")
        return "", []

def generate_stream(client, context, question):
    """
    G·ªçi Groq API ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi streaming
    """
    # System prompt ƒë∆∞·ª£c t·ªëi ∆∞u cho gi√°o d·ª•c
    system_prompt = f"""B·∫°n l√† KTC Assistant - tr·ª£ l√Ω ·∫£o th√¥ng minh h·ªó tr·ª£ h·ªçc t·∫≠p m√¥n Tin h·ªçc THPT.

NHI·ªÜM V·ª§:
- Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n CONTEXT ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi
- N·∫øu CONTEXT kh√¥ng ƒë·ªß th√¥ng tin, h√£y d√πng ki·∫øn th·ª©c Tin h·ªçc chu·∫©n (Ch∆∞∆°ng tr√¨nh GDPT 2018)
- N·∫øu kh√¥ng bi·∫øt, h√£y th√†nh th·∫≠t n√≥i "M√¨nh ch∆∞a c√≥ ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c"

VƒÇN PHONG:
- Th√¢n thi·ªán, g·∫ßn g≈©i nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n h·ªçc (x∆∞ng h√¥: m√¨nh/b·∫°n)
- Gi·∫£i th√≠ch d·ªÖ hi·ªÉu, c√≥ v√≠ d·ª• c·ª• th·ªÉ
- Khuy·∫øn kh√≠ch tinh th·∫ßn h·ªçc t·∫≠p

ƒê·ªäNH D·∫†NG:
- S·ª≠ d·ª•ng Markdown: **in ƒë·∫≠m** cho thu·∫≠t ng·ªØ quan tr·ªçng
- D√πng bullet points cho danh s√°ch
- Chia ƒëo·∫°n r√µ r√†ng ƒë·ªÉ d·ªÖ ƒë·ªçc

[CONTEXT T√ÄI LI·ªÜU]:
{context if context else "Kh√¥ng c√≥ t√†i li·ªáu li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."}
"""
    
    try:
        completion = client.chat.completions.create(
            model=AppConfig.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            stream=True,
            temperature=0.4,  # TƒÉng nh·∫π ƒë·ªÉ c√¢u tr·∫£ l·ªùi t·ª± nhi√™n h∆°n
            max_tokens=1500
        )
        
        for chunk in completion:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
    except Exception as e:
        yield f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi AI: {str(e)}\n\nVui l√≤ng th·ª≠ l·∫°i sau!"

# ==============================================================================
# 6. MAIN APPLICATION
# ==============================================================================

def main():
    """H√†m ch√≠nh ch·∫°y ·ª©ng d·ª•ng"""
    
    # Ki·ªÉm tra dependencies
    if not DEPENDENCIES_OK:
        st.error(f"‚ùå Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.info("üí° Ch·∫°y l·ªánh: `pip install -r requirements.txt`")
        st.stop()
    
    # Inject CSS
    inject_custom_css()
    
    # ============= SIDEBAR =============
    with st.sidebar:
        # Logo
        if os.path.exists(AppConfig.LOGO_PATH):
            st.image(AppConfig.LOGO_PATH, use_container_width=True)
        else:
            st.markdown("### ü§ñ KTC AI Assistant")

        st.markdown("---")
        
        # Th√¥ng tin d·ª± √°n
        st.markdown("""
        <div class="sidebar-card">
            <h4>üèÜ S·∫¢N PH·∫®M KHKT C·∫§P TR∆Ø·ªúNG</h4>
            <p style="font-size: 0.9rem; margin: 8px 0;"><b>üè´ ƒê∆°n v·ªã:</b><br>THCS & THPT Ph·∫°m Ki·ªát</p>
            <p style="font-size: 0.9rem; margin: 8px 0;"><b>üë®‚Äçüíª T√°c gi·∫£:</b><br>‚Ä¢ B√πi T√° T√πng<br>‚Ä¢ Cao S·ªπ B·∫£o Chung</p>
            <p style="font-size: 0.9rem; margin: 8px 0;"><b>üßë‚Äçüè´ GVHD:</b> Th·∫ßy Khanh</p>
        </div>
        """, unsafe_allow_html=True)
        
        # C√†i ƒë·∫∑t n√¢ng cao
        with st.expander("üõ†Ô∏è C√†i ƒë·∫∑t n√¢ng cao"):
            top_k = st.slider(
                "S·ªë l∆∞·ª£ng chunks t√¨m ki·∫øm", 
                min_value=1, 
                max_value=10, 
                value=AppConfig.TOP_K_RETRIEVAL,
                help="TƒÉng ƒë·ªÉ t√¨m nhi·ªÅu th√¥ng tin h∆°n, nh∆∞ng c√≥ th·ªÉ l√†m ch·∫≠m"
            )
            AppConfig.TOP_K_RETRIEVAL = top_k
            
            if st.button("üîÑ L√†m m·ªõi Database", use_container_width=True):
                with st.spinner("ƒêang x√¢y d·ª±ng l·∫°i Database..."):
                    kb = KnowledgeBase()
                    st.session_state.vector_db = kb.rebuild_database()
                st.success("‚úÖ ƒê√£ l√†m m·ªõi Database!")
                st.rerun()

        st.markdown("---")
        
        # N√∫t x√≥a l·ªãch s·ª≠
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            st.session_state.messages = []
            st.success("‚úÖ ƒê√£ x√≥a l·ªãch s·ª≠!")
            time.sleep(0.5)
            st.rerun()
        
        # H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
        with st.expander("üìñ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng"):
            st.markdown("""
            **C√°ch s·ª≠ d·ª•ng:**
            1. ƒê·∫∑t file PDF v√†o th∆∞ m·ª•c `PDF_KNOWLEDGE`
            2. Nh·∫•n "L√†m m·ªõi Database" ·ªü tr√™n
            3. B·∫Øt ƒë·∫ßu h·ªèi c√¢u h·ªèi!
            
            **M·∫πo:**
            - H·ªèi c√¢u h·ªèi c·ª• th·ªÉ ƒë·ªÉ ƒë∆∞·ª£c tr·∫£ l·ªùi t·ªët h∆°n
            - Ki·ªÉm tra "Ngu·ªìn t√†i li·ªáu" ƒë·ªÉ x√°c minh th√¥ng tin
            """)

    # ============= MAIN CONTENT =============
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>üéì TR·ª¢ L√ù ·∫¢O KTC</h1>
        <p>H·ªá th·ªëng AI h·ªó tr·ª£ h·ªçc t·∫≠p Tin h·ªçc & Nghi√™n c·ª©u Khoa h·ªçc</p>
    </div>
    """, unsafe_allow_html=True)

    # Kh·ªüi t·∫°o chat history
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant", 
                "content": "Ch√†o b·∫°n! üëã M√¨nh l√† **KTC Assistant**.\n\nM√¨nh c√≥ th·ªÉ gi√∫p b·∫°n:\n- Gi·∫£i ƒë√°p th·∫Øc m·∫Øc v·ªÅ Tin h·ªçc\n- H·ªó tr·ª£ d·ª± √°n KHKT\n- Tra c·ª©u t√†i li·ªáu chuy√™n ng√†nh\n\nH√£y ƒë·∫∑t c√¢u h·ªèi ƒë·ªÉ b·∫Øt ƒë·∫ßu nh√©! üòä"
            }
        ]

    # Load resources
    groq_client = load_groq_client()
    
    if not groq_client:
        st.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi Groq API. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh!")
        st.stop()

    # Load/Create Vector DB
    if "vector_db" not in st.session_state:
        kb = KnowledgeBase()
        st.session_state.vector_db = kb.get_vector_store()

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # Chat input
    if prompt := st.chat_input("üí¨ Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y..."):
        # L∆∞u tin nh·∫Øn user
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(prompt)

        # X·ª≠ l√Ω v√† tr·∫£ l·ªùi
        with st.chat_message("assistant", avatar="ü§ñ"):
            response_placeholder = st.empty()
            
            # Status processing
            with st.status("üöÄ ƒêang x·ª≠ l√Ω c√¢u h·ªèi...", expanded=True) as status:
                st.write("üîç ƒêang t√¨m ki·∫øm t√†i li·ªáu li√™n quan...")
                context, sources = get_context(st.session_state.vector_db, prompt)
                
                if sources:
                    st.write(f"‚úÖ T√¨m th·∫•y {len(sources)} ngu·ªìn t√†i li·ªáu")
                else:
                    st.write("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y trong t√†i li·ªáu, s·ª≠ d·ª•ng ki·∫øn th·ª©c n·ªÅn")
                
                st.write("üí≠ ƒêang suy nghƒ© v√† so·∫°n c√¢u tr·∫£ l·ªùi...")
                status.update(label="‚ú® Ho√†n th√†nh!", state="complete", expanded=False)

            # Stream response
            full_response = ""
            for chunk in generate_stream(groq_client, context, prompt):
                full_response += chunk
                response_placeholder.markdown(full_response + "‚ñå")
            
            response_placeholder.markdown(full_response)
            
            # Hi·ªÉn th·ªã ngu·ªìn t√†i li·ªáu
            if sources:
                with st.expander("üìö Ngu·ªìn t√†i li·ªáu tham kh·∫£o"):
                    for idx, src in enumerate(sources, 1):
                        st.caption(f"{idx}. {src}")

            # L∆∞u response
            st.session_state.messages.append({
                "role": "assistant", 
                "content": full_response
            })

if __name__ == "__main__":
    main()