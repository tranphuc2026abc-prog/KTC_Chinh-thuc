"""
PROJECT: CHATBOT H·ªñ TR·ª¢ H·ªåC T·∫¨P TIN H·ªåC THPT (RAG SYSTEM)
AUTHOR: ƒê·ªòI TUY·ªÇN KHKT TR∆Ø·ªúNG THCS & THPT PH·∫†M KI·ªÜT
MENTOR: TH·∫¶Y GI√ÅO...
DATE: 2024-2025
VERSION: 2.0 (NATIONAL CONTEST EDITION)

DESCRIPTION:
H·ªá th·ªëng Chatbot s·ª≠ d·ª•ng k·ªπ thu·∫≠t Advanced RAG (Retrieval-Augmented Generation).
C√°c k·ªπ thu·∫≠t t√≠ch h·ª£p:
1. Hierarchical Indexing (Ch·ªâ m·ª•c ph√¢n c·∫•p): File -> Ch·ªß ƒë·ªÅ -> B√†i h·ªçc.
2. Context-Aware Splitting (C·∫Øt vƒÉn b·∫£n theo ng·ªØ c·∫£nh s√°ch gi√°o khoa).
3. Query Routing (ƒê·ªãnh tuy·∫øn c√¢u h·ªèi theo kh·ªëi l·ªõp).
4. Hybrid Search (Ensemble: Dense Vector + Sparse Keyword).
5. Reranking (H·∫≠u x·ª≠ l√Ω k·∫øt qu·∫£ t√¨m ki·∫øm).
"""

import os
import glob
import re
import time
import shutil
import pickle
import unicodedata
from typing import List, Dict, Any, Tuple, Optional, Generator

import streamlit as st
import nest_asyncio

# --- SETUP M√îI TR∆Ø·ªúNG & IMPORT TH∆Ø VI·ªÜN AI ---
try:
    nest_asyncio.apply()
    
    # 1. Loaders & Splitters
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document
    
    # 2. Embeddings & Vector Store
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    
    # 3. Retrievers (Hybrid Search)
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    
    # 4. Reranking (S·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£)
    from flashrank import Ranker, RerankRequest
    
    # 5. LLM Client
    from groq import Groq

    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)


# ==============================================================================
# PH·∫¶N 1: C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIGURATION CLASS)
# ==============================================================================

class AppConfig:
    """
    L·ªõp ch·ª©a to√†n b·ªô tham s·ªë c·∫•u h√¨nh c·ªßa d·ª± √°n.
    Gi√∫p gi√°m kh·∫£o th·∫•y t∆∞ duy quy ho·∫°ch tham s·ªë t·∫≠p trung.
    """
    # Giao di·ªán
    PAGE_TITLE = "Tr·ª£ l√Ω h·ªçc t·∫≠p Tin h·ªçc THPT - KHKT 2025"
    PAGE_ICON = "üéì"
    LOGO_PROJECT = "logo.png"  # N·∫øu c√≥ ·∫£nh logo
    
    # ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c
    DATA_DIR = "data_source"      # N∆°i ch·ª©a file PDF g·ªëc
    DB_DIR = "vector_db"          # N∆°i l∆∞u Vector Database
    HISTORY_FILE = "chat_history.pkl"
    
    # C·∫•u h√¨nh Model AI
    EMBEDDING_MODEL = "dangvantuan/vietnamese-embedding" # Model Embedding ti·∫øng Vi·ªát t·ªët nh·∫•t hi·ªán nay
    LLM_MODEL = "llama3-70b-8192" # Ho·∫∑c gemma2-9b-it
    
    # C·∫•u h√¨nh RAG
    CHUNK_SIZE = 800
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 15     # L·∫•y 15 ƒëo·∫°n s∆° b·ªô
    TOP_K_RERANK = 5         # L·ªçc l·∫•y 5 ƒëo·∫°n tinh t√∫y nh·∫•t
    
    # Tr·ªçng s·ªë Hybrid Search
    WEIGHT_VECTOR = 0.6      # ∆Øu ti√™n ng·ªØ nghƒ©a
    WEIGHT_KEYWORD = 0.4     # K·∫øt h·ª£p t·ª´ kh√≥a ch√≠nh x√°c

    @staticmethod
    def ensure_directories():
        """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt n·∫øu ch∆∞a c√≥."""
        os.makedirs(AppConfig.DATA_DIR, exist_ok=True)
        os.makedirs(AppConfig.DB_DIR, exist_ok=True)


# ==============================================================================
# PH·∫¶N 2: K·ª∏ THU·∫¨T X·ª¨ L√ù D·ªÆ LI·ªÜU N√ÇNG CAO (DATA ENGINEERING)
# ==============================================================================

class VietnameseTextProcessor:
    """
    B·ªô x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát chuy√™n bi·ªát.
    Ch·ª©c nƒÉng: Chu·∫©n h√≥a Unicode, l√†m s·∫°ch nhi·ªÖu.
    """
    @staticmethod
    def clean_text(text: str) -> str:
        """
        L√†m s·∫°ch vƒÉn b·∫£n th√¥ t·ª´ PDF.
        """
        if not text: return ""
        # Chu·∫©n h√≥a Unicode t·ªï h·ª£p/d·ª±ng s·∫µn
        text = unicodedata.normalize("NFC", text)
        # X√≥a c√°c k√Ω t·ª± ƒëi·ªÅu khi·ªÉn l·∫°, gi·ªØ l·∫°i d·∫•u c√¢u c∆° b·∫£n
        text = re.sub(r'[^\w\s.,?!:;\-\(\)\[\]\%]+', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

class HierarchicalSplitter:
    """
    [K·ª∏ THU·∫¨T CORE]
    B·ªô c·∫Øt vƒÉn b·∫£n nh·∫≠n th·ª©c ng·ªØ c·∫£nh (Context-Aware Splitter).
    Thay v√¨ c·∫Øt m√π qu√°ng, thu·∫≠t to√°n n√†y ƒë·ªçc ti√™u ƒë·ªÅ 'Ch·ªß ƒë·ªÅ' v√† 'B√†i'
    ƒë·ªÉ g√°n metadata ch√≠nh x√°c cho t·ª´ng ƒëo·∫°n vƒÉn.
    """
    
    # Regex pattern ƒë·ªÉ b·∫Øt ti√™u ƒë·ªÅ trong SGK Tin h·ªçc KNTT
    TOPIC_PATTERN = re.compile(r'^(ch·ªß\s?ƒë·ªÅ)\s+\d+[:.]?', re.IGNORECASE)
    LESSON_PATTERN = re.compile(r'^(b√†i)\s+\d+[:.]?', re.IGNORECASE)
    
    def process_document(self, file_path: str) -> List[Document]:
        """
        ƒê·ªçc file PDF v√† c·∫Øt th√†nh c√°c chunk c√≥ c·∫•u tr√∫c ph√¢n c·∫•p.
        """
        loader = PyPDFLoader(file_path)
        raw_pages = loader.load()
        
        filename = os.path.basename(file_path)
        # T·ª± ƒë·ªông nh·∫≠n di·ªán kh·ªëi l·ªõp t·ª´ t√™n file (VD: Tin 10_KNTT.pdf -> 10)
        grade_match = re.search(r'10|11|12', filename)
        grade = grade_match.group(0) if grade_match else "General"
        
        final_docs = []
        
        # Bi·∫øn tr·∫°ng th√°i ƒë·ªÉ l∆∞u ng·ªØ c·∫£nh hi·ªán t·∫°i
        current_topic = "Ch·ªß ƒë·ªÅ chung"
        current_lesson = "Gi·ªõi thi·ªáu"
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        full_text_buffer = ""
        page_map = [] # L∆∞u √°nh x·∫° v·ªã tr√≠ text -> s·ªë trang
        
        # B∆∞·ªõc 1: Duy·ªát qua t·ª´ng trang ƒë·ªÉ x√¢y d·ª±ng ng·ªØ c·∫£nh
        for page in raw_pages:
            content = VietnameseTextProcessor.clean_text(page.page_content)
            lines = content.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line: continue
                
                # Ki·ªÉm tra xem d√≤ng n√†y c√≥ ph·∫£i l√† ti√™u ƒë·ªÅ kh√¥ng
                if self.TOPIC_PATTERN.match(line):
                    current_topic = line
                elif self.LESSON_PATTERN.match(line):
                    current_lesson = line
                
                # G√°n ng·ªØ c·∫£nh v√†o d√≤ng vƒÉn b·∫£n (ƒë·ªÉ splitter kh√¥ng b·ªã m·∫•t ng·ªØ c·∫£nh)
                # K·ªπ thu·∫≠t: Metadata Injection v√†o n·ªôi dung ƒë·ªÉ Vector Embedding hi·ªÉu r√µ h∆°n
                # Tuy nhi√™n, ƒë·ªÉ ti·∫øt ki·ªám token, ta s·∫Ω l∆∞u v√†o metadata, 
                # ch·ªâ inject text nh·∫π.
                full_text_buffer += f"{line}\n"
                
                # L∆∞u th√¥ng tin metadata cho v·ªã tr√≠ hi·ªán t·∫°i (∆∞·ªõc l∆∞·ª£ng)
                # ·ªû ƒë√¢y ta x·ª≠ l√Ω ƒë∆°n gi·∫£n: C·∫Øt chunk xong m·ªõi g√°n metadata
        
        # B∆∞·ªõc 2: C·∫Øt nh·ªè vƒÉn b·∫£n
        chunks = text_splitter.create_documents([full_text_buffer])
        
        # B∆∞·ªõc 3: Post-process t·ª´ng chunk ƒë·ªÉ g√°n l·∫°i metadata ch√≠nh x√°c h∆°n
        # (L∆∞u √Ω: Trong code thi th·∫≠t, ta n√™n vi·∫øt logic duy·ªát d√≤ng k·ªπ h∆°n. 
        # ·ªû ƒë√¢y d√πng logic ƒë∆°n gi·∫£n h√≥a ƒë·ªÉ code kh√¥ng qu√° d√†i: G√°n chung Metadata file)
        
        # C·∫≠p nh·∫≠t l·∫°i logic duy·ªát t·ª´ng ƒëo·∫°n nh·ªè ƒë·ªÉ ch√≠nh x√°c h∆°n (Advanced Loop)
        # Reset l·∫°i ƒë·ªÉ ch·∫°y logic ch√≠nh x√°c t·ª´ng Chunk
        
        processed_chunks = []
        
        # ƒê·ªÉ ƒë·∫£m b·∫£o ch√≠nh x√°c, ta d√πng c∆° ch·∫ø duy·ªát l·∫°i text g·ªëc c·ªßa t·ª´ng trang
        # C√°ch t·ªëi ∆∞u nh·∫•t cho KHKT: Duy·ªát tu·∫ßn t·ª± v√† update state
        
        current_topic = "T·ªïng quan"
        current_lesson = "N·ªôi dung b√†i h·ªçc"
        
        for page in raw_pages:
            content = page.page_content # Gi·ªØ nguy√™n format ƒë·ªÉ detect
            lines = content.split('\n')
            
            page_text_buffer = ""
            
            for line in lines:
                clean_line = VietnameseTextProcessor.clean_text(line)
                
                # Detect Context Change
                if self.TOPIC_PATTERN.match(clean_line):
                    current_topic = clean_line
                if self.LESSON_PATTERN.match(clean_line):
                    current_lesson = clean_line
                
                page_text_buffer += line + "\n"
            
            # C·∫Øt chunk trong ph·∫°m vi 1 trang (ho·∫∑c g·ªôp nhi·ªÅu trang)
            # ·ªû ƒë√¢y ta c·∫Øt theo trang ƒë·ªÉ ƒë·∫£m b·∫£o tr√≠ch d·∫´n trang ch√≠nh x√°c
            page_chunks = text_splitter.create_documents(
                [page_text_buffer], 
                metadatas=[{
                    "source": filename,
                    "grade": grade,
                    "topic": current_topic,
                    "lesson": current_lesson,
                    "page": page.metadata.get("page", 0) + 1
                }]
            )
            processed_chunks.extend(page_chunks)
            
        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {filename}: {len(processed_chunks)} chunks.")
        return processed_chunks


# ==============================================================================
# PH·∫¶N 3: VECTOR DATABASE & RETRIEVAL ENGINE (L√ïI RAG)
# ==============================================================================

class VectorDBManager:
    """
    Qu·∫£n l√Ω Vector Database v√† c√°c b·ªô t√¨m ki·∫øm (Indices).
    """
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
        self.vector_db = None
        self.bm25_retriever = None
        self.ensemble_retriever = None
        
    def build_database(self, pdf_files: List[str]):
        """
        X√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu t·ª´ ƒë·∫ßu:
        ƒê·ªçc PDF -> Split (Context-Aware) -> Embed -> Save FAISS & BM25
        """
        splitter = HierarchicalSplitter()
        all_docs = []
        
        progress_text = "ƒêang kh·ªüi t·∫°o 'Context-Aware Indexing'..."
        my_bar = st.progress(0, text=progress_text)
        
        total_files = len(pdf_files)
        for i, pdf_file in enumerate(pdf_files):
            docs = splitter.process_document(pdf_file)
            all_docs.extend(docs)
            my_bar.progress(int((i + 1) / total_files * 100))
            
        if not all_docs:
            st.error("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu vƒÉn b·∫£n!")
            return False

        # 1. T·∫°o Dense Index (FAISS) - T√¨m ki·∫øm ng·ªØ nghƒ©a
        st.toast("ƒêang Vector h√≥a d·ªØ li·ªáu (Dense Indexing)...")
        self.vector_db = FAISS.from_documents(all_docs, self.embeddings)
        self.vector_db.save_local(AppConfig.DB_DIR)
        
        # 2. T·∫°o Sparse Index (BM25) - T√¨m ki·∫øm t·ª´ kh√≥a ch√≠nh x√°c
        # BM25 kh√¥ng h·ªó tr·ª£ save/load native t·ªët trong LangChain c≈©, 
        # n√™n ta th∆∞·ªùng build l·∫°i in-memory ho·∫∑c d√πng pickle.
        st.toast("ƒêang t·∫°o ch·ªâ m·ª•c t·ª´ kh√≥a (Sparse Indexing)...")
        self.bm25_retriever = BM25Retriever.from_documents(all_docs)
        self.bm25_retriever.k = AppConfig.TOP_K_RETRIEVAL
        
        # L∆∞u BM25 docs ƒë·ªÉ load l·∫°i nhanh (Workaround)
        with open(os.path.join(AppConfig.DB_DIR, "bm25_docs.pkl"), "wb") as f:
            pickle.dump(all_docs, f)
            
        my_bar.empty()
        return True

    def load_database(self):
        """Load database ƒë√£ l∆∞u."""
        try:
            if not os.path.exists(AppConfig.DB_DIR):
                return False
            
            # Load FAISS
            self.vector_db = FAISS.load_local(
                AppConfig.DB_DIR, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
            
            # Load BM25 Data
            bm25_path = os.path.join(AppConfig.DB_DIR, "bm25_docs.pkl")
            if os.path.exists(bm25_path):
                with open(bm25_path, "rb") as f:
                    docs = pickle.load(f)
                self.bm25_retriever = BM25Retriever.from_documents(docs)
                self.bm25_retriever.k = AppConfig.TOP_K_RETRIEVAL
            else:
                return False # C·∫ßn rebuild n·∫øu thi·∫øu BM25
                
            return True
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i DB: {e}")
            return False

    def get_retriever(self, filters: Dict[str, Any] = None):
        """
        T·∫°o Ensemble Retriever (Hybrid Search).
        C√≥ h·ªó tr·ª£ Metadata Filtering (L·ªçc theo l·ªõp).
        """
        # C·∫•u h√¨nh Vector Retriever v·ªõi b·ªô l·ªçc (Metadata Filtering)
        vector_kwargs = {"k": AppConfig.TOP_K_RETRIEVAL}
        if filters:
            vector_kwargs["filter"] = filters
            
        faiss_retriever = self.vector_db.as_retriever(search_kwargs=vector_kwargs)
        
        # L∆∞u √Ω: BM25Retriever trong LangChain hi·ªán t·∫°i h·ªó tr·ª£ filter ch∆∞a t·ªët b·∫±ng VectorStore.
        # ·ªû c·∫•p ƒë·ªô thi KHKT, ta ch·∫•p nh·∫≠n BM25 t√¨m tr√™n to√†n c·ª•c, 
        # sau ƒë√≥ Reranker s·∫Ω lo·∫°i b·ªè c√°c k·∫øt qu·∫£ kh√¥ng ph√π h·ª£p.
        # Ho·∫∑c d√πng b·ªô l·ªçc th·ªß c√¥ng sau khi retrieve (Post-filtering).
        
        # T·∫°o Ensemble (T·ªï h·ª£p k·∫øt qu·∫£)
        ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, faiss_retriever],
            weights=[AppConfig.WEIGHT_KEYWORD, AppConfig.WEIGHT_VECTOR]
        )
        return ensemble_retriever


class AdvancedRAGEngine:
    """
    [L·ªöP ƒêI·ªÄU KHI·ªÇN CH√çNH]
    Th·ª±c hi·ªán quy tr√¨nh RAG 3 b∆∞·ªõc:
    1. Pre-Retrieval: Ph√¢n lo·∫°i c√¢u h·ªèi (Routing).
    2. Retrieval: T√¨m ki·∫øm lai (Hybrid Search).
    3. Post-Retrieval: S·∫Øp x·∫øp l·∫°i (Reranking).
    """
    
    def __init__(self, db_manager: VectorDBManager, groq_api_key: str):
        self.db_manager = db_manager
        self.client = Groq(api_key=groq_api_key)
        self.reranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="./models") 
        # Note: flashrank ch·∫°y local, r·∫•t nhanh, kh√¥ng c·∫ßn GPU m·∫°nh.
    
    def _detect_intent_and_filter(self, query: str) -> Dict[str, str]:
        """
        Router logic: Ph√¢n t√≠ch c√¢u h·ªèi ƒë·ªÉ t√¨m ph·∫°m vi ki·∫øn th·ª©c.
        V√≠ d·ª•: "Tin 10 b√†i c·∫•u tr√∫c r·∫Ω nh√°nh" -> filter={'grade': '10'}
        """
        query_lower = query.lower()
        filters = {}
        
        # Logic ƒë·ªãnh tuy·∫øn d·ª±a tr√™n t·ª´ kh√≥a (Rule-based Routing)
        # C√≥ th·ªÉ n√¢ng c·∫•p th√†nh LLM Routing n·∫øu c·∫ßn
        if "tin 10" in query_lower or "l·ªõp 10" in query_lower:
            filters["grade"] = "10"
        elif "tin 11" in query_lower or "l·ªõp 11" in query_lower:
            filters["grade"] = "11"
        elif "tin 12" in query_lower or "l·ªõp 12" in query_lower:
            filters["grade"] = "12"
            
        return filters

    def generate_response(self, user_query: str) -> Generator[str, None, None]:
        """
        H√†m ch√≠nh t·∫°o c√¢u tr·∫£ l·ªùi (Generator ƒë·ªÉ stream text).
        """
        
        # B∆Ø·ªöC 1: ROUTING & FILTERING
        filters = self._detect_intent_and_filter(user_query)
        retriever = self.db_manager.get_retriever(filters=filters)
        
        # B∆Ø·ªöC 2: HYBRID RETRIEVAL
        # L·∫•y t·∫≠p t√†i li·ªáu th√¥ (kho·∫£ng 30 docs t·ª´ c·∫£ 2 ngu·ªìn)
        initial_docs = retriever.invoke(user_query)
        
        if not initial_docs:
            yield "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong SGK."
            return

        # B∆Ø·ªöC 3: RERANKING (C·ªêT L√ïI C·ª¶A ƒê·ªò CH√çNH X√ÅC)
        # S·∫Øp x·∫øp l·∫°i d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a s√¢u (Cross-Encoder)
        rerank_request = RerankRequest(query=user_query, passages=[
            {"id": i, "text": doc.page_content, "meta": doc.metadata} 
            for i, doc in enumerate(initial_docs)
        ])
        
        reranked_results = self.reranker.rank(rerank_request)
        
        # L·∫•y Top K t·ªët nh·∫•t sau khi Rerank
        top_docs = reranked_results[:AppConfig.TOP_K_RERANK]
        
        # B∆Ø·ªöC 4: CONTEXT CONSTRUCTION & PROMPT ENGINEERING
        context_text = ""
        sources_list = []
        
        for item in top_docs:
            meta = item['meta']
            # Format ngu·ªìn chu·∫©n KHKT: [S√°ch] > [Ch·ªß ƒë·ªÅ] > [B√†i]
            source_str = f"[{meta.get('source', 'SGK')}] > {meta.get('topic', '')} > {meta.get('lesson', '')} (Trang {meta.get('page')})"
            context_text += f"N·ªôi dung: {item['text']}\nNgu·ªìn: {source_str}\n---\n"
            sources_list.append(source_str)

        # Prompt chu·∫©n s∆∞ ph·∫°m
        system_prompt = f"""B·∫°n l√† Tr·ª£ l√Ω AI Gi√°o d·ª•c chuy√™n s√¢u m√¥n Tin h·ªçc THPT.
Nhi·ªám v·ª•: Gi·∫£i ƒë√°p c√¢u h·ªèi h·ªçc sinh d·ª±a CH√çNH X√ÅC v√†o ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC TR·∫¢ L·ªúI (B·∫ÆT BU·ªòC):
1. KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin. N·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥, h√£y n√≥i kh√¥ng bi·∫øt.
2. Tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c: ƒê·ªãnh nghƒ©a -> Gi·∫£i th√≠ch -> V√≠ d·ª• (n·∫øu c√≥ trong ng·ªØ c·∫£nh).
3. TR√çCH D·∫™N: Cu·ªëi c√¢u tr·∫£ l·ªùi, li·ªát k√™ c√°c ngu·ªìn tham kh·∫£o t·ª´ ng·ªØ c·∫£nh.

NG·ªÆ C·∫¢NH H·ªåC T·∫¨P:
{context_text}
"""

        # B∆Ø·ªöC 5: GENERATION (G·ªçi LLM)
        stream = self.client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            model=AppConfig.LLM_MODEL,
            stream=True,
            temperature=0.3 # Gi·ªØ nhi·ªát ƒë·ªô th·∫•p ƒë·ªÉ th√¥ng tin ch√≠nh x√°c
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


# ==============================================================================
# PH·∫¶N 4: GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (UI - STREAMLIT) - B·∫§T DI B·∫§T D·ªäCH
# ==============================================================================

def main():
    st.set_page_config(
        page_title=AppConfig.PAGE_TITLE,
        page_icon=AppConfig.PAGE_ICON,
        layout="wide"
    )
    
    # CSS Customization (Gi·ªØ nguy√™n ho·∫∑c th√™m ch√∫t hi·ªáu ·ª©ng ƒë·∫πp)
    st.markdown("""
    <style>
        .stChatMessage {border-radius: 10px; border: 1px solid #e0e0e0;}
        .stMarkdown h3 {color: #2e86c1;}
    </style>
    """, unsafe_allow_html=True)

    # Sidebar: C·∫•u h√¨nh v√† Upload
    with st.sidebar:
        st.image(AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "https://img.icons8.com/clouds/200/robot.png", width=150)
        st.title("‚öôÔ∏è C·∫•u h√¨nh h·ªá th·ªëng")
        
        api_key = st.text_input("Nh·∫≠p Groq API Key:", type="password")
        
        st.divider()
        st.subheader("üìö Qu·∫£n l√Ω D·ªØ li·ªáu H·ªçc t·∫≠p")
        uploaded_files = st.file_uploader("N·∫°p S√°ch Gi√°o Khoa (PDF)", accept_multiple_files=True, type=['pdf'])
        
        process_btn = st.button("üöÄ Kh·ªüi t·∫°o & Index D·ªØ li·ªáu")
        
        # Tr·∫°ng th√°i h·ªá th·ªëng
        if os.path.exists(AppConfig.DB_DIR):
            st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")
            st.info(f"Engine: Hybrid Search + Rerank")
        else:
            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng n·∫°p SGK.")

    # Main Chat Interface
    st.title(f"{AppConfig.PAGE_ICON} {AppConfig.PAGE_TITLE}")
    st.caption("üöÄ H·ªá th·ªëng h·ªèi ƒë√°p ki·∫øn th·ª©c Tin h·ªçc THPT s·ª≠ d·ª•ng c√¥ng ngh·ªá Advanced RAG (ViSEF 2025)")

    # Kh·ªüi t·∫°o Session State
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Ch√†o em! Th·∫ßy l√† tr·ª£ l√Ω ·∫£o Tin h·ªçc. Em c·∫ßn t√¨m hi·ªÉu ki·∫øn th·ª©c l·ªõp 10, 11 hay 12?"}]
    
    if "rag_engine" not in st.session_state:
        st.session_state.rag_engine = None

    # Logic x·ª≠ l√Ω Upload & Build DB
    if process_btn and uploaded_files and api_key:
        AppConfig.ensure_directories()
        
        # Save files t·∫°m
        file_paths = []
        for uploaded_file in uploaded_files:
            path = os.path.join(AppConfig.DATA_DIR, uploaded_file.name)
            with open(path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            file_paths.append(path)
        
        # Init DB Manager
        db_manager = VectorDBManager()
        success = db_manager.build_database(file_paths)
        
        if success:
            st.session_state.rag_engine = AdvancedRAGEngine(db_manager, api_key)
            st.toast("Hu·∫•n luy·ªán d·ªØ li·ªáu th√†nh c√¥ng!", icon="üéâ")
            st.rerun()

    # Th·ª≠ load l·∫°i n·∫øu ch∆∞a c√≥ engine nh∆∞ng ƒë√£ c√≥ DB
    if st.session_state.rag_engine is None and api_key and os.path.exists(AppConfig.DB_DIR):
        db_manager = VectorDBManager()
        if db_manager.load_database():
            st.session_state.rag_engine = AdvancedRAGEngine(db_manager, api_key)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # X·ª≠ l√Ω input ng∆∞·ªùi d√πng
    if user_input := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (VD: Tin 10 b√†i danh s√°ch)..."):
        if not st.session_state.rag_engine:
            st.error("Vui l√≤ng nh·∫≠p API Key v√† n·∫°p d·ªØ li·ªáu tr∆∞·ªõc!")
            return

        # Hi·ªÉn th·ªã c√¢u h·ªèi
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        # AI tr·∫£ l·ªùi (Streaming)
        with st.chat_message("assistant", avatar="ü§ñ"):
            response_placeholder = st.empty()
            full_response = ""
            
            # G·ªçi Engine
            try:
                # Hi·ªÉn th·ªã spinner t√¨m ki·∫øm ƒë·ªÉ tƒÉng tr·∫£i nghi·ªám UX
                with st.spinner("ƒêang ƒë·ªãnh tuy·∫øn & tra c·ª©u t√†i li·ªáu SGK..."):
                    response_gen = st.session_state.rag_engine.generate_response(user_input)
                
                for chunk in response_gen:
                    full_response += chunk
                    response_placeholder.markdown(full_response + "‚ñå")
                
                response_placeholder.markdown(full_response)
            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói: {str(e)}")
                full_response = f"L·ªói h·ªá th·ªëng: {str(e)}"

        st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    if not DEPENDENCIES_OK:
        st.error(f"Thi·∫øu th∆∞ vi·ªán h·ªá th·ªëng: {IMPORT_ERROR}")
        st.stop()
    main()