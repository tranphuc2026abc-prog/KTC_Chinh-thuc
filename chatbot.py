import os
import glob
import base64
import streamlit as st
import shutil
import re
import uuid
import hashlib
import time
from typing import List, Generator

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply()  # B·∫Øt bu·ªôc cho LlamaParse ch·∫°y trong Streamlit
    from llama_parse import LlamaParse

    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    # Rerank optimization
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)


class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant'

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD"

    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Parameters
    RETRIEVAL_K = 30
    FINAL_K = 5  # Gi·∫£m xu·ªëng 5 ƒë·ªÉ t·∫≠p trung ƒë·ªô ch√≠nh x√°c

    # Hybrid Search Weights
    BM25_WEIGHT = 0.4
    FAISS_WEIGHT = 0.6

    LLM_TEMPERATURE = 0.0  # B·∫ÆT BU·ªòC = 0.0 ƒë·ªÉ tri·ªát ti√™u s√°ng t·∫°o ·∫£o gi√°c


# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER )
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .header-left p {
                color: #e0fbfc; margin: 5px 0 0 0; font-size: 1rem; opacity: 0.9;
            }
            /* Style cho Citation chu·∫©n KHKT - D·∫°ng Badge */
            .citation-badge {
                font-size: 0.75em;
                color: white; 
                background-color: #0077b6; /* Xanh chu·∫©n SGK */
                padding: 3px 8px;
                border-radius: 12px;
                font-weight: 600;
                margin-left: 5px;
                display: inline-flex;
                align-items: center;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                border: 1px solid rgba(255,255,255,0.2);
            }
            /* Chat Message */
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p T·ªânh</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)

            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                st.session_state.pop('retriever_engine', None)
                st.toast("ƒê√£ x√≥a cache. Vui l√≤ng reload trang!", icon="‚úÖ")
                time.sleep(1)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" style="width:100px; height:100px; border-radius:50%; border:3px solid rgba(255,255,255,0.3); object-fit:cover; box-shadow: 0 4px 10px rgba(0,0,0,0.2);">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p style="font-size: 1.1rem; margin-top: 5px;">H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)


# ==================================
# 3. LOGIC BACKEND - VERIFIABLE HYBRID RAG
# ==================================

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key: return None
            return Groq(api_key=api_key)
        except Exception: return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception: return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception: return None

    @staticmethod
    def _structural_chunking(text: str, source_meta: dict) -> List[Document]:
        """
        C·∫Øt chunk th√¥ng minh c√≥ b·∫Øt s·ªë trang (Page Extraction)
        """
        lines = text.split('\n')
        chunks = []

        # Default Tracking
        current_chapter = "Ch∆∞∆°ng m·ªü ƒë·∫ßu"
        current_lesson = "B√†i m·ªü ƒë·∫ßu"
        current_section = "N·ªôi dung chi ti·∫øt"
        current_page = "N/A" # Placeholder

        buffer = []

        # Regex patterns
        p_chapter = re.compile(r'^#*\s*\**\s*(CH∆Ø∆†NG|Ch∆∞∆°ng)\s+([IVX0-9]+).*$', re.IGNORECASE)
        p_lesson = re.compile(r'^#*\s*\**\s*(B√ÄI|B√†i)\s+([0-9]+).*$', re.IGNORECASE)
        p_section = re.compile(r'^(###\s+|[IV0-9]+\.\s+|[a-z]\)\s+).*')
        # Regex gi·∫£ ƒë·ªãnh b·∫Øt s·ªë trang n·∫øu LlamaParse tr·∫£ v·ªÅ d·∫°ng "--- Page 1 ---" ho·∫∑c "Trang 1"
        p_page = re.compile(r'^-+\s*(Page|Trang)\s*(\d+)\s*-+$', re.IGNORECASE)

        def clean_header(text): return text.replace('#', '').replace('*', '').strip()

        def commit_chunk(buf, meta, page):
            if not buf: return
            content = "\n".join(buf).strip()
            if len(content) < 20: return 

            # Create Deterministic UID based on content hash
            hash_input = (meta.get("source", "") + str(page) + content[:50]).encode('utf-8')
            chunk_hash = hashlib.sha256(hash_input).hexdigest()[:8]

            new_meta = meta.copy()
            new_meta.update({
                "chunk_uid": chunk_hash,
                "chapter": current_chapter,
                "lesson": current_lesson,
                "section": current_section,
                "page": page
            })
            chunks.append(Document(page_content=content, metadata=new_meta))

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped: continue

            # Detect Page Break
            if p_page.match(line_stripped):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []
                current_page = p_page.match(line_stripped).group(2)
                continue

            if p_chapter.match(line_stripped):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []; current_chapter = clean_header(line_stripped)
            elif p_lesson.match(line_stripped):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []; current_lesson = clean_header(line_stripped)
            elif p_section.match(line_stripped) or line_stripped.startswith("### "):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []; current_section = clean_header(line_stripped)
            else:
                buffer.append(line)

        commit_chunk(buffer, source_meta, current_page)
        return chunks

    @staticmethod
    def _parse_pdf_with_llama(file_path: str) -> str:
        os.makedirs(AppConfig.PROCESSED_MD_DIR, exist_ok=True)
        file_name = os.path.basename(file_path)
        md_file_path = os.path.join(AppConfig.PROCESSED_MD_DIR, f"{file_name}.md")

        if os.path.exists(md_file_path):
            with open(md_file_path, "r", encoding="utf-8") as f: return f.read()

        llama_api_key = st.secrets.get("LLAMA_CLOUD_API_KEY")
        if not llama_api_key: return "ERROR: Missing LLAMA_CLOUD_API_KEY"

        try:
            parser = LlamaParse(api_key=llama_api_key, result_type="markdown", language="vi")
            documents = parser.load_data(file_path)
            if documents:
                with open(md_file_path, "w", encoding="utf-8") as f: f.write(documents[0].text)
                return documents[0].text
        except Exception: pass
        return ""

    @staticmethod
    def build_hybrid_retriever(embeddings):
        if not embeddings: return None

        # Load Existing Vector DB
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
                return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
            except Exception: pass

        # Process new files
        if not os.path.exists(AppConfig.PDF_DIR): return None
        files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
        if not files: return None

        all_chunks = []
        st_text = st.empty()
        for f in files:
            st_text.text(f"ƒêang x·ª≠ l√Ω: {os.path.basename(f)}...")
            txt = RAGEngine._parse_pdf_with_llama(f)
            if len(txt) > 50:
                chunks = RAGEngine._structural_chunking(txt, {"source": os.path.basename(f)})
                all_chunks.extend(chunks)
        st_text.empty()

        if all_chunks:
            vector_db = FAISS.from_documents(all_chunks, embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)
            return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
        return None

    # =========================================================================
    # STRICT RAG GENERATION LOGIC - LEVEL 2 VERIFICATION
    # =========================================================================
    @staticmethod
    def generate_response(client, retriever, query) -> Generator[str, None, None]:
        if not retriever:
            yield "H·ªá th·ªëng ƒëang kh·ªüi t·∫°o... vui l√≤ng ch·ªù gi√¢y l√°t."
            return

        # --- GIAI ƒêO·∫†N 1: RETRIEVAL & RERANK ---
        initial_docs = retriever.invoke(query)
        
        scored_docs = []
        for doc in initial_docs:
            src = doc.metadata.get('source', '')
            bonus = 1.0 if ("KNTT" in src or "SGK" in src) else 0.0
            scored_docs.append({"doc": doc, "bonus": bonus})

        final_docs = []
        try:
            ranker = RAGEngine.load_reranker()
            if ranker and scored_docs:
                passages = [{"id": str(i), "text": x["doc"].page_content, "meta": x["doc"].metadata} for i, x in enumerate(scored_docs)]
                req = RerankRequest(query=query, passages=passages)
                results = ranker.rank(req)
                
                reranked = []
                for res in results:
                    idx = int(res['id'])
                    final_score = res['score'] + (scored_docs[idx]['bonus'] * 0.1) 
                    reranked.append({"res": res, "score": final_score})
                
                reranked.sort(key=lambda x: x['score'], reverse=True)
                final_docs = [Document(page_content=r['res']['text'], metadata=r['res']['meta']) for r in reranked[:AppConfig.FINAL_K]]
            else:
                scored_docs.sort(key=lambda x: x['bonus'], reverse=True)
                final_docs = [x["doc"] for x in scored_docs[:AppConfig.FINAL_K]]
        except Exception:
            final_docs = [x["doc"] for x in scored_docs[:AppConfig.FINAL_K]]

        if not final_docs:
            yield "Xin l·ªói, hi·ªán t·∫°i c∆° s·ªü d·ªØ li·ªáu SGK ch∆∞a c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y."
            return

        # --- GIAI ƒêO·∫†N 2: X√ÇY D·ª∞NG CONTEXT & REGISTRY (Level 2) ---
        valid_uids = {} 
        context_parts = []
        
        for doc in final_docs:
            uid = doc.metadata.get('chunk_uid')
            if not uid: continue
            
            src_name = doc.metadata.get('source', 'T√†i li·ªáu').replace('.pdf', '')
            lesson = doc.metadata.get('lesson', 'B√†i ?')
            page = doc.metadata.get('page', 'N/A')
            
            display_name = src_name if len(src_name) < 15 else src_name[:12] + "..."
            
            # T·∫°o HTML Badge chu·∫©n KHKT: [T√™n S√°ch > B√†i > Trang]
            page_str = f" - Tr.{page}" if page != "N/A" else ""
            badge_html = f'<span class="citation-badge">üìò {display_name} > {lesson}{page_str}</span>'
            
            valid_uids[uid] = badge_html
            
            context_parts.append(f"--- Document ID: {uid} ---\nSource: {src_name} | Lesson: {lesson} | Page: {page}\nContent: {doc.page_content}\n----------------")

        full_context = "\n".join(context_parts)

        # --- GIAI ƒêO·∫†N 3: PROMPT K·ª∏ THU·∫¨T (Strict Verification) ---
        system_prompt = (
            "B·∫°n l√† Tr·ª£ l√Ω AI gi√°o d·ª•c KHKT nghi√™m ng·∫∑t.\n"
            "NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n Context ƒë∆∞·ª£c cung c·∫•p.\n\n"
            "QUY T·∫ÆC TUY·ªÜT ƒê·ªêI (VI PH·∫†M S·∫º B·ªä TR·ª™ ƒêI·ªÇM):\n"
            "1. KH√îNG S√ÅNG T·∫†O: Ch·ªâ d√πng th√¥ng tin trong Context. N·∫øu kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi, in ra 'NO_INFO'.\n"
            "2. B·∫ÆT BU·ªòC TR√çCH D·∫™N: M·ªçi c√¢u tr·∫£ l·ªùi ph·∫£i k·∫øt th√∫c b·∫±ng th·∫ª ngu·ªìn [ID:uid].\n"
            "   - Sai: Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh.\n"
            "   - ƒê√∫ng: Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh [ID:12ab34cd].\n"
            "3. TRUNG TH·ª∞C: Kh√¥ng ƒë∆∞·ª£c b·ªãa ID kh√¥ng c√≥ trong context.\n"
            "4. NG√îN NG·ªÆ: Ti·∫øng Vi·ªát ph·ªï th√¥ng, s∆∞ ph·∫°m, d·ªÖ hi·ªÉu cho h·ªçc sinh.\n\n"
            f"CONTEXT D·ªÆ LI·ªÜU:\n{full_context}"
        )

        try:
            completion = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": query}],
                temperature=0.0, # Zero Temperature ƒë·ªÉ tri·ªát ti√™u ·∫£o gi√°c
                stream=False
            )
            raw_response = completion.choices[0].message.content.strip()

            # --- GIAI ƒêO·∫†N 4: H·∫¨U KI·ªÇM (VALIDATION LAYER) ---
            
            if "NO_INFO" in raw_response:
                yield "D·ªØ li·ªáu SGK hi·ªán t·∫°i ch∆∞a c√≥ th√¥ng tin ch√≠nh x√°c v·ªÅ c√¢u h·ªèi n√†y."
                return

            pattern = r"\[ID:([a-fA-F0-9]+)\]"
            found_ids = re.findall(pattern, raw_response)
            
            # LU·∫¨T S·∫ÆT: KH√îNG C√ì NGU·ªíN = KH√îNG HI·ªÇN TH·ªä
            if not found_ids:
                yield "‚ö†Ô∏è C√¢u tr·∫£ l·ªùi b·ªã h·ªá th·ªëng ch·∫∑n v√¨ AI kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c ngu·ªìn ch·ª©ng th·ª±c (Verification Fail)."
                return

            # Ki·ªÉm tra ID ·∫£o
            valid_response = True
            invalid_ids = []
            for uid in found_ids:
                if uid not in valid_uids:
                    valid_response = False
                    invalid_ids.append(uid)
            
            if not valid_response:
                yield f"‚ö†Ô∏è H·ªá th·ªëng ph√°t hi·ªán tr√≠ch d·∫´n kh√¥ng h·ª£p l·ªá ({', '.join(invalid_ids)}). C√¢u tr·∫£ l·ªùi b·ªã h·ªßy b·ªè ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c."
                return

            # Thay th·∫ø ID b·∫±ng Badge ƒë·∫πp
            def replace_with_badge(match):
                uid_found = match.group(1)
                return valid_uids.get(uid_found, "")

            final_display = re.sub(pattern, replace_with_badge, raw_response)
            
            yield final_display

        except Exception as e:
            yield f"L·ªói x·ª≠ l√Ω AI: {str(e)}"


# ===================
# 4. MAIN APPLICATION
# ===================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! M√¨nh l√† KTC Chatbot. H√£y h·ªèi m√¨nh v·ªÅ n·ªôi dung SGK Tin h·ªçc nh√©!"}]

    groq_client = RAGEngine.load_groq_client()

    # Kh·ªüi t·∫°o Retriever
    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c s·ªë..."):
            embeddings = RAGEngine.load_embedding_model()
            st.session_state.retriever_engine = RAGEngine.build_hybrid_retriever(embeddings)
    
    # Hi·ªÉn th·ªã Chat
    bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
    for msg in st.session_state.messages:
        role = msg["role"]
        avatar = "üßë‚Äçüéì" if role == "user" else bot_avatar
        with st.chat_message(role, avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True)

    # X·ª≠ l√Ω Input
    if user_input := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=bot_avatar):
            response_placeholder = st.empty()
            full_response = ""
            
            response_gen = RAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                user_input
            )
            
            for chunk in response_gen:
                full_response += chunk
                response_placeholder.markdown(full_response + "‚ñå", unsafe_allow_html=True)
            
            response_placeholder.markdown(full_response, unsafe_allow_html=True)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()