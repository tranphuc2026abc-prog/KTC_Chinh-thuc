import os
import glob
import base64
import streamlit as st
import shutil
import re
import uuid
import time
from typing import List, Tuple, Optional, Dict, Any

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply() # B·∫Øt bu·ªôc cho LlamaParse ch·∫°y trong Streamlit
    from llama_parse import LlamaParse 
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    # Rerank optimization
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) 
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant'

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD" 

    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Parameters - Scientific Standard
    CHUNK_SIZE = 800       # Gi·∫£m size ƒë·ªÉ t·∫≠p trung ng·ªØ nghƒ©a
    CHUNK_OVERLAP = 100    
    RETRIEVAL_K = 30       # L·∫•y r·ªông ƒë·ªÉ BM25 l·ªçc t·ª´ kh√≥a
    FINAL_K = 5            # Top 5 context ch·∫•t l∆∞·ª£ng nh·∫•t sau Rerank
    
    # Hybrid Search Weights
    BM25_WEIGHT = 0.4      
    FAISS_WEIGHT = 0.6     

    LLM_TEMPERATURE = 0.0  # Zero temperature for factual consistency

# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER - GI·ªÆ NGUY√äN 100%) 
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .header-left p {
                color: #e0fbfc; margin: 5px 0 0 0; font-size: 1rem; opacity: 0.9;
            }
            .header-right img {
                border-radius: 50%; border: 3px solid rgba(255,255,255,0.3);
                box-shadow: 0 4px 10px rgba(0,0,0,0.2); width: 100px; height: 100px;
                object-fit: cover;
            }
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
            [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
                background: #e3f2fd; color: #0d47a1;
            }
            [data-testid="stChatMessageContent"]:not(:has(+ [data-testid="stChatMessageAvatar"])) {
                background: white; border: 1px solid #e9ecef;
                border-left: 5px solid #00b4d8;
            }
            div.stButton > button {
                border-radius: 8px; background-color: white; color: #0077b6;
                border: 1px solid #90e0ef; transition: all 0.2s;
            }
            div.stButton > button:hover {
                background-color: #0077b6; color: white;
                border-color: #0077b6; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p T·ªânh</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">NƒÉm h·ªçc:</span>
                        <span style="text-align: right; color: #222;"><b>2025 - 2026</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)

            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                st.session_state.pop('retriever_engine', None)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" alt="Logo">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p style="font-size: 1.1rem; margin-top: 5px;">H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)

# ==============================================================================
# 3. KERNEL KHOA H·ªåC K·ª∏ THU·∫¨T (SCIENTIFIC KERNEL)
# T√°i c·∫•u tr√∫c theo chu·∫©n Verifiable RAG: Semantic Chunking, Metadata Tracing
# ==============================================================================

class SemanticProcessor:
    """X·ª≠ l√Ω ph√¢n r√£ vƒÉn b·∫£n theo ng·ªØ nghƒ©a c·∫•u tr√∫c SGK"""
    
    @staticmethod
    def _extract_grade(filename: str) -> str:
        """Tr√≠ch xu·∫•t kh·ªëi l·ªõp t·ª´ t√™n file (VD: Tin_10.pdf -> 10)"""
        match = re.search(r'(\d+)', filename)
        return match.group(1) if match else "general"

    @staticmethod
    def _detect_topic_heuristics(text: str) -> str:
        tx = text.lower()
        if any(t in tx for t in ["<html", "css", "javascript", "th·∫ª"]): return "Web Dev"
        if any(t in tx for t in ["def ", "import ", "python", "bi·∫øn", "h√†m", "list", "dict"]): return "Python Programming"
        if any(t in tx for t in ["sql", "primary key", "csdl", "b·∫£ng", "truy v·∫•n", "kh√≥a ch√≠nh"]): return "Database"
        if any(t in tx for t in ["m·∫°ng", "internet", "giao th·ª©c", "iot", "robot"]): return "Network & IoT"
        return "General CS"

    @staticmethod
    def semantic_chunking(markdown_text: str, source_filename: str) -> List[Document]:
        """
        Chi·∫øn thu·∫≠t Chunking ƒëa t·∫ßng (Hierarchical Semantic Chunking):
        - C·∫Øt theo Header Markdown (#, ##, ###) ƒë·ªÉ gi·ªØ nguy√™n v·∫πn ng·ªØ c·∫£nh b√†i h·ªçc.
        - G·∫Øn Metadata chi ti·∫øt (Chapter, Lesson, Chunk ID) ƒë·ªÉ truy v·∫øt.
        """
        headers_to_split_on = [
            ("#", "chapter"),
            ("##", "lesson"),
            ("###", "section"),
        ]
        
        # 1. C·∫Øt c·∫•u tr√∫c b·∫±ng LangChain Markdown splitter
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        md_header_splits = markdown_splitter.split_text(markdown_text)
        
        # 2. C·∫Øt m·ªãn n·ªôi dung n·∫øu qu√° d√†i (Recursive) nh∆∞ng gi·ªØ metadata
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " "],
            add_start_index=True
        )
        
        final_chunks = []
        grade = SemanticProcessor._extract_grade(source_filename)
        
        for doc in md_header_splits:
            splits = text_splitter.split_documents([doc])
            for split in splits:
                # B·ªï sung Rich Metadata cho Scientific Verification
                meta = split.metadata
                meta["source_file"] = source_filename
                meta["grade"] = grade
                meta["topic"] = SemanticProcessor._detect_topic_heuristics(split.page_content)
                meta["chunk_uid"] = str(uuid.uuid4())[:8] # ƒê·ªãnh danh duy nh·∫•t cho chunk (Short UUID)
                
                # T·∫°o format hi·ªÉn th·ªã tr√≠ch d·∫´n ƒë·∫πp
                citation = f"{meta.get('source_file', '')}"
                if 'lesson' in meta: citation += f" - {meta['lesson']}"
                meta["citation_str"] = citation
                
                final_chunks.append(split)
                
        return final_chunks

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key: return None
            return Groq(api_key=api_key)
        except Exception: return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception: return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception: return None

    @staticmethod
    def _parse_pdf_with_llama(file_path: str) -> str:
        # C∆° ch·∫ø Caching Markdown ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô demo
        os.makedirs(AppConfig.PROCESSED_MD_DIR, exist_ok=True)
        file_name = os.path.basename(file_path)
        md_file_path = os.path.join(AppConfig.PROCESSED_MD_DIR, f"{file_name}.md")
        
        if os.path.exists(md_file_path):
            with open(md_file_path, "r", encoding="utf-8") as f:
                return f.read()
        
        llama_api_key = st.secrets.get("LLAMA_CLOUD_API_KEY")
        if not llama_api_key: return "ERROR: Missing API Key"

        try:
            # LlamaParse mode "markdown" t·ªëi ∆∞u cho vi·ªác gi·ªØ c·∫•u tr√∫c b·∫£ng v√† code
            parser = LlamaParse(
                api_key=llama_api_key,
                result_type="markdown",
                language="vi",
                verbose=True,
                parsing_instruction="ƒê√¢y l√† t√†i li·ªáu gi√°o khoa Tin h·ªçc. H√£y gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng b·∫£ng bi·ªÉu, code block v√† c√¥ng th·ª©c to√°n h·ªçc. Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ ch∆∞∆°ng m·ª•c r√µ r√†ng b·∫±ng d·∫•u #"
            )
            documents = parser.load_data(file_path)
            markdown_text = documents[0].text
            
            with open(md_file_path, "w", encoding="utf-8") as f:
                f.write(markdown_text)
            return markdown_text
        except Exception as e:
            return f"Error: {str(e)}"

    @staticmethod
    def build_hybrid_retriever(embeddings):
        """
        X√¢y d·ª±ng Hybrid Retriever (FAISS + BM25)
        ƒê√¢y l√† ti√™u chu·∫©n v√†ng cho RAG hi·ªán ƒë·∫°i:
        - BM25: B·∫Øt ch√≠nh x√°c t·ª´ kh√≥a chuy√™n ng√†nh (Keyword Match).
        - FAISS: B·∫Øt ng·ªØ nghƒ©a, kh√°i ni·ªám t∆∞∆°ng ƒë·ªìng (Semantic Match).
        """
        if not embeddings: return None

        vector_db = None
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            except Exception: pass

        if not vector_db:
            if not os.path.exists(AppConfig.PDF_DIR):
                os.makedirs(AppConfig.PDF_DIR)
                return None
            
            pdf_files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
            if not pdf_files: return None
            
            all_chunks = []
            status_text = st.empty()

            for file_path in pdf_files:
                source_file = os.path.basename(file_path)
                status_text.text(f"ƒêang x·ª≠ l√Ω c·∫•u tr√∫c: {source_file}...")
                
                # 1. Parse PDF -> Markdown
                markdown_content = RAGEngine._parse_pdf_with_llama(file_path)
                
                if "Error" not in markdown_content and len(markdown_content) > 50:
                    # 2. Semantic Chunking (Advanced)
                    file_chunks = SemanticProcessor.semantic_chunking(markdown_content, source_file)
                    all_chunks.extend(file_chunks)
                else:
                    # Fallback n·∫øu LlamaParse l·ªói
                    try:
                        from pypdf import PdfReader
                        reader = PdfReader(file_path)
                        text = "".join([p.extract_text() for p in reader.pages])
                        all_chunks.append(Document(page_content=text, metadata={"source": source_file, "chunk_uid": "fallback"}))
                    except: pass
            
            status_text.empty()
            if not all_chunks: return None

            vector_db = FAISS.from_documents(all_chunks, embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)

        # T·∫°o Ensemble Retriever
        try:
            # L·∫•y docs t·ª´ VectorStore ƒë·ªÉ d·ª±ng BM25
            # L∆∞u √Ω: docstore._dict l√† implementation details c·ªßa Langchain FAISS
            docstore_docs = list(vector_db.docstore._dict.values())
            bm25_retriever = BM25Retriever.from_documents(docstore_docs)
            bm25_retriever.k = AppConfig.RETRIEVAL_K

            faiss_retriever = vector_db.as_retriever(
                search_type="mmr", # Maximal Marginal Relevance ƒë·ªÉ ƒëa d·∫°ng h√≥a k·∫øt qu·∫£
                search_kwargs={"k": AppConfig.RETRIEVAL_K, "lambda_mult": 0.5}
            )

            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[AppConfig.BM25_WEIGHT, AppConfig.FAISS_WEIGHT]
            )
            return ensemble_retriever
        except Exception:
            return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})

    @staticmethod
    def validate_grounding(response_text: str, context_docs: List[Document]) -> Tuple[bool, str]:
        """
        K·ªπ thu·∫≠t Post-Generation Verification (Ki·ªÉm ch·ª©ng sau sinh):
        Ki·ªÉm tra xem model c√≥ b·ªãa ra ngu·ªìn kh√¥ng.
        """
        valid_uids = [doc.metadata.get('chunk_uid') for doc in context_docs if doc.metadata.get('chunk_uid')]
        
        # ƒê∆°n gi·∫£n h√≥a cho Demo: Ki·ªÉm tra xem c√≥ √≠t nh·∫•t 1 t·ª´ kh√≥a chuy√™n ng√†nh kh·ªõp kh√¥ng
        # Ho·∫∑c ki·ªÉm tra xem model c√≥ th·ª±c s·ª± ƒë∆∞a ra th√¥ng tin li√™n quan kh√¥ng.
        # ·ªû m·ª©c ƒë·ªô KHKT THPT, ta d√πng heuristic: N·∫øu response qu√° ng·∫Øn ho·∫∑c kh√¥ng c√≥ n·ªôi dung tin h·ªçc -> Warning.
        
        if "kh√¥ng t√¨m th·∫•y" in response_text.lower():
            return False, "No Info"
        
        return True, "Verified"

    @staticmethod
    def generate_response(client, retriever, query):
        if not retriever:
            return ["H·ªá th·ªëng ƒëang kh·ªüi t·∫°o... vui l√≤ng ch·ªù gi√¢y l√°t."], []
        
        # 1. Retrieval (Thu th·∫≠p)
        initial_docs = retriever.invoke(query)
        
        # 2. Reranking (S·∫Øp x·∫øp l·∫°i theo ƒë·ªô ph√π h·ª£p ng·ªØ nghƒ©a s√¢u)
        final_docs = []
        try:
            ranker = RAGEngine.load_reranker()
            if ranker and initial_docs:
                passages = [
                    {"id": str(i), "text": d.page_content, "meta": d.metadata} 
                    for i, d in enumerate(initial_docs)
                ]
                rerank_req = RerankRequest(query=query, passages=passages)
                results = ranker.rank(rerank_req)
                
                # Ch·ªâ l·∫•y Top K sau rerank
                for res in results[:AppConfig.FINAL_K]:
                    final_docs.append(Document(page_content=res["text"], metadata=res["meta"]))
            else:
                final_docs = initial_docs[:AppConfig.FINAL_K]
        except Exception:
            final_docs = initial_docs[:AppConfig.FINAL_K]

        if not final_docs:
            return ["Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong d·ªØ li·ªáu SGK ƒë·ªÉ tr·∫£ l·ªùi."], []

        # 3. Context Construction (T·∫°o ng·ªØ c·∫£nh c√≥ c·∫•u tr√∫c)
        context_parts = []
        sources_list = []
        
        for i, doc in enumerate(final_docs):
            meta = doc.metadata
            uid = meta.get('chunk_uid', 'N/A')
            citation = meta.get('citation_str', meta.get('source_file', 'TaiLieu'))
            
            # Context block c√≥ ID ƒë·ªÉ model tham chi·∫øu
            context_parts.append(f"--- DOCUMENT ID: {uid} ---\n[Ngu·ªìn: {citation}]\n{doc.page_content}\n")
            
            sources_list.append(f"{citation}")
        
        full_context = "\n".join(context_parts)

        # 4. Strict Prompt Engineering (Ch·ªëng ·∫£o gi√°c)
        # Y√™u c·∫ßu model ho·∫°t ƒë·ªông nh∆∞ m·ªôt m√°y tr√≠ch xu·∫•t th√¥ng tin ch√≠nh x√°c.
        system_prompt = f"""B·∫°n l√† KTC Chatbot, tr·ª£ l√Ω AI h·ªó tr·ª£ m√¥n Tin h·ªçc.
NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n [CONTEXT] ƒë∆∞·ª£c cung c·∫•p.

QUY T·∫ÆC C·ªêT L√ïI (STRICT RULES):
1. **Grounding:** Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong [CONTEXT]. Tuy·ªát ƒë·ªëi KH√îNG s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i SGK.
2. **Citation:** Khi ƒë∆∞a ra m·ªôt kh·∫≥ng ƒë·ªãnh, h√£y c·ªë g·∫Øng tham chi·∫øu.
3. **Honesty:** N·∫øu [CONTEXT] kh√¥ng ch·ª©a c√¢u tr·∫£ l·ªùi, h√£y n√≥i: "D·ªØ li·ªáu SGK hi·ªán t·∫°i ch∆∞a c·∫≠p nh·∫≠t th√¥ng tin n√†y".
4. **Format:** Tr√¨nh b√†y code trong ```python/cpp``` block. D√πng Markdown cho ti√™u ƒë·ªÅ.

[CONTEXT B·∫ÆT ƒê·∫¶U]
{full_context}
[CONTEXT K·∫æT TH√öC]
"""

        try:
            stream = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                stream=True,
                temperature=AppConfig.LLM_TEMPERATURE, # Nhi·ªát ƒë·ªô 0 ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
                max_tokens=1500
            )
            return stream, list(set(sources_list)) # Tr·∫£ v·ªÅ unique sources
        except Exception as e:
            return [f"L·ªói API: {str(e)}"], []

# ===================
# 4. MAIN APPLICATION
# ===================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! KTC Chatbot s·∫µn s√†ng h·ªó tr·ª£ tra c·ª©u ki·∫øn th·ª©c."}]

    groq_client = RAGEngine.load_groq_client()

    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c s·ªë (Semantic Parsing + Hybrid RAG)..."):
            embeddings = RAGEngine.load_embedding_model()
            st.session_state.retriever_engine = RAGEngine.build_hybrid_retriever(embeddings)
            if st.session_state.retriever_engine:
                st.toast("‚úÖ D·ªØ li·ªáu SGK ƒë√£ s·∫µn s√†ng!", icon="üìö")
            else:
                st.toast("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu PDF trong th∆∞ m·ª•c PDF_KNOWLEDGE", icon="üìÇ")

    for msg in st.session_state.messages:
        bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else bot_avatar
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y...")
    
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"):
            response_placeholder = st.empty()
            
            stream, sources = RAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                user_input
            )

            full_response = ""
            if isinstance(stream, list):
                full_response = stream[0]
                response_placeholder.markdown(full_response)
            else:
                for chunk in stream:
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                        response_placeholder.markdown(full_response + "‚ñå")
                response_placeholder.markdown(full_response)

            # --- VERIFICATION DISPLAY ---
            # Hi·ªÉn th·ªã ngu·ªìn x√°c th·ª±c ngay d∆∞·ªõi c√¢u tr·∫£ l·ªùi (T√≠nh nƒÉng KHKT)
            if sources:
                with st.expander("‚úÖ Ngu·ªìn x√°c th·ª±c (Verified Sources)", expanded=False):
                    st.markdown("H·ªá th·ªëng ƒë√£ tham chi·∫øu c√°c t√†i li·ªáu sau:")
                    for src in sources:
                        st.markdown(f"- üìñ *{src}*")
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()