import os
import streamlit as st
import shutil
import re
import uuid
import unicodedata
import pickle # Th√™m pickle ƒë·ªÉ l∆∞u/ƒë·ªçc cache BM25
from pathlib import Path
from typing import List

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- KH·ªêI IMPORT AN TO√ÄN (Tr√°nh crash n·∫øu thi·∫øu th∆∞ vi·ªán ph·ª•) ---
try:
    import nest_asyncio
    nest_asyncio.apply()
    from llama_parse import LlamaParse
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    
    # Flashrank (T√πy ch·ªçn)
    try:
        from flashrank import Ranker, RerankRequest
        HAS_FLASHRANK = True
    except ImportError:
        HAS_FLASHRANK = False
        
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# =============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# =============================

class AppConfig:
    # --- ƒêI·ªÄN API KEY C·ª¶A TH·∫¶Y V√ÄO ƒê√ÇY ---
    GROQ_API_KEY = "gsk_..."  # Thay b·∫±ng key th·∫≠t c·ªßa th·∫ßy
    LLAMA_CLOUD_API_KEY = "llx-..." # Thay b·∫±ng key th·∫≠t (n·∫øu d√πng LlamaParse)
    
    LLM_MODEL = "llama-3.3-70b-versatile"
    EMBEDDING_MODEL = "dangvantuan/vietnamese-embedding"
    
    UPLOAD_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_DIR = "FAISS_DB"
    BM25_PATH = os.path.join(VECTOR_DB_DIR, "bm25_docs.pkl")
    LOGO_PROJECT = "LOGO.jpg"
    
    SYSTEM_PROMPT = """B·∫°n l√† Tr·ª£ l√Ω h·ªçc t·∫≠p m√¥n Tin h·ªçc, h·ªó tr·ª£ gi√°o vi√™n v√† h·ªçc sinh tr∆∞·ªùng Ph·∫°m Ki·ªát theo SGK K·∫øt n·ªëi tri th·ª©c.
    
    QUY T·∫ÆC TR·∫¢ L·ªúI:
    1. CƒÉn c·ª© CH√çNH X√ÅC v√†o ng·ªØ c·∫£nh (Context) ƒë∆∞·ª£c cung c·∫•p.
    2. N·∫øu ng·ªØ c·∫£nh c√≥ th√¥ng tin: Tr·∫£ l·ªùi chi ti·∫øt, s∆∞ ph·∫°m, d·ªÖ hi·ªÉu.
    3. TR√çCH D·∫™N NGU·ªíN: B·∫Øt bu·ªôc ghi r√µ (S√°ch n√†o -> Ch·ªß ƒë·ªÅ n√†o -> B√†i n√†o).
    4. N·∫øu kh√¥ng c√≥ th√¥ng tin: Tr·∫£ l·ªùi "D·ª±a tr√™n t√†i li·ªáu SGK hi·ªán c√≥, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin n√†y."
    """

# =============================
# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU (FIX LOGIC KNTT)
# =============================

class KNTT_Processor:
    @staticmethod
    def normalize_text(text: str) -> str:
        if not text: return ""
        text = unicodedata.normalize("NFC", text)
        return re.sub(r'\s+', ' ', text).strip()

    @staticmethod
    def parse_kntt_structure(raw_text: str, filename: str) -> List[Document]:
        """
        Ph√¢n t√≠ch c·∫•u tr√∫c KNTT: T√™n s√°ch -> Ch·ªß ƒë·ªÅ -> B√†i.
        Regex ƒë∆∞·ª£c c·∫£i ti·∫øn ƒë·ªÉ b·∫Øt c·∫£ Markdown (## Ch·ªß ƒë·ªÅ...)
        """
        lines = raw_text.split('\n')
        docs = []
        
        # Regex c·∫£i ti·∫øn: B·∫Øt ch·∫•p nh·∫≠n d·∫•u #, *, kho·∫£ng tr·∫Øng ·ªü ƒë·∫ßu d√≤ng
        # B·∫Øt: "## Ch·ªß ƒë·ªÅ 1:", "**Ch·ªß ƒë·ªÅ A**", "Ch·ªß ƒë·ªÅ 1."
        topic_pattern = re.compile(r'^[\#\*\s]*(?:Ch·ªß ƒë·ªÅ|CH·ª¶ ƒê·ªÄ)\s+([0-9A-Za-z]+)(?:[:\.]|\s+)(.+?)(?:[\#\*]*)$', re.IGNORECASE)
        
        # B·∫Øt: "### B√†i 1:", "B√†i 5.", "**B√†i 17**"
        lesson_pattern = re.compile(r'^[\#\*\s]*(?:B√†i|B√ÄI)\s+([0-9]+)(?:[:\.]|\s+)(.+?)(?:[\#\*]*)$', re.IGNORECASE)

        current_topic = "Ch∆∞a x√°c ƒë·ªãnh"
        current_lesson = "Ch∆∞a x√°c ƒë·ªãnh"
        
        # Bi·∫øn c·ªù (Flag) ƒë·ªÉ bi·∫øt ƒë√£ v√†o v√πng n·ªôi dung h·ª£p l·ªá ch∆∞a
        in_valid_section = False 
        
        buffer = []
        source_name = os.path.splitext(filename)[0]

        def flush_buffer():
            if buffer and in_valid_section:
                content = "\n".join(buffer).strip()
                if len(content) > 50: # Ch·ªâ l∆∞u n·∫øu n·ªôi dung ƒë·ªß d√†i > 50 k√Ω t·ª±
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": source_name,
                            "topic": current_topic,
                            "lesson": current_lesson,
                            "chunk_uid": str(uuid.uuid4())
                        }
                    )
                    docs.append(doc)

        for line in lines:
            clean_line = KNTT_Processor.normalize_text(line)
            if not clean_line: continue

            # Ki·ªÉm tra Ch·ªß ƒë·ªÅ
            topic_match = topic_pattern.match(clean_line)
            if topic_match:
                flush_buffer() # L∆∞u n·ªôi dung b√†i c≈©
                t_id = topic_match.group(1).strip()
                t_name = topic_match.group(2).strip()
                current_topic = f"Ch·ªß ƒë·ªÅ {t_id}: {t_name}"
                current_lesson = "ƒêang ch·ªù b√†i..."
                in_valid_section = False # Reset flag, ch·ªù g·∫∑p B√†i m·ªõi b·∫≠t l√™n
                buffer = []
                continue

            # Ki·ªÉm tra B√†i
            lesson_match = lesson_pattern.match(clean_line)
            if lesson_match:
                flush_buffer()
                l_id = lesson_match.group(1).strip()
                l_name = lesson_match.group(2).strip()
                current_lesson = f"B√†i {l_id}: {l_name}"
                
                # QUAN TR·ªåNG: Ch·ªâ khi c√≥ Ch·ªß ƒë·ªÅ V√Ä B√†i th√¨ m·ªõi b·∫≠t c·ªù ghi d·ªØ li·ªáu
                if "Ch∆∞a x√°c ƒë·ªãnh" not in current_topic:
                    in_valid_section = True
                buffer = []
                continue

            # Ch·ªâ l∆∞u n·ªôi dung n·∫øu ƒëang ·ªü trong v√πng h·ª£p l·ªá (Topic + Lesson)
            if in_valid_section:
                buffer.append(clean_line)
        
        flush_buffer() # L∆∞u ƒëo·∫°n cu·ªëi
        return docs

class VectorStoreManager:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)

    def build_db(self, uploaded_files):
        if not os.path.exists(AppConfig.UPLOAD_DIR): os.makedirs(AppConfig.UPLOAD_DIR)
        
        all_docs = []
        status = st.empty()
        progress_bar = st.progress(0)

        for i, file in enumerate(uploaded_files):
            status.text(f"‚è≥ ƒêang ƒë·ªçc file: {file.name}...")
            file_path = os.path.join(AppConfig.UPLOAD_DIR, file.name)
            with open(file_path, "wb") as f: f.write(file.getbuffer())

            # 1. Parse PDF
            # L∆ØU √ù: Th·∫ßy c·∫ßn c√†i ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng LLAMA_CLOUD_API_KEY ho·∫∑c set tr·ª±c ti·∫øp
            if AppConfig.LLAMA_CLOUD_API_KEY.startswith("llx-"):
                os.environ["LLAMA_CLOUD_API_KEY"] = AppConfig.LLAMA_CLOUD_API_KEY
                
            try:
                parser = LlamaParse(result_type="markdown", language="vi") # D√πng markdown ƒë·ªÉ gi·ªØ c·∫•u tr√∫c t·ªët h∆°n
                documents = parser.load_data(file_path)
                raw_text = documents[0].text
            except Exception as e:
                st.error(f"L·ªói LlamaParse file {file.name}: {e}")
                continue

            # 2. X·ª≠ l√Ω Logic KNTT
            status.text(f"‚öôÔ∏è ƒêang c·∫•u tr√∫c h√≥a: {file.name}...")
            kntt_docs = KNTT_Processor.parse_kntt_structure(raw_text, file.name)
            
            if not kntt_docs:
                st.warning(f"‚ö†Ô∏è File {file.name}: Kh√¥ng t√¨m th·∫•y c·∫•u tr√∫c 'Ch·ªß ƒë·ªÅ -> B√†i'. Ki·ªÉm tra l·∫°i file PDF.")
                continue

            # 3. Chia nh·ªè chunk
            chunks = self.text_splitter.split_documents(kntt_docs)
            all_docs.extend(chunks)
            progress_bar.progress((i + 1) / len(uploaded_files))

        if not all_docs:
            st.error("‚ùå Kh√¥ng t·∫°o ƒë∆∞·ª£c d·ªØ li·ªáu n√†o h·ª£p l·ªá! Vui l√≤ng ki·ªÉm tra file PDF ƒë·∫ßu v√†o.")
            return None

        # 4. L∆∞u Vector DB & BM25
        status.text("üíæ ƒêang l∆∞u v√†o b·ªô nh·ªõ...")
        if not os.path.exists(AppConfig.VECTOR_DB_DIR): os.makedirs(AppConfig.VECTOR_DB_DIR)
        
        # Save FAISS
        db = FAISS.from_documents(all_docs, self.embeddings)
        db.save_local(AppConfig.VECTOR_DB_DIR)
        
        # Save BM25 Docs (Pickle)
        with open(AppConfig.BM25_PATH, "wb") as f:
            pickle.dump(all_docs, f)

        status.empty()
        progress_bar.empty()
        return db

    def load_db(self):
        if os.path.exists(AppConfig.VECTOR_DB_DIR) and os.path.exists(os.path.join(AppConfig.VECTOR_DB_DIR, "index.faiss")):
            return FAISS.load_local(AppConfig.VECTOR_DB_DIR, self.embeddings, allow_dangerous_deserialization=True)
        return None

# =============================
# 3. RAG ENGINE (HYBRID SEARCH)
# =============================

class RAGEngine:
    @staticmethod
    def get_retriever(vector_db):
        # 1. FAISS Retriever
        faiss_retriever = vector_db.as_retriever(search_kwargs={"k": 4})
        
        # 2. BM25 Retriever
        bm25_retriever = None
        if os.path.exists(AppConfig.BM25_PATH):
            try:
                with open(AppConfig.BM25_PATH, "rb") as f:
                    docs = pickle.load(f)
                bm25_retriever = BM25Retriever.from_documents(docs)
                bm25_retriever.k = 4
            except Exception:
                pass
        
        # 3. Ensemble
        if bm25_retriever:
            return EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[0.4, 0.6]
            )
        return faiss_retriever

    @staticmethod
    def generate_response(client, retriever, query):
        # A. Truy xu·∫•t
        docs = retriever.invoke(query)
        
        # B. Rerank (N·∫øu c√≥ th∆∞ vi·ªán Flashrank)
        if HAS_FLASHRANK and docs:
            try:
                ranker = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir="./opt")
                rerank_req = RerankRequest(query=query, passages=[
                    {"id": d.metadata.get("chunk_uid", "0"), "text": d.page_content, "meta": d.metadata} 
                    for d in docs
                ])
                results = ranker.rank(rerank_req)
                # L·∫•y top 3 v√† map l·∫°i format document
                final_results = results[:3]
                context_str = ""
                for r in final_results:
                    meta = r['meta']
                    src = f"{meta.get('source')} ‚Üí {meta.get('topic')} ‚Üí {meta.get('lesson')}"
                    context_str += f"\n[Ngu·ªìn: {src}]\nN·ªôi dung: {r['text']}\n---\n"
            except Exception as e:
                # Fallback n·∫øu l·ªói rerank
                context_str = "\n---\n".join([f"[Ngu·ªìn: {d.metadata.get('source')} -> {d.metadata.get('topic')} -> {d.metadata.get('lesson')}]\n{d.page_content}" for d in docs[:3]])
        else:
            context_str = "\n---\n".join([f"[Ngu·ªìn: {d.metadata.get('source')} -> {d.metadata.get('topic')} -> {d.metadata.get('lesson')}]\n{d.page_content}" for d in docs[:3]])

        # C. T·∫°o Prompt
        full_prompt = f"""{AppConfig.SYSTEM_PROMPT}
        
        D·ªÆ LI·ªÜU THAM KH·∫¢O:
        {context_str}
        
        C√ÇU H·ªéI: {query}
        """

        # D. G·ªçi LLM Stream
        try:
            chat_completion = client.chat.completions.create(
                messages=[{"role": "user", "content": full_prompt}],
                model=AppConfig.LLM_MODEL,
                stream=True,
            )
            for chunk in chat_completion:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"L·ªói k·∫øt n·ªëi LLM: {str(e)}"

# =============================
# 4. GIAO DI·ªÜN CH√çNH (UI)
# =============================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ùå L·ªói th∆∞ vi·ªán: {IMPORT_ERROR}. Vui l√≤ng ki·ªÉm tra requirements.txt")
        st.stop()

    # --- Sidebar ---
    with st.sidebar:
        st.image(AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "https://via.placeholder.com/150", width=100)
        st.title("üóÇÔ∏è QU·∫¢N L√ù D·ªÆ LI·ªÜU")
        
        uploaded_files = st.file_uploader("N·∫°p SGK (PDF)", type=["pdf"], accept_multiple_files=True)
        
        if st.button("üöÄ X√¢y d·ª±ng Tri th·ª©c (Build RAG)"):
            if not uploaded_files:
                st.warning("Vui l√≤ng ch·ªçn file PDF!")
            elif not AppConfig.GROQ_API_KEY.startswith("gsk_"):
                st.error("Ch∆∞a c·∫•u h√¨nh API Key Groq trong code!")
            else:
                manager = VectorStoreManager()
                with st.spinner("ƒêang ph√¢n t√≠ch c·∫•u tr√∫c SGK..."):
                    db = manager.build_db(uploaded_files)
                    if db:
                        st.success("‚úÖ ƒê√£ h·ªçc xong! S·∫µn s√†ng tr·∫£ l·ªùi.")
                        st.session_state.retriever_engine = RAGEngine.get_retriever(db)
                        st.rerun()

    # --- Main Chat ---
    st.title("ü§ñ TR·ª¢ L√ù H·ªåC T·∫¨P (CHU·∫®N KNTT)")
    
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n! T√¥i l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ SGK Tin h·ªçc. B·∫°n c·∫ßn t√¨m hi·ªÉu Ch·ªß ƒë·ªÅ hay B√†i n√†o?"}]

    # Load DB khi kh·ªüi ƒë·ªông l·∫°i trang
    if "retriever_engine" not in st.session_state:
        manager = VectorStoreManager()
        db = manager.load_db()
        if db:
            st.session_state.retriever_engine = RAGEngine.get_retriever(db)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        st.chat_message(msg["role"], avatar=avatar).markdown(msg["content"])

    # X·ª≠ l√Ω input
    if prompt := st.chat_input("H·ªèi v·ªÅ b√†i h·ªçc..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user", avatar="üßë‚Äçüéì").markdown(prompt)

        with st.chat_message("assistant", avatar="ü§ñ"):
            if "retriever_engine" not in st.session_state:
                st.error("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu! Vui l√≤ng n·∫°p SGK ·ªü menu b√™n tr√°i.")
            else:
                try:
                    client = Groq(api_key=AppConfig.GROQ_API_KEY)
                    response_gen = RAGEngine.generate_response(client, st.session_state.retriever_engine, prompt)
                    st.write_stream(response_gen)
                    # L∆∞u l·∫°i response v√†o history (c·∫ßn gh√©p chu·ªói stream n·∫øu mu·ªën l∆∞u - ·ªü ƒë√¢y demo hi·ªÉn th·ªã tr·ª±c ti·∫øp)
                except Exception as e:
                    st.error(f"L·ªói h·ªá th·ªëng: {e}")

if __name__ == "__main__":
    main()