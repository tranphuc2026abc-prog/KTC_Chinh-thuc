import os
import glob
import time
import streamlit as st
from pathlib import Path

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói th√¥ng minh ---
try:
    from pypdf import PdfReader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==============================================================================

st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω KHKT",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    """C·∫•u h√¨nh trung t√¢m cho ·ª©ng d·ª•ng"""
    # Model AI
    LLM_MODEL = 'llama-3.1-8b-instant'
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    # ƒê∆∞·ªùng d·∫´n files
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    LOGO_PATH = "LOGO.jpg"
    
    # Tham s·ªë RAG
    CHUNK_SIZE = 1000 
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 4
    
    # T·ªëi ∆∞u performance
    MAX_CONTEXT_LENGTH = 3000

# ==============================================================================
# 2. UI/UX: GIAO DI·ªÜN HI·ªÜN ƒê·∫†I (ƒê√É T·ªêI ∆ØU)
# ==============================================================================

def inject_custom_css():
    """CSS t·ªëi ∆∞u cho giao di·ªán - Clean & Compact"""
    st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
        
        /* Font ch·ªØ to√†n h·ªá th·ªëng */
        html, body, [class*="css"] {
            font-family: 'Inter', sans-serif;
        }

        /* 1. Header nh·ªè g·ªçn, hi·ªán ƒë·∫°i h∆°n */
        .main-header {
            background: linear-gradient(90deg, #4b6cb7 0%, #182848 100%);
            padding: 1rem 1.5rem; /* Thu nh·ªè padding */
            border-radius: 12px;
            color: white;
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-bottom: 1.5rem;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .main-header h1 {
            color: white !important;
            font-weight: 700;
            margin: 0;
            font-size: 1.5rem; /* Font nh·ªè l·∫°i cho c√¢n ƒë·ªëi */
        }
        
        .main-header p {
            margin: 0;
            opacity: 0.8;
            font-size: 0.9rem;
            font-style: italic;
        }

        /* 2. Sidebar tinh t·∫ø (Flat Design) */
        [data-testid="stSidebar"] {
            background-color: #f8f9fa;
        }
        
        .sidebar-card {
            background: transparent; /* B·ªè n·ªÅn tr·∫Øng n·ªïi */
            padding: 10px 0;
            border-bottom: 1px solid #e9ecef;
            margin-bottom: 15px;
        }
        
        .sidebar-card h4 {
            color: #182848;
            font-size: 1rem;
            font-weight: 700;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        .sidebar-text {
            font-size: 0.9rem;
            color: #495057;
            margin-bottom: 5px;
            line-height: 1.4;
        }

        /* 3. Chat Messages */
        .stChatMessage {
            background-color: transparent;
        }
        
        /* User message styling */
        [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
            background-color: #e7f1ff;
            border-radius: 15px 15px 0 15px;
            color: #0f172a;
        }
        
        /* AI message styling */
        [data-testid="stChatMessageContent"]:not(:has(+ [data-testid="stChatMessageAvatar"])) {
            background-color: white;
            border: 1px solid #e2e8f0;
            border-radius: 15px 15px 15px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.02);
        }

        /* 4. Suggestion Chips (N√∫t g·ª£i √Ω) */
        .suggestion-btn {
            border: 1px solid #e2e8f0;
            background: white;
            border-radius: 20px;
            padding: 5px 15px;
            margin: 0 5px;
            cursor: pointer;
            font-size: 0.85rem;
            transition: all 0.2s;
            color: #64748b;
        }
        .suggestion-btn:hover {
            border-color: #4b6cb7;
            color: #4b6cb7;
            background: #f8fafc;
        }

        /* ·∫®n b·ªõt decoration c·ªßa Streamlit */
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        
    </style>
    """, unsafe_allow_html=True)

# ==============================================================================
# 3. LOGIC X·ª¨ L√ù (BACKEND)
# ==============================================================================

@st.cache_resource(show_spinner=False)
def load_groq_client():
    try:
        api_key = st.secrets.get("GROQ_API_KEY")
        if not api_key:
            return None
        return Groq(api_key=api_key)
    except Exception:
        return None

@st.cache_resource(show_spinner=False)
def load_embedding_model():
    try:
        with st.spinner("üîÑ Kh·ªüi ƒë·ªông h·ªá th·ªëng AI..."):
            embeddings = HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        return embeddings
    except Exception:
        return None

@st.cache_data(show_spinner=False, ttl=3600)
def load_and_process_pdfs(pdf_dir):
    docs = []
    if not os.path.exists(pdf_dir):
        os.makedirs(pdf_dir, exist_ok=True)
        return docs
    
    pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
    if not pdf_files: return docs
    
    # Gom g·ªçn x·ª≠ l√Ω PDF ƒë·ªÉ giao di·ªán kh√¥ng b·ªã gi·∫≠t
    for pdf_path in pdf_files:
        try:
            reader = PdfReader(pdf_path)
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text and len(text.strip()) > 50:
                    docs.append(Document(
                        page_content=text,
                        metadata={"source": os.path.basename(pdf_path), "page": page_num + 1}
                    ))
        except: continue
    
    if docs:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP
        )
        return splitter.split_documents(docs)
    return []

class KnowledgeBase:
    def __init__(self):
        self.embeddings = load_embedding_model()
        self.db_path = AppConfig.VECTOR_DB_PATH

    def get_vector_store(self, force_rebuild=False):
        if not self.embeddings: return None
        if os.path.exists(self.db_path) and not force_rebuild:
            try:
                return FAISS.load_local(self.db_path, self.embeddings, allow_dangerous_deserialization=True)
            except: pass
        
        splits = load_and_process_pdfs(AppConfig.PDF_DIR)
        if splits:
            vector_db = FAISS.from_documents(splits, self.embeddings)
            vector_db.save_local(self.db_path)
            return vector_db
        return None
    
    def rebuild_database(self):
        if os.path.exists(self.db_path):
            import shutil
            shutil.rmtree(self.db_path)
        load_and_process_pdfs.clear()
        return self.get_vector_store(force_rebuild=True)

def get_context(vector_db, query):
    if not vector_db: return "", []
    try:
        results = vector_db.similarity_search_with_score(query, k=AppConfig.TOP_K_RETRIEVAL)
        context_parts, sources = [], []
        total_len = 0
        for doc, score in results:
            if score > 1.6: continue
            src = doc.metadata.get('source', 'T√†i li·ªáu')
            page = doc.metadata.get('page', '1')
            content = doc.page_content.replace("\n", " ").strip()
            if total_len + len(content) > AppConfig.MAX_CONTEXT_LENGTH: break
            context_parts.append(f"[{src} - Tr.{page}]: {content}")
            sources.append(f"{src} (Trang {page})")
            total_len += len(content)
        return "\n\n".join(context_parts), list(set(sources))
    except: return "", []

def generate_stream(client, context, question):
    system_prompt = f"""B·∫°n l√† KTC Assistant - Tr·ª£ l√Ω AI chuy√™n v·ªÅ Tin h·ªçc & KHKT.
    
    Y√äU C·∫¶U:
    1. D·ª±a C·ªêT L√ïI v√†o [CONTEXT] b√™n d∆∞·ªõi.
    2. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
    3. ƒê·ªãnh d·∫°ng Markdown ƒë·∫πp m·∫Øt (Bold t·ª´ kh√≥a ch√≠nh).
    4. Gi·ªçng vƒÉn: Th√¢n thi·ªán, kh√≠ch l·ªá (nh∆∞ m·ªôt ng∆∞·ªùi th·∫ßy/ng∆∞·ªùi b·∫°n).
    
    [CONTEXT]:
    {context if context else "Kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu."}
    """
    try:
        completion = client.chat.completions.create(
            model=AppConfig.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            stream=True,
            temperature=0.3,
            max_tokens=1500
        )
        for chunk in completion:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    except Exception as e:
        yield f"‚ö†Ô∏è L·ªói: {str(e)}"

# ==============================================================================
# 4. H√ÄM CH√çNH (MAIN APP)
# ==============================================================================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ùå Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()
    
    inject_custom_css()
    
    # --- SIDEBAR ---
    with st.sidebar:
        if os.path.exists(AppConfig.LOGO_PATH):
            st.image(AppConfig.LOGO_PATH, use_container_width=True)
        else:
            st.markdown("### ü§ñ KTC Assistant")

        st.markdown("---")
        
        # Th·∫ª th√¥ng tin g·ªçn g√†ng h∆°n
        st.markdown("""
        <div class="sidebar-card">
            <h4>D·ª∞ √ÅN KHKT 2024-2025</h4>
            <div class="sidebar-text"><b>üè´ ƒê∆°n v·ªã:</b> THCS & THPT Ph·∫°m Ki·ªát</div>
            <div class="sidebar-text"><b>üë®‚Äçüíª T√°c gi·∫£:</b> T√° T√πng & B·∫£o Chung</div>
            <div class="sidebar-text"><b>üßë‚Äçüè´ GVHD:</b> Th·∫ßy Khanh</div>
        </div>
        """, unsafe_allow_html=True)
        
        # C√†i ƒë·∫∑t ƒë√£ ƒë·ªïi t√™n th√¢n thi·ªán
        with st.expander("‚öôÔ∏è C·∫•u h√¨nh h·ªá th·ªëng"):
            top_k = st.slider("ƒê·ªô r·ªông t√¨m ki·∫øm (Chunks)", 1, 8, AppConfig.TOP_K_RETRIEVAL)
            AppConfig.TOP_K_RETRIEVAL = top_k
            
            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                with st.spinner("ƒêang n·∫°p l·∫°i d·ªØ li·ªáu..."):
                    kb = KnowledgeBase()
                    st.session_state.vector_db = kb.rebuild_database()
                st.success("ƒê√£ xong!")
                time.sleep(1)
                st.rerun()

        if st.button("üóëÔ∏è X√≥a h·ªôi tho·∫°i", use_container_width=True):
            st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n! M√¨nh l√† tr·ª£ l√Ω ·∫£o KTC. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨ v·ªÅ d·ª± √°n ho·∫∑c b√†i h·ªçc h√¥m nay?"}]
            st.rerun()

    # --- MAIN CONTENT ---
    
    # Header nh·ªè g·ªçn
    st.markdown("""
    <div class="main-header">
        <div>
            <h1>TR·ª¢ L√ù ·∫¢O KTC</h1>
            <p>H·ªá th·ªëng AI h·ªó tr·ª£ Nghi√™n c·ª©u Khoa h·ªçc & Tin h·ªçc</p>
        </div>
        <div style="font-size: 2rem;">üéì</div>
    </div>
    """, unsafe_allow_html=True)

    # Init State
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n! M√¨nh l√† tr·ª£ l√Ω ·∫£o KTC. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨ v·ªÅ d·ª± √°n ho·∫∑c b√†i h·ªçc h√¥m nay?"}]
    
    if "vector_db" not in st.session_state:
        kb = KnowledgeBase()
        st.session_state.vector_db = kb.get_vector_store()
    
    groq_client = load_groq_client()

    # Render Chat
    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # --- G·ª¢I √ù C√ÇU H·ªéI (SUGGESTION CHIPS) ---
    # Ch·ªâ hi·ªán khi ch∆∞a c√≥ nhi·ªÅu h·ªôi tho·∫°i
    if len(st.session_state.messages) < 3:
        st.markdown("Running suggestion chips...") # Debug invisible line
        col1, col2, col3 = st.columns(3)
        # L∆∞u √Ω: Button trong Streamlit s·∫Ω rerun app. Ta c·∫ßn x·ª≠ l√Ω input t·ª´ button.
        suggestion = None
        if col1.button("üìù C·∫•u tr√∫c b√°o c√°o KHKT?", use_container_width=True):
            suggestion = "H√£y cho t√¥i bi·∫øt c·∫•u tr√∫c chu·∫©n c·ªßa m·ªôt b√†i b√°o c√°o KHKT c·∫•p tr∆∞·ªùng."
        if col2.button("üêç Python c∆° b·∫£n?", use_container_width=True):
            suggestion = "T·ªïng h·ª£p c√°c ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ Python trong Tin h·ªçc 11."
        if col3.button("ü§ñ AI l√† g√¨?", use_container_width=True):
            suggestion = "Gi·∫£i th√≠ch kh√°i ni·ªám Tr√≠ tu·ªá nh√¢n t·∫°o ƒë∆°n gi·∫£n nh·∫•t."
            
        if suggestion:
            # Gi·∫£ l·∫≠p vi·ªác nh·∫≠p li·ªáu
            st.session_state.messages.append({"role": "user", "content": suggestion})
            st.rerun()

    # --- CHAT INPUT & X·ª¨ L√ù ---
    # Logic: ∆Øu ti√™n l·∫•y t·ª´ suggestion n·∫øu c√≥ (ƒë√£ x·ª≠ l√Ω ·ªü tr√™n qua session state), n·∫øu kh√¥ng th√¨ l·∫•y t·ª´ input
    # Nh∆∞ng v√¨ button rerun, ta c·∫ßn check message cu·ªëi c√πng xem c√≥ ph·∫£i user kh√¥ng ƒë·ªÉ trigger tr·∫£ l·ªùi
    
    prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")
    
    # Bi·∫øn ƒë·ªÉ trigger AI tr·∫£ l·ªùi
    process_response = False
    
    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        process_response = True
    elif len(st.session_state.messages) > 0 and st.session_state.messages[-1]["role"] == "user":
        # Tr∆∞·ªùng h·ª£p v·ª´a click button g·ª£i √Ω, app rerun, message cu·ªëi l√† user -> c·∫ßn tr·∫£ l·ªùi
        process_response = True
        prompt = st.session_state.messages[-1]["content"]

    if process_response:
        if not prompt: prompt = st.session_state.messages[-1]["content"]
        
        # Ch·ªâ hi·ªÉn th·ªã prompt n·∫øu ch∆∞a hi·ªÉn th·ªã (tr√°nh duplicate khi rerun)
        # (Streamlit chat input t·ª± hi·ªÉn th·ªã, nh∆∞ng button th√¨ kh√¥ng -> ƒë√£ append v√†o session)
        
        with st.chat_message("assistant", avatar="ü§ñ"):
            response_holder = st.empty()
            
            # Status ƒë·∫πp h∆°n
            with st.status("üîç KTC ƒëang tra c·ª©u d·ªØ li·ªáu...", expanded=True) as status:
                st.write("ƒêang ƒë·ªçc t√†i li·ªáu PDF...")
                context, sources = get_context(st.session_state.vector_db, prompt)
                st.write("ƒêang t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...")
                status.update(label="‚úÖ ƒê√£ t√¨m th·∫•y th√¥ng tin!", state="complete", expanded=False)
            
            # Stream response
            full_res = ""
            if groq_client:
                for chunk in generate_stream(groq_client, context, prompt):
                    full_res += chunk
                    response_holder.markdown(full_res + "‚ñå")
                response_holder.markdown(full_res)
            else:
                st.error("Ch∆∞a k·∫øt n·ªëi ƒë∆∞·ª£c Groq API.")

            # Sources Citation
            if sources:
                with st.expander("üìö Ngu·ªìn t√†i li·ªáu tham kh·∫£o (Minh ch·ª©ng)"):
                    for src in sources:
                        st.markdown(f"- *{src}*")
            
            # L∆∞u l·∫°i c√¢u tr·∫£ l·ªùi AI (n·∫øu ch∆∞a l∆∞u)
            if st.session_state.messages[-1]["role"] != "assistant":
                st.session_state.messages.append({"role": "assistant", "content": full_res})

if __name__ == "__main__":
    main()