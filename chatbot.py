import os
import glob
import base64
import streamlit as st
import shutil
from pathlib import Path
import time

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    from pypdf import PdfReader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from langchain.retrievers import EnsembleRetriever
    from langchain_community.retrievers import BM25Retriever
    from groq import Groq
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG) - CHU·∫®N KHKT
# ==============================================================================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config - C·∫≠p nh·∫≠t model Multimodal m·ªõi nh·∫•t c·ªßa Groq
    LLM_TEXT_MODEL = 'llama-3.1-8b-instant'
    LLM_VISION_MODEL = 'llama-3.2-11b-vision-preview' # Model nh√¨n ƒë∆∞·ª£c ·∫£nh
    LLM_AUDIO_MODEL = 'whisper-large-v3'              # Model nghe √¢m thanh
    
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"
    
    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    
    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"
    
    # Hybrid RAG Parameters (C√¥ng ngh·ªá l√µi)
    CHUNK_SIZE = 800        # Gi·∫£m size ƒë·ªÉ ch√≠nh x√°c h∆°n
    CHUNK_OVERLAP = 150     
    RETRIEVAL_K = 20        
    FINAL_K = 5             
    WEIGHT_BM25 = 0.4       # Tr·ªçng s·ªë t√¨m ki·∫øm t·ª´ kh√≥a
    WEIGHT_FAISS = 0.6      # Tr·ªçng s·ªë t√¨m ki·∫øm ng·ªØ nghƒ©a

# ==============================================================================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER)
# ==============================================================================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path): return ""
        with open(file_path, "rb") as f: data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, .stMarkdown, .stButton, .stTextInput { font-family: 'Inter', sans-serif !important; }
            .project-card { background: white; padding: 15px; border-radius: 12px; border: 1px solid #dee2e6; box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px; }
            .project-title { color: #0077b6; font-weight: 800; text-align: center; text-transform: uppercase; }
            .main-header { background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%); padding: 1.5rem; border-radius: 15px; color: white; display: flex; justify-content: space-between; align-items: center; margin-bottom: 2rem; }
            .header-left h1 { color: #caf0f8; font-weight: 900; font-size: 2.2rem; margin: 0; }
            [data-testid="stChatMessageContent"] { border-radius: 15px !important; padding: 1rem !important; }
            div.stButton > button:hover { background-color: #0077b6; color: white; border-color: #0077b6; }
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                st.image(AppConfig.LOGO_SCHOOL, width=100)
            
            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div style="text-align:center; font-size: 0.8rem; color: #666; font-style:italic;">Hybrid RAG & Multimodal AI</div>
                <hr>
                <small><b>T√°c gi·∫£:</b> B√πi T√° T√πng - Cao S·ªπ B·∫£o Chung<br>
                <b>GVHD:</b> Th·∫ßy Nguy·ªÖn Th·∫ø Khanh</small>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown("### üß† ƒêa ph∆∞∆°ng th·ª©c (Multimodal)")
            with st.expander("üì∏ T·∫£i l√™n ·∫¢nh/Code/Voice", expanded=True):
                uploaded_file = st.file_uploader("Ch·ªçn file (·∫¢nh l·ªói code, S∆° ƒë·ªì, Ghi √¢m)", type=['png', 'jpg', 'jpeg', 'mp3', 'wav', 'py'])
                if uploaded_file:
                    st.session_state.uploaded_file = uploaded_file
                    st.success(f"ƒê√£ nh·∫≠n file: {uploaded_file.name}")
            
            st.markdown("---")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.session_state.uploaded_file = None
                st.rerun()

# ==============================================================================
# 3. LOGIC BACKEND (HYBRID RAG + MULTIMODAL)
# ==============================================================================

class RAGEngine:
    @staticmethod
    @st.cache_resource
    def load_groq_client():
        api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
        return Groq(api_key=api_key) if api_key else None

    @staticmethod
    @st.cache_resource
    def load_embedding_model():
        return HuggingFaceEmbeddings(model_name=AppConfig.EMBEDDING_MODEL)

    @staticmethod
    @st.cache_resource
    def load_reranker():
        return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)

    @staticmethod
    def build_or_load_retriever(embeddings):
        # 1. Load Vector DB (FAISS)
        vector_store = None
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            vector_store = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
        else:
            # Logic t·∫°o m·ªõi DB (r√∫t g·ªçn ƒë·ªÉ t·∫≠p trung v√†o logic ch√≠nh)
            if not os.path.exists(AppConfig.PDF_DIR): return None
            docs = []
            files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.*"))
            for f in files:
                if f.endswith('.pdf'):
                    reader = PdfReader(f)
                    for i, page in enumerate(reader.pages):
                        txt = page.extract_text()
                        if txt: docs.append(Document(page_content=txt, metadata={"source": os.path.basename(f), "page": i+1}))
            
            if docs:
                splitter = RecursiveCharacterTextSplitter(chunk_size=AppConfig.CHUNK_SIZE, chunk_overlap=AppConfig.CHUNK_OVERLAP)
                splits = splitter.split_documents(docs)
                vector_store = FAISS.from_documents(splits, embeddings)
                vector_store.save_local(AppConfig.VECTOR_DB_PATH)
        
        if not vector_store: return None

        # 2. T·∫°o Hybrid Retriever (FAISS + BM25) -> ƒêI·ªÇM S√ÅNG KHOA H·ªåC
        # L·∫•y l·∫°i documents t·ª´ vector store ƒë·ªÉ t·∫°o BM25
        # L∆∞u √Ω: Trong th·ª±c t·∫ø n√™n cache BM25 ri√™ng, nh∆∞ng demo th√¨ load t·ª´ docstore
        try:
            docstore_docs = list(vector_store.docstore._dict.values())
            bm25_retriever = BM25Retriever.from_documents(docstore_docs)
            bm25_retriever.k = AppConfig.RETRIEVAL_K

            faiss_retriever = vector_store.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})

            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[AppConfig.WEIGHT_BM25, AppConfig.WEIGHT_FAISS]
            )
            return ensemble_retriever
        except:
            return vector_store.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})

    @staticmethod
    def process_multimodal_input(client, uploaded_file, user_query):
        """X·ª≠ l√Ω ·∫¢nh v√† √Çm thanh"""
        vision_content = None
        audio_transcript = None
        
        # X·ª≠ l√Ω ·∫¢nh
        if uploaded_file.type in ['image/png', 'image/jpeg', 'image/jpg']:
            # Encode ·∫£nh
            base64_image = base64.b64encode(uploaded_file.getvalue()).decode('utf-8')
            # G·ªçi Vision Model ƒë·ªÉ m√¥ t·∫£ ·∫£nh
            try:
                chat_completion = client.chat.completions.create(
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": "M√¥ t·∫£ chi ti·∫øt n·ªôi dung trong b·ª©c ·∫£nh n√†y li√™n quan ƒë·∫øn Tin h·ªçc/L·∫≠p tr√¨nh. N·∫øu l√† code l·ªói, h√£y ch·ªâ ra l·ªói."},
                                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
                            ],
                        }
                    ],
                    model=AppConfig.LLM_VISION_MODEL,
                )
                vision_content = chat_completion.choices[0].message.content
            except Exception as e:
                vision_content = f"L·ªói ƒë·ªçc ·∫£nh: {str(e)}"

        # X·ª≠ l√Ω √Çm thanh (Whisper)
        elif uploaded_file.type in ['audio/mpeg', 'audio/wav', 'audio/mp3']:
            try:
                # C·∫ßn l∆∞u t·∫°m file ƒë·ªÉ g·ª≠i v√†o API
                with open("temp_audio.mp3", "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                with open("temp_audio.mp3", "rb") as file:
                    transcription = client.audio.transcriptions.create(
                        file=("temp_audio.mp3", file.read()),
                        model=AppConfig.LLM_AUDIO_MODEL,
                        response_format="text",
                        language="vi"
                    )
                audio_transcript = transcription
            except Exception as e:
                audio_transcript = f"L·ªói nghe: {str(e)}"
        
        # X·ª≠ l√Ω File Python
        elif uploaded_file.name.endswith('.py'):
             vision_content = f"N·ªôi dung file code h·ªçc sinh upload:\n```python\n{uploaded_file.getvalue().decode('utf-8')}\n```"

        return vision_content, audio_transcript

    @staticmethod
    def generate_response(client, retriever, query, vision_context=None):
        # 1. Retrieval (Hybrid Search)
        docs = retriever.invoke(query)
        
        # 2. Rerank (FlashRank)
        try:
            ranker = RAGEngine.load_reranker()
            passages = [{"id": str(i), "text": doc.page_content, "meta": doc.metadata} for i, doc in enumerate(docs)]
            rerank_request = RerankRequest(query=query, passages=passages)
            ranked_results = ranker.rank(rerank_request)[:AppConfig.FINAL_K]
            final_docs = [Document(page_content=r['text'], metadata=r['meta']) for r in ranked_results]
        except:
            final_docs = docs[:AppConfig.FINAL_K]

        # 3. Context Construction
        context_text = ""
        sources = []
        for doc in final_docs:
            src = doc.metadata.get('source', 'Unknown')
            page = doc.metadata.get('page', '?')
            context_text += f"\n[Ngu·ªìn: {src} - Tr {page}]: {doc.page_content}"
            sources.append(f"{src} - Trang {page}")

        # 4. System Prompt (Pedagogical & Multimodal)
        multimodal_instruction = ""
        if vision_context:
            multimodal_instruction = f"H·ªçc sinh c√≥ g·ª≠i k√®m h√¨nh ·∫£nh/code v·ªõi n·ªôi dung sau: '{vision_context}'. H√£y k·∫øt h·ª£p n·ªôi dung n√†y ƒë·ªÉ tr·∫£ l·ªùi."

        system_prompt = f"""B·∫°n l√† KTC Chatbot - Tr·ª£ l√Ω AI d·∫°y Tin h·ªçc gi·ªèi c·∫•p Qu·ªëc gia.
        
        NHI·ªÜM V·ª§:
        1. Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n [KI·∫æN TH·ª®C SGK] ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.
        2. N·∫øu c√¢u h·ªèi v·ªÅ l·∫≠p tr√¨nh, h√£y ƒë√≥ng vai 'Reviewer' (Ng∆∞·ªùi h∆∞·ªõng d·∫´n): Ch·ªâ ra l·ªói sai, gi·∫£i th√≠ch nguy√™n l√Ω, KH√îNG vi·∫øt code gi·∫£i b√†i t·∫≠p v·ªÅ nh√† thay cho h·ªçc sinh.
        3. Phong c√°ch: S∆∞ ph·∫°m, kh√≠ch l·ªá, ng·∫Øn g·ªçn.
        
        {multimodal_instruction}
        
        [KI·∫æN TH·ª®C SGK]:
        {context_text}
        """

        try:
            stream = client.chat.completions.create(
                model=AppConfig.LLM_TEXT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                stream=True,
                temperature=0.3
            )
            return stream, sorted(list(set(sources)))
        except Exception as e:
            return str(e), []

# ==============================================================================
# 4. MAIN RUNTIME
# ==============================================================================

def main():
    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    
    # Header
    logo_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
    img_tag = f'<img src="data:image/png;base64,{logo_b64}" style="width:80px;border-radius:50%;">' if logo_b64 else ""
    st.markdown(f"""
    <div class="main-header">
        <div class="header-left">
            <h1>KTC CHATBOT AI</h1>
            <p>Tr·ª£ l√Ω h·ªçc t·∫≠p Tin h·ªçc :: Hybrid RAG & Vision</p>
        </div>
        {img_tag}
    </div>
    """, unsafe_allow_html=True)

    # Init State
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Ch√†o em! Th·∫ßy l√† tr·ª£ l√Ω AI. Em c·∫ßn h·ªèi v·ªÅ b√†i h·ªçc hay mu·ªën th·∫ßy xem gi√∫p ƒëo·∫°n code n√†o?"}]
    if "uploaded_file" not in st.session_state:
        st.session_state.uploaded_file = None

    # Load Resources
    groq_client = RAGEngine.load_groq_client()
    embeddings = RAGEngine.load_embedding_model()
    
    if "retriever" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng Hybrid Search..."):
            st.session_state.retriever = RAGEngine.build_or_load_retriever(embeddings)

    # Display Chat
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # Chat Input
    if user_input := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").markdown(user_input)

        with st.chat_message("assistant"):
            if not groq_client:
                st.error("‚ùå Ch∆∞a k·∫øt n·ªëi API Groq.")
                st.stop()
            
            # X·ª≠ l√Ω Multimodal n·∫øu c√≥ file upload
            vision_context = None
            if st.session_state.uploaded_file:
                with st.status("üñºÔ∏è ƒêang ph√¢n t√≠ch file ƒë√≠nh k√®m...", expanded=False):
                    vision_context, audio_text = RAGEngine.process_multimodal_input(groq_client, st.session_state.uploaded_file, user_input)
                    if audio_text: # N·∫øu l√† file √¢m thanh, thay th·∫ø user_input b·∫±ng text ƒë√£ d·ªãch
                        st.info(f"üéôÔ∏è N·ªôi dung ghi √¢m: {audio_text}")
                        user_input = f"{user_input} (N·ªôi dung n√≥i: {audio_text})"
            
            # Generate Response
            response_placeholder = st.empty()
            stream, sources = RAGEngine.generate_response(
                groq_client, 
                st.session_state.retriever, 
                user_input, 
                vision_context
            )
            
            full_response = ""
            if isinstance(stream, str):
                response_placeholder.error(stream)
            else:
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                        response_placeholder.markdown(full_response + "‚ñå")
                response_placeholder.markdown(full_response)
            
            # Show Sources
            if sources:
                with st.expander("üìö CƒÉn c·ª© khoa h·ªçc (Tr√≠ch d·∫´n SGK)"):
                    for src in sources: st.markdown(f"- {src}")

            st.session_state.messages.append({"role": "assistant", "content": full_response})
            # Reset file sau khi x·ª≠ l√Ω xong
            if st.session_state.uploaded_file:
                st.session_state.uploaded_file = None

if __name__ == "__main__":
    main()