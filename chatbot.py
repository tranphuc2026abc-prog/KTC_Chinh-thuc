import os
import glob
import base64
import streamlit as st
import shutil
from pathlib import Path
from typing import List, Tuple, Optional, Dict

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    from pypdf import PdfReader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever  # Hybrid Search
    from langchain.retrievers import EnsembleRetriever        # Hybrid Search
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    # Rerank
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================================================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==============================================================================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="LOGO.jpg",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    # Model Config
    LLM_MODEL = 'llama-3.1-8b-instant'
    LLM_VISION_MODEL = 'llama-3.2-11b-vision-preview'  # Model nh√¨n ·∫£nh
    LLM_AUDIO_MODEL = 'whisper-large-v3'               # Model nghe

    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

    # Model Rerank nh·ªè, ch·∫°y t·ªët tr√™n CPU
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"

    # Paths
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"  # Cache model rerank

    # Assets
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"

    # RAG Parameters
    CHUNK_SIZE = 700       # tinh h∆°n, tr√°nh lan man
    CHUNK_OVERLAP = 120    # v·ª´a ƒë·ªß n·ªëi m·∫°ch ng·ªØ nghƒ©a
    RETRIEVAL_K = 24       # l·∫•y r·ªông tr∆∞·ªõc
    FINAL_K = 6            # l·ªçc tinh sau rerank
    MMR_FETCH_K = 50       # s·ªë doc th√¥ ƒë·ªÉ MMR ƒëa d·∫°ng
    MMR_LAMBDA = 0.6       # trade-off relevance/diversity

    # LLM gen
    LLM_TEMPERATURE = 0.2
    LLM_MAX_TOKENS = 1400

# ==============================================================================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER) - GI·ªÆ NGUY√äN 100% NH∆Ø C≈®
# ==============================================================================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .header-left p {
                color: #e0fbfc; margin: 5px 0 0 0; font-size: 1rem; opacity: 0.9;
            }
            .header-right img {
                border-radius: 50%; border: 3px solid rgba(255,255,255,0.3);
                box-shadow: 0 4px 10px rgba(0,0,0,0.2); width: 100px; height: 100px;
                object-fit: cover;
            }
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
            [data-testid="stChatMessageContent"]:has(+ [data-testid="stChatMessageAvatar"]) {
                background: #e3f2fd; color: #0d47a1;
            }
            [data-testid="stChatMessageContent"]:not(:has(+ [data-testid="stChatMessageAvatar"])) {
                background: white; border: 1px solid #e9ecef;
                border-left: 5px solid #00b4d8;
            }
            div.stButton > button {
                border-radius: 8px; background-color: white; color: #0077b6;
                border: 1px solid #90e0ef; transition: all 0.2s;
            }
            div.stButton > button:hover {
                background-color: #0077b6; color: white;
                border-color: #0077b6; box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            }
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p tr∆∞·ªùng</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">NƒÉm h·ªçc:</span>
                        <span style="text-align: right; color: #222;"><b>2025 - 2026</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)

            # Expander upload gi·ªØ nguy√™n
            with st.expander("üìÇ T√≠nh nƒÉng n√¢ng cao (AI Vision)", expanded=False):
                st.markdown("<small>T·∫£i ·∫£nh l·ªói code ho·∫∑c file ghi √¢m c√¢u h·ªèi</small>", unsafe_allow_html=True)
                uploaded_file = st.file_uploader("", type=['png', 'jpg', 'jpeg', 'mp3', 'wav', 'py'], key="multimodal_upload")
                if uploaded_file:
                    st.session_state.uploaded_file_obj = uploaded_file
                    st.success("ƒê√£ nh·∫≠n file!")

            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.session_state.uploaded_file_obj = None
                st.rerun()

            # N√∫t Rebuild DB
            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                st.session_state.pop('retriever_engine', None)  # Clear cache
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" alt="Logo">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p style="font-size: 1.1rem; margin-top: 5px;">H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">
                {img_html}
            </div>
        </div>
        """, unsafe_allow_html=True)

# ==============================================================================
# 3. LOGIC BACKEND (RAG ENGINE + RERANK + MULTIMODAL)
# ==============================================================================

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            if not api_key:
                return None
            return Groq(api_key=api_key)
        except Exception:
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception as e:
            st.error(f"L·ªói t·∫£i Embedding Model: {e}")
            return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception as e:
            st.warning(f"L·ªói t·∫£i Reranker (fallback kh√¥ng rerank): {e}")
            return None

    # Detect topic per chunk
    @staticmethod
    def _detect_topic(text: str) -> str:
        tx = (text or "").lower()
        if any(tag in tx for tag in ["<html", "<head", "<body", "css", "javascript", "<title", "<div", "<span"]):
            return "html"
        if any(p in tx for p in ["def ", "import ", "for ", "while ", "list", "tuple", "dict", "print(", "input("]):
            return "python"
        if any(d in tx for d in ["sql", "kh√≥a ch√≠nh", "primary key", "foreign key", "c∆° s·ªü d·ªØ li·ªáu", "b·∫£ng", "quan h·ªá"]):
            return "database"
        if any(s in tx for s in ["an ninh m·∫°ng", "b·∫£o m·∫≠t", "an to√†n th√¥ng tin", "m·∫°ng x√£ h·ªôi", "tin gi·∫£"]):
            return "security"
        return "general"

    @staticmethod
    def _read_source_files(pdf_dir: str) -> List[Document]:
        """ƒê·ªçc PDF/TXT th√†nh Document th√¥, ƒë√≠nh metadata ngu·ªìn + trang."""
        pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
        txt_files = glob.glob(os.path.join(pdf_dir, "*.txt"))
        all_files = pdf_files + txt_files

        docs: List[Document] = []
        for file_path in all_files:
            try:
                source_file = os.path.basename(file_path)
                source_name = source_file.replace('.pdf', '').replace('.txt', '').replace('_', ' ')
                if file_path.endswith('.pdf'):
                    reader = PdfReader(file_path)
                    for page_num, page in enumerate(reader.pages):
                        text = page.extract_text()
                        if text and len(text.strip()) > 50:
                            clean_text = text.replace('\x00', '').strip()
                            docs.append(Document(
                                page_content=clean_text,
                                metadata={"source": source_file, "page": page_num + 1, "title": source_name}
                            ))
                else:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        text = f.read().strip()
                        if text:
                            docs.append(Document(
                                page_content=text,
                                metadata={"source": source_file, "page": 1, "title": source_name}
                            ))
            except Exception:
                continue
        return docs

    @staticmethod
    def _chunk_documents(docs: List[Document]) -> List[Document]:
        """Chunk-level chu·∫©n, gi·ªØ trace: source/page/chunk_id + topic."""
        if not docs:
            return []
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=AppConfig.CHUNK_SIZE,
            chunk_overlap=AppConfig.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""],
            add_start_index=True
        )
        chunks: List[Document] = []
        for d in docs:
            split_docs = splitter.split_documents([d])
            for i, sd in enumerate(split_docs):
                meta = dict(d.metadata)
                meta["chunk_id"] = f"{meta.get('source','unk')}#p{meta.get('page','?')}#c{i}"
                content = sd.page_content.replace("\u0000", "").strip()
                meta["topic"] = RAGEngine._detect_topic(content)
                chunks.append(Document(page_content=content, metadata=meta))
        return chunks

    @staticmethod
    def build_hybrid_retriever(embeddings):
        """
        X√¢y d·ª±ng Hybrid Search (BM25 + FAISS) ·ªïn ƒë·ªãnh, c√≥ MMR.
        """
        if not embeddings:
            return None

        vector_db = None
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
            except Exception as e:
                st.warning(f"Reload DB l·ªói: {e}")

        if not vector_db:
            if not os.path.exists(AppConfig.PDF_DIR):
                st.error(f"‚ö†Ô∏è Th∆∞ m·ª•c '{AppConfig.PDF_DIR}' kh√¥ng t·ªìn t·∫°i!")
                return None

            status = st.empty()
            raw_docs = RAGEngine._read_source_files(AppConfig.PDF_DIR)
            status.info(f"üìö ƒêang s·ªë h√≥a {len(raw_docs)} t√†i li·ªáu. Vui l√≤ng ƒë·ª£i...")
            chunk_docs = RAGEngine._chunk_documents(raw_docs)
            if not chunk_docs:
                status.empty()
                st.error("Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ x√¢y Vector DB.")
                return None

            # T·∫°o FAISS t·ª´ chunk-level ƒë·ªÉ citation traceable
            vector_db = FAISS.from_documents(chunk_docs, embeddings)
            vector_db.save_local(AppConfig.VECTOR_DB_PATH)
            status.empty()

        try:
            # BM25 t·ª´ to√†n b·ªô docstore (chunk-level)
            docstore_docs = list(vector_db.docstore._dict.values())
            bm25_retriever = BM25Retriever.from_documents(docstore_docs)
            bm25_retriever.k = AppConfig.RETRIEVAL_K

            # FAISS retriever v·ªõi MMR ƒë·ªÉ ƒëa d·∫°ng
            faiss_retriever = vector_db.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": AppConfig.RETRIEVAL_K,
                    "fetch_k": AppConfig.MMR_FETCH_K,
                    "lambda_mult": AppConfig.MMR_LAMBDA
                }
            )

            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[0.4, 0.6]
            )
            return ensemble_retriever
        except Exception:
            # Fallback v·ªÅ vector th∆∞·ªùng n·∫øu l·ªói
            return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})

    @staticmethod
    def process_multimodal(client, uploaded_file):
        vision_desc = ""
        audio_text = ""

        if uploaded_file.type.startswith('image'):
            base64_image = base64.b64encode(uploaded_file.getvalue()).decode('utf-8')
            try:
                resp = client.chat.completions.create(
                    model=AppConfig.LLM_VISION_MODEL,
                    messages=[{
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "M√¥ t·∫£ chi ti·∫øt code ho·∫∑c n·ªôi dung trong ·∫£nh n√†y."},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                        ]
                    }]
                )
                vision_desc = resp.choices[0].message.content or ""
            except Exception:
                vision_desc = "L·ªói ƒë·ªçc ·∫£nh."
        elif uploaded_file.type.startswith('audio'):
            try:
                tmp = "temp_audio_input"
                suffix = ".mp3" if uploaded_file.type.endswith("mpeg") or uploaded_file.type.endswith("mp3") else ".wav"
                temp_path = f"{tmp}{suffix}"
                with open(temp_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                with open(temp_path, "rb") as f:
                    transcription = client.audio.transcriptions.create(
                        file=(os.path.basename(temp_path), f.read()),
                        model=AppConfig.LLM_AUDIO_MODEL,
                        response_format="text"
                    )
                audio_text = transcription or ""
                try:
                    os.remove(temp_path)
                except Exception:
                    pass
            except Exception:
                audio_text = "L·ªói nghe √¢m thanh."
        return vision_desc, audio_text

    @staticmethod
    def _format_context_for_llm(final_docs: List[Document]) -> Tuple[str, List[str], Dict[str, str]]:
        """
        T·∫°o [NG·ªÆ C·∫¢NH] chu·∫©n, chunk-level, v√† mapping citation ID -> label [S1]...
        Tr·∫£ v·ªÅ:
          - context_text: block vƒÉn b·∫£n cho LLM
          - source_labels: danh s√°ch hi·ªÉn th·ªã d∆∞·ªõi expander
          - cite_map: dict {chunk_id: "[S#]"}
        """
        context_text = []
        source_labels = []
        cite_map: Dict[str, str] = {}
        for idx, doc in enumerate(final_docs):
            src = doc.metadata.get('source', 'T√†i li·ªáu')
            page = doc.metadata.get('page', 'Unknown')
            chunk_id = doc.metadata.get('chunk_id', f"{src}#p{page}#c{idx}")
            topic = doc.metadata.get('topic', 'general')
            label = f"[S{idx+1}]"
            cite_map[chunk_id] = label
            content = doc.page_content.replace("\n", " ").strip()
            # Hi·ªÉn th·ªã topic ƒë·ªÉ ng∆∞·ªùi ƒë·ªçc hi·ªÉu ƒë∆°n v·ªã tr√≠ch d·∫´n
            context_text.append(f"{label} | {src} | Ch·ªß ƒë·ªÅ: {topic} | Trang {page}\n{content}\n---")
            source_labels.append(f"{src} - Trang {page} ({label}, {topic})")
        return "\n".join(context_text), source_labels, cite_map

    @staticmethod
    def _controlled_refusal_message() -> str:
        return (
            "M√¨nh kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu ƒë√£ cung c·∫•p, n√™n kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. "
            "B·∫°n c√≥ th·ªÉ h·ªèi l·∫°i r√µ h∆°n ho·∫∑c ki·ªÉm tra t√†i li·ªáu li√™n quan."
        )

    @staticmethod
    def generate_response(client, retriever, query, vision_context=None):
        """
        - Retrieval (Hybrid + MMR)
        - Soft-filter theo ch·ªß ƒë·ªÅ/l·ªõp t·ª´ c√¢u h·ªèi
        - Rerank (FlashRank)
        - Chunk-level citation [S1], [S2]... √©p inline
        - T·ª´ ch·ªëi c√≥ ki·ªÉm so√°t n·∫øu kh√¥ng c√≥ ng·ªØ c·∫£nh
        - Stream ·ªïn ƒë·ªãnh
        """
        # 1. Retrieval
        final_docs: List[Document] = []
        initial_docs: List[Document] = []

        def infer_intent(q: str):
            ql = (q or "").lower()
            grade_hint = "12" if ("12" in ql or "l·ªõp 12" in ql) else \
                         "11" if ("11" in ql or "l·ªõp 11" in ql) else \
                         "10" if ("10" in ql or "l·ªõp 10" in ql) else None
            topic_hint = "html" if ("html" in ql or "css" in ql) else \
                         "python" if "python" in ql else \
                         "database" if ("sql" in ql or "c∆° s·ªü d·ªØ li·ªáu" in ql or "csdl" in ql) else \
                         "security" if ("an ninh" in ql or "b·∫£o m·∫≠t" in ql) else None
            return grade_hint, topic_hint

        grade_hint, topic_hint = infer_intent(query)

        def soft_filter(docs: List[Document]) -> List[Document]:
            if not docs:
                return []
            def score(d: Document) -> int:
                s = 0
                md = d.metadata or {}
                # ∆Øu ti√™n ƒë√∫ng ch·ªß ƒë·ªÅ/l·ªõp n·∫øu ƒëo√°n ƒë∆∞·ª£c
                if grade_hint and md.get("title", "").lower().find(grade_hint) != -1:
                    s += 2
                if grade_hint and md.get("source", "").lower().find(grade_hint) != -1:
                    s += 1
                if topic_hint and md.get("topic") == topic_hint:
                    s += 3
                # Heuristic cho HTML: c√≥ tag
                pc = (d.page_content or "").lower()
                if topic_hint == "html" and any(tag in pc for tag in ["<html", "<head", "<body", "css", "javascript"]):
                    s += 1
                return s
            return sorted(docs, key=score, reverse=True)[:max(AppConfig.RETRIEVAL_K, 12)]

        if retriever:
            try:
                initial_docs = retriever.invoke(query) or []
                # √Åp d·ª•ng soft-filter theo topic/l·ªõp tr∆∞·ªõc rerank
                initial_docs = soft_filter(initial_docs)
            except Exception:
                initial_docs = []

            # 2. Rerank
            try:
                ranker = RAGEngine.load_reranker()
                if ranker and initial_docs:
                    passages = [{"id": str(i), "text": d.page_content, "meta": d.metadata} for i, d in enumerate(initial_docs)]
                    rerank_request = RerankRequest(query=query, passages=passages)
                    ranked_results = ranker.rank(rerank_request)
                    for res in ranked_results[:AppConfig.FINAL_K]:
                        final_docs.append(Document(page_content=res["text"], metadata=res["meta"]))
                else:
                    final_docs = initial_docs[:AppConfig.FINAL_K]
            except Exception:
                final_docs = initial_docs[:AppConfig.FINAL_K]

        # N·∫øu kh√¥ng c√≥ doc ph√π h·ª£p ‚Üí t·ª´ ch·ªëi
        if not final_docs:
            refusal_text = RAGEngine._controlled_refusal_message()
            return [refusal_text], []

        # 3. T·∫°o ng·ªØ c·∫£nh + citation map
        context_text, source_labels, cite_map = RAGEngine._format_context_for_llm(final_docs)

        # 4. Prompt √©p tr√≠ch d·∫´n, kh√¥ng suy di·ªÖn
        extra_instruct = ""
        if vision_context:
            extra_instruct = (
                f"H·ªçc sinh c√≥ g·ª≠i k√®m ·∫£nh/code m√¥ t·∫£: '{vision_context}'. "
                "Ch·ªâ d√πng th√¥ng tin n√†y ƒë·ªÉ h·ªó tr·ª£ di·ªÖn gi·∫£i trong ph·∫°m vi [NG·ªÆ C·∫¢NH]."
            )

        system_prompt = f"""B·∫°n l√† KTC Chatbot - Tr·ª£ l√Ω AI gi√°o d·ª•c c·ªßa tr∆∞·ªùng Ph·∫°m Ki·ªát.

Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n [NG·ªÆ C·∫¢NH] b√™n d∆∞·ªõi. Kh√¥ng th√™m ki·∫øn th·ª©c ngo√†i.
- M·ªói √Ω ch√≠nh ph·∫£i k√®m citation inline theo d·∫°ng [S1], [S2]... t∆∞∆°ng ·ª©ng t·ª´ng ƒëo·∫°n trong [NG·ªÆ C·∫¢NH].
- Kh√¥ng g·ªôp ngu·ªìn, kh√¥ng ƒë·ªïi ƒë·ªãnh d·∫°ng citation, kh√¥ng d√πng [T√™n_File - Trang X] trong n·ªôi dung. Ch·ªâ d√πng [S#].
- N·∫øu l√† b√†i t·∫≠p code: ch·ªâ g·ª£i √Ω h∆∞·ªõng, gi·∫£i th√≠ch l·ªói; kh√¥ng vi·∫øt tr·ªçn l·ªùi gi·∫£i.
- N·∫øu [NG·ªÆ C·∫¢NH] kh√¥ng ch·ª©a th√¥ng tin c·∫ßn thi·∫øt: tr·∫£ l·ªùi t·ª´ ch·ªëi m·ªôt c√°ch l·ªãch s·ª±, ng·∫Øn g·ªçn.

{extra_instruct}

[NG·ªÆ C·∫¢NH]
{context_text}
"""

        # 5. G·ªçi LLM v·ªõi stream
        try:
            stream = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ],
                stream=True,
                temperature=AppConfig.LLM_TEMPERATURE,
                max_tokens=AppConfig.LLM_MAX_TOKENS
            )
            unique_sources = source_labels
            return stream, unique_sources
        except Exception as e:
            return [f"L·ªói k·∫øt n·ªëi AI: {str(e)}"], []

# ==============================================================================
# 4. MAIN APPLICATION
# ==============================================================================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è L·ªói th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.info("Vui l√≤ng ch·∫°y l·ªánh: pip install flashrank rank_bm25")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! M√¨nh l√† KTC Chatbot. B·∫°n c·∫ßn h·ªó tr·ª£ b√†i t·∫≠p Tin h·ªçc ph·∫ßn n√†o?"}]

    groq_client = RAGEngine.load_groq_client()

    # Init Retriever (Hybrid)
    if "retriever_engine" not in st.session_state:
        with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c s·ªë (Hybrid)..."):
            embeddings = RAGEngine.load_embedding_model()
            st.session_state.retriever_engine = RAGEngine.build_hybrid_retriever(embeddings)
            if st.session_state.retriever_engine:
                st.toast("‚úÖ ƒê√£ t·∫£i xong d·ªØ li·ªáu!", icon="üìö")

    for msg in st.session_state.messages:
        bot_avatar = AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else bot_avatar
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"])

    # GI·ªÆ NGUY√äN PH·∫¶N G·ª¢I √ù C√ÇU H·ªéI NH∆Ø C≈®
    if len(st.session_state.messages) < 2:
        st.markdown("##### üí° G·ª£i √Ω √¥n t·∫≠p:")
        cols = st.columns(3)
        prompt_btn = None
        if cols[0].button("üêç Python: S·ªë nguy√™n t·ªë"):
            prompt_btn = "Vi·∫øt ch∆∞∆°ng tr√¨nh Python nh·∫≠p v√†o m·ªôt s·ªë nguy√™n n v√† ki·ªÉm tra xem n c√≥ ph·∫£i l√† s·ªë nguy√™n t·ªë hay kh√¥ng. Gi·∫£i th√≠ch code."
        if cols[1].button("üóÉÔ∏è CSDL: Kh√≥a ch√≠nh"):
            prompt_btn = "Gi·∫£i th√≠ch kh√°i ni·ªám Kh√≥a ch√≠nh (Primary Key) trong CSDL quan h·ªá v√† cho v√≠ d·ª• minh h·ªça."
        if cols[2].button("‚öñÔ∏è Lu·∫≠t An ninh m·∫°ng"):
            prompt_btn = "N√™u c√°c h√†nh vi b·ªã nghi√™m c·∫•m theo Lu·∫≠t An ninh m·∫°ng Vi·ªát Nam. Tr√≠ch d·∫´n ƒëi·ªÅu kho·∫£n n·∫øu c√≥."
        if prompt_btn:
            st.session_state.temp_input = prompt_btn
            st.rerun()

    if "temp_input" in st.session_state and st.session_state.temp_input:
        user_input = st.session_state.temp_input
        del st.session_state.temp_input
    else:
        user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y...")

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=AppConfig.LOGO_PROJECT if os.path.exists(AppConfig.LOGO_PROJECT) else "ü§ñ"):
            response_placeholder = st.empty()

            if not groq_client:
                st.error("‚ùå Ch∆∞a c·∫•u h√¨nh API Key.")
            else:
                # Ki·ªÉm tra Multimodal Input
                vision_context = None
                if "uploaded_file_obj" in st.session_state and st.session_state.uploaded_file_obj:
                    with st.status("üñºÔ∏è ƒêang ph√¢n t√≠ch file...", expanded=False):
                        vision_desc, audio_text = RAGEngine.process_multimodal(groq_client, st.session_state.uploaded_file_obj)

                        if audio_text:
                            user_input = f"{user_input} (N·ªôi dung ghi √¢m: {audio_text})"
                            st.info(f"üéôÔ∏è ƒê√£ nghe: {audio_text}")

                        if vision_desc:
                            vision_context = vision_desc

                # Generate Response
                stream, sources = RAGEngine.generate_response(
                    groq_client,
                    st.session_state.retriever_engine,
                    user_input,
                    vision_context
                )

                full_response = ""
                # Stream ·ªïn ƒë·ªãnh: n·∫øu stream l√† list -> hi·ªÉn th·ªã ngay (d·∫°ng l·ªói/t·ª´ ch·ªëi)
                if isinstance(stream, list):
                    full_response = stream[0] if stream else "ƒê√£ x·∫£y ra l·ªói kh√¥ng x√°c ƒë·ªãnh."
                    response_placeholder.markdown(full_response)
                else:
                    try:
                        for chunk in stream:
                            delta = getattr(chunk.choices[0].delta, "content", None)
                            if delta:
                                full_response += delta
                                response_placeholder.markdown(full_response + "‚ñå")
                        response_placeholder.markdown(full_response)
                    except Exception as e:
                        response_placeholder.error(f"L·ªói stream: {e}")

                if sources:
                    with st.expander("üìö T√†i li·ªáu tham kh·∫£o (ƒê√£ ki·ªÉm ch·ª©ng)"):
                        for src in sources:
                            st.markdown(f"- üìñ *{src}*")

                st.session_state.messages.append({"role": "assistant", "content": full_response})
                # Reset file upload sau khi tr·∫£ l·ªùi xong
                if "uploaded_file_obj" in st.session_state and st.session_state.uploaded_file_obj:
                    st.session_state.uploaded_file_obj = None

if __name__ == "__main__":
    main()
