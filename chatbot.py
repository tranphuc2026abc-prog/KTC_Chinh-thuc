# ==============================================================
#   TR·ª¢ L√ù KHKT ‚Äì PHI√äN B·∫¢N KH√îNG L·ªñI TRANSLATOR (SAFE VERSION)
# ==============================================================

import os
import glob
import time
from typing import List, Optional

import streamlit as st
from pypdf import PdfReader

# AI / RAG libs
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# Translator (SAFE version)
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# Groq client for LLM streaming
from groq import Groq


# ==============================================================
# 0. C·∫§U H√åNH CHUNG
# ==============================================================

st.set_page_config(
    page_title="KTC Assistant - Tr·ª£ l√Ω Tin h·ªçc 2025",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

CONSTANTS = {
    "MODEL_NAME": 'llama-3.1-8b-instant',
    "PDF_DIR": "./PDF_KNOWLEDGE",
    "VECTOR_STORE_PATH": "./faiss_db_index",
    "LOGO_PATH": "LOGO.jpg",

    # Embedding ƒëa ng√¥n ng·ªØ ‚Äì gi√∫p fallback khi kh√¥ng d·ªãch ƒë∆∞·ª£c
    "EMBEDDING_MODEL": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",

    # Model d·ªãch (s·∫Ω c√≥ fallback n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c)
    "TRANSLATION_MODEL": "Helsinki-NLP/opus-mt-vi-en",

    "CHUNK_SIZE": 800,
    "CHUNK_OVERLAP": 150,
    "TOP_K": 3,
}


# ==============================================================
# 1. GIAO DI·ªÜN CSS
# ==============================================================

st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap');
    html, body, [class*="css"] { font-family: 'Roboto', sans-serif; }
    .stApp {background-color: #f8f9fa;}
    [data-testid="stSidebar"] { background-color: #ffffff; border-right: 1px solid #e0e0e0; }
    .gradient-text {
        background: linear-gradient(90deg, #0052cc, #00c6ff);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: 800;
        font-size: 2.2rem;
        text-align: center;
        padding: 10px 0;
    }
    .source-box {
        font-size: 0.85rem; color: #444; background: #f1f1f1;
        padding: 8px; border-radius: 6px; margin-top: 8px; border-left: 3px solid #0284c7;
    }
</style>
""", unsafe_allow_html=True)


# ==============================================================
# 2. CACHE T√ÄI NGUY√äN
# ==============================================================

@st.cache_resource(show_spinner=False)
def get_groq_client():
    try:
        api_key = st.secrets["GROQ_API_KEY"]
        return Groq(api_key=api_key)
    except Exception:
        return None


@st.cache_resource(show_spinner=False)
def get_embeddings():
    return HuggingFaceEmbeddings(model_name=CONSTANTS["EMBEDDING_MODEL"])


# --------------------------------------------------
#  üî• TRANSLATOR AN TO√ÄN ‚Äì KH√îNG BAO GI·ªú CRASH
# --------------------------------------------------
@st.cache_resource(show_spinner=False)
def get_translator():
    """
    Translator an to√†n ‚Äì n·∫øu l·ªói th√¨ tr·∫£ v·ªÅ None.
    App v·∫´n ho·∫°t ƒë·ªông nh·ªù embedding ƒëa ng√¥n ng·ªØ.
    """
    model_name = CONSTANTS["TRANSLATION_MODEL"]

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        translator = pipeline(
            "translation",
            model=model,
            tokenizer=tokenizer,
            src_lang="vi",
            tgt_lang="en",
        )
        return translator

    except Exception as e:
        st.warning(
            "‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c model d·ªãch ti·∫øng Vi·ªát ‚Üí ti·∫øng Anh.\n"
            "‚Üí H·ªá th·ªëng t·ª± ƒë·ªông chuy·ªÉn sang ch·∫ø ƒë·ªô fallback (kh√¥ng c·∫ßn d·ªãch).\n"
            f"Chi ti·∫øt l·ªói: {str(e)}"
        )
        return None


# ==============================================================
# 3. CLASS KNOWLEDGEBASE
# ==============================================================

class KnowledgeBase:
    def __init__(self, embeddings):
        self.embeddings = embeddings

    def load_documents(self) -> List[Document]:
        if not os.path.exists(CONSTANTS["PDF_DIR"]):
            os.makedirs(CONSTANTS["PDF_DIR"])
            return []

        pdf_files = glob.glob(os.path.join(CONSTANTS["PDF_DIR"], "*.pdf"))
        docs = []

        for pdf_path in pdf_files:
            try:
                reader = PdfReader(pdf_path)
                fname = os.path.basename(pdf_path)
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text:
                        docs.append(Document(
                            page_content=text,
                            metadata={"source": fname, "page": i + 1}
                        ))
            except Exception as e:
                st.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file {pdf_path}: {e}")

        return docs

    def build_or_load_vector_db(self, force_rebuild=False):
        path = CONSTANTS["VECTOR_STORE_PATH"]

        if os.path.exists(path) and not force_rebuild:
            try:
                return FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)
            except:
                pass

        docs = self.load_documents()
        if not docs:
            return None

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=CONSTANTS["CHUNK_SIZE"],
            chunk_overlap=CONSTANTS["CHUNK_OVERLAP"]
        )
        chunks = splitter.split_documents(docs)

        vector_db = FAISS.from_documents(chunks, self.embeddings)

        try:
            vector_db.save_local(path)
        except:
            pass

        return vector_db


# ==============================================================
# 4. H√ÄM TI·ªÜN √çCH
# ==============================================================

def translate_vi_to_en(translator, text):
    if not translator:
        return None
    try:
        out = translator(text, max_length=512)
        return out[0]["translation_text"]
    except:
        return None


def retrieve_context(vector_db, query, k=3):
    if not vector_db or not query:
        return "", []

    try:
        docs = vector_db.similarity_search(query, k=k)
        ctx = []
        srcs = []

        for d in docs:
            ctx.append(f"[TR√çCH]: {d.page_content.strip()}")
            srcs.append(f"{d.metadata.get('source')} (Tr. {d.metadata.get('page')})")

        return "\n\n".join(ctx), srcs
    except:
        return "", []


def build_system_prompt(context_text):
    return f"""
B·∫°n l√† tr·ª£ l√Ω ·∫£o KTC, chuy√™n gia Tin h·ªçc GDPT 2018.
Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n [NGU·ªíN T√ÄI LI·ªÜU] b√™n d∆∞·ªõi.
N·∫øu t√†i li·ªáu kh√¥ng c√≥ th√¥ng tin ‚Üí tr·∫£ l·ªùi: "SGK hi·ªán ch∆∞a ƒë·ªÅ c·∫≠p v·∫•n ƒë·ªÅ n√†y."

[NGU·ªíN T√ÄI LI·ªÜU]:
{context_text}
"""


# ==============================================================
# 5. KH·ªûI T·∫†O
# ==============================================================

groq_client = get_groq_client()
if groq_client is None:
    st.error("‚ùå Ch∆∞a c·∫•u h√¨nh GROQ_API_KEY!")
    st.stop()

embeddings = get_embeddings()
translator = get_translator()
kb = KnowledgeBase(embeddings)

if "vector_db" not in st.session_state:
    with st.spinner("üîÑ ƒêang t·∫£i Vector Database..."):
        st.session_state.vector_db = kb.build_or_load_vector_db()


# ==============================================================
# 6. SIDEBAR
# ==============================================================

with st.sidebar:
    if os.path.exists(CONSTANTS["LOGO_PATH"]):
        st.image(CONSTANTS["LOGO_PATH"], use_container_width=True)

    st.title("‚öôÔ∏è Control Panel")

    st.markdown("---")

    if st.button("üîÑ Rebuild d·ªØ li·ªáu"):
        with st.spinner("ƒêang c·∫≠p nh·∫≠t..."):
            st.session_state.vector_db = kb.build_or_load_vector_db(force_rebuild=True)
        st.success("ƒê√£ x√¢y d·ª±ng l·∫°i!")
        st.rerun()

    if st.button("üóë X√≥a l·ªãch s·ª≠ chat"):
        st.session_state.messages = []
        st.rerun()


# ==============================================================
# 7. CHAT HISTORY
# ==============================================================

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Xin ch√†o! T√¥i l√† **KTC AI** ‚Äì tr·ª£ l√Ω Tin h·ªçc c·ªßa b·∫°n."}
    ]


# ==============================================================
# 8. MAIN CHAT UI
# ==============================================================

col1, col2, col3 = st.columns([1, 8, 1])

with col2:
    st.markdown('<h1 class="gradient-text">TR·ª¢ L√ù ·∫¢O TIN H·ªåC KTC</h1>', unsafe_allow_html=True)

    for msg in st.session_state.messages:
        avatar = "üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ"
        with st.chat_message(msg["role"], avatar=avatar):
            st.markdown(msg["content"], unsafe_allow_html=True)

    user_input = st.chat_input("B·∫°n mu·ªën h·ªèi g√¨? (g√µ ti·∫øng Vi·ªát)")

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})

        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        # --------------------------------------------
        # 1) D·ªãch Vi ‚Üí En (n·∫øu translator c√≥)
        # --------------------------------------------
        query_en = translate_vi_to_en(translator, user_input)
        search_text = query_en if query_en else user_input

        # --------------------------------------------
        # 2) L·∫•y context
        # --------------------------------------------
        with st.spinner("üîé ƒêang truy v·∫•n d·ªØ li·ªáu..."):
            context_text, sources = retrieve_context(st.session_state.vector_db, search_text)

        # --------------------------------------------
        # 3) Build prompt
        # --------------------------------------------
        sys_prompt = build_system_prompt(context_text)

        # --------------------------------------------
        # 4) Model tr·∫£ l·ªùi d·∫°ng streaming
        # --------------------------------------------
        with st.chat_message("assistant", avatar="ü§ñ"):
            placeholder = st.empty()
            full = ""

            try:
                stream = groq_client.chat.completions.create(
                    model=CONSTANTS["MODEL_NAME"],
                    stream=True,
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user", "content": user_input}
                    ]
                )

                for chunk in stream:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, "content") and delta.content:
                        full += delta.content
                        placeholder.markdown(full + "‚ñå")

                # Hi·ªán ngu·ªìn
                if sources:
                    src_html = "<div class='source-box'>üìö <b>Ngu·ªìn:</b><br>" + "<br>".join([f"‚Ä¢ {s}" for s in sources]) + "</div>"
                    full = full + "\n\n" + src_html

                placeholder.markdown(full, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": full})

            except Exception as e:
                err = f"‚ùå L·ªói khi g·ªçi m√¥ h√¨nh: {str(e)}"
                placeholder.markdown(err)
                st.session_state.messages.append({"role": "assistant", "content": err})
