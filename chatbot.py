import os
import glob
import base64
import streamlit as st
import shutil
import re
import uuid
import hashlib
import time
from typing import List, Generator

# --- Imports v·ªõi x·ª≠ l√Ω l·ªói ---
try:
    import nest_asyncio
    nest_asyncio.apply()
    from llama_parse import LlamaParse

    from langchain_community.vectorstores import FAISS
    from langchain_community.retrievers import BM25Retriever
    from langchain.retrievers import EnsembleRetriever
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.documents import Document
    from groq import Groq
    from flashrank import Ranker, RerankRequest
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    IMPORT_ERROR = str(e)

# ==============================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==============================

st.set_page_config(
    page_title="KTC Chatbot - THCS & THPT Ph·∫°m Ki·ªát",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

class AppConfig:
    LLM_MODEL = 'llama-3.1-8b-instant'
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    RERANK_MODEL_NAME = "ms-marco-TinyBERT-L-2-v2"
    
    PDF_DIR = "PDF_KNOWLEDGE"
    VECTOR_DB_PATH = "faiss_db_index"
    RERANK_CACHE = "./opt"
    PROCESSED_MD_DIR = "PROCESSED_MD"
    
    LOGO_PROJECT = "LOGO.jpg"
    LOGO_SCHOOL = "LOGO PKS.png"
    
    RETRIEVAL_K = 30
    FINAL_K = 5 
    LLM_TEMPERATURE = 0.0

# ===============================
# 2. X·ª¨ L√ù GIAO DI·ªÜN (UI MANAGER - GI·ªÆ NGUY√äN)
# ===============================

class UIManager:
    @staticmethod
    def get_img_as_base64(file_path):
        if not os.path.exists(file_path):
            return ""
        with open(file_path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    @staticmethod
    def inject_custom_css():
        st.markdown("""
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap');
            html, body, [class*="css"], .stMarkdown, .stButton, .stTextInput, .stChatInput {
                font-family: 'Inter', sans-serif !important;
            }
            section[data-testid="stSidebar"] {
                background-color: #f8f9fa; border-right: 1px solid #e9ecef;
            }
            .project-card {
                background: white; padding: 15px; border-radius: 12px;
                box-shadow: 0 2px 8px rgba(0,0,0,0.05); margin-bottom: 20px;
                border: 1px solid #dee2e6;
            }
            .project-title {
                color: #0077b6; font-weight: 800; font-size: 1.1rem;
                margin-bottom: 5px; text-align: center; text-transform: uppercase;
            }
            .project-sub {
                font-size: 0.8rem; color: #6c757d; text-align: center;
                margin-bottom: 15px; font-style: italic;
            }
            .main-header {
                background: linear-gradient(135deg, #023e8a 0%, #0077b6 100%);
                padding: 1.5rem 2rem; border-radius: 15px; color: white;
                margin-bottom: 2rem; box-shadow: 0 8px 20px rgba(0, 119, 182, 0.3);
                display: flex; align-items: center; justify-content: space-between;
            }
            .header-left h1 {
                color: #caf0f8 !important; font-weight: 900; margin: 0;
                font-size: 2.2rem; letter-spacing: -0.5px;
            }
            .citation-badge {
                font-size: 0.75em; color: white; background-color: #0077b6;
                padding: 3px 8px; border-radius: 12px; font-weight: 600;
                margin-left: 5px; display: inline-flex; align-items: center;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            [data-testid="stChatMessageContent"] {
                border-radius: 15px !important; padding: 1rem !important;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
        </style>
        """, unsafe_allow_html=True)

    @staticmethod
    def render_sidebar():
        with st.sidebar:
            if os.path.exists(AppConfig.LOGO_SCHOOL):
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2: st.image(AppConfig.LOGO_SCHOOL, use_container_width=True)
                st.markdown("<div style='text-align:center; font-weight:700; color:#023e8a; margin-bottom:20px;'>THCS & THPT PH·∫†M KI·ªÜT</div>", unsafe_allow_html=True)

            st.markdown("""
            <div class="project-card">
                <div class="project-title">KTC CHATBOT</div>
                <div class="project-sub">S·∫£n ph·∫©m d·ª± thi KHKT c·∫•p T·ªânh</div>
                <hr style="margin: 10px 0; border-top: 1px dashed #dee2e6;">
                <div style="font-size: 0.9rem; line-height: 1.6;">
                    <div style="display: flex; justify-content: space-between;">
                        <span style="font-weight: 600; color: #555;">T√°c gi·∫£:</span>
                        <span style="text-align: right; color: #222;"><b>B√πi T√° T√πng</b><br><b>Cao S·ªπ B·∫£o Chung</b></span>
                    </div>
                    <div style="display: flex; justify-content: space-between; margin-top: 8px;">
                        <span style="font-weight: 600; color: #555;">GVHD:</span>
                        <span style="text-align: right; color: #222;">Th·∫ßy <b>Nguy·ªÖn Th·∫ø Khanh</b></span>
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)

            st.markdown("### ‚öôÔ∏è Ti·ªán √≠ch")
            if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()

            if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu (Rebuild DB)", use_container_width=True):
                if os.path.exists(AppConfig.VECTOR_DB_PATH):
                    shutil.rmtree(AppConfig.VECTOR_DB_PATH)
                st.session_state.pop('retriever_engine', None)
                st.toast("ƒê√£ x√≥a cache. H·ªá th·ªëng s·∫Ω h·ªçc l·∫°i t·ª´ ƒë·∫ßu!", icon="‚úÖ")
                time.sleep(1)
                st.rerun()

    @staticmethod
    def render_header():
        logo_nhom_b64 = UIManager.get_img_as_base64(AppConfig.LOGO_PROJECT)
        img_html = f'<img src="data:image/jpeg;base64,{logo_nhom_b64}" style="width:100px; height:100px; border-radius:50%; border:3px solid rgba(255,255,255,0.3); object-fit:cover;">' if logo_nhom_b64 else ""

        st.markdown(f"""
        <div class="main-header">
            <div class="header-left">
                <h1>KTC CHATBOT</h1>
                <p>H·ªçc Tin d·ªÖ d√†ng - Thao t√°c v·ªØng v√†ng</p>
            </div>
            <div class="header-right">{img_html}</div>
        </div>
        """, unsafe_allow_html=True)


# ==================================
# 3. LOGIC BACKEND (REFACTORED FOR ROBUSTNESS)
# ==================================

class RAGEngine:
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_groq_client():
        try:
            api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
            return Groq(api_key=api_key) if api_key else None
        except Exception: return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_embedding_model():
        try:
            return HuggingFaceEmbeddings(
                model_name=AppConfig.EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        except Exception: return None

    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_reranker():
        try:
            return Ranker(model_name=AppConfig.RERANK_MODEL_NAME, cache_dir=AppConfig.RERANK_CACHE)
        except Exception: return None

    @staticmethod
    def _structural_chunking(text: str, source_meta: dict) -> List[Document]:
        # Gi·ªØ nguy√™n logic chunking
        lines = text.split('\n')
        chunks = []
        current_chapter = "T·ªïng quan"
        current_lesson = "B√†i h·ªçc chung"
        current_section = "N·ªôi dung"
        current_page = "N/A"
        buffer = []
        
        p_chapter = re.compile(r'^#*\s*\**\s*(CH∆Ø∆†NG|Ch∆∞∆°ng)\s+([IVX0-9]+).*$', re.IGNORECASE)
        p_lesson = re.compile(r'^#*\s*\**\s*(B√ÄI|B√†i)\s+([0-9]+).*$', re.IGNORECASE)
        p_section = re.compile(r'^(###\s+|[IV0-9]+\.\s+|[a-z]\)\s+).*')
        p_page = re.compile(r'^-+\s*(Page|Trang)\s*(\d+)\s*-+$', re.IGNORECASE)

        def clean_header(text): return text.replace('#', '').replace('*', '').strip()
        def commit_chunk(buf, meta, page):
            if not buf: return
            content = "\n".join(buf).strip()
            if len(content) < 20: return 
            hash_input = (meta.get("source", "") + str(page) + content[:50]).encode('utf-8')
            chunk_hash = hashlib.sha256(hash_input).hexdigest()[:8]
            new_meta = meta.copy()
            new_meta.update({"chunk_uid": chunk_hash, "chapter": current_chapter, "lesson": current_lesson, "section": current_section, "page": page})
            chunks.append(Document(page_content=content, metadata=new_meta))

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped: continue
            if p_page.match(line_stripped):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []; current_page = p_page.match(line_stripped).group(2)
                continue
            if p_chapter.match(line_stripped):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []; current_chapter = clean_header(line_stripped)
            elif p_lesson.match(line_stripped):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []; current_lesson = clean_header(line_stripped)
            elif p_section.match(line_stripped) or line_stripped.startswith("### "):
                commit_chunk(buffer, source_meta, current_page)
                buffer = []; current_section = clean_header(line_stripped)
            else:
                buffer.append(line)
        commit_chunk(buffer, source_meta, current_page)
        return chunks

    @staticmethod
    def _parse_pdf_with_llama(file_path: str) -> str:
        os.makedirs(AppConfig.PROCESSED_MD_DIR, exist_ok=True)
        file_name = os.path.basename(file_path)
        md_file_path = os.path.join(AppConfig.PROCESSED_MD_DIR, f"{file_name}.md")

        if os.path.exists(md_file_path):
            with open(md_file_path, "r", encoding="utf-8") as f: return f.read()

        llama_api_key = st.secrets.get("LLAMA_CLOUD_API_KEY") or os.environ.get("LLAMA_CLOUD_API_KEY")
        if not llama_api_key: 
            st.error("‚ùå L·ªói: Thi·∫øu LLAMA_CLOUD_API_KEY.")
            return ""

        try:
            parser = LlamaParse(api_key=llama_api_key, result_type="markdown", language="vi")
            documents = parser.load_data(file_path)
            if documents:
                with open(md_file_path, "w", encoding="utf-8") as f: f.write(documents[0].text)
                return documents[0].text
        except Exception as e: 
            st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc file {file_name}: {str(e)}")
        return ""

    @staticmethod
    def build_hybrid_retriever(embeddings):
        """
        Phi√™n b·∫£n Robust: T·ª± ƒë·ªông ph√°t hi·ªán l·ªói DB v√† Rebuild.
        Kh√¥ng d√πng 'try-except-pass' c·∫©u th·∫£.
        """
        if not embeddings: return None

        # 1. Th·ª≠ load DB c≈©
        if os.path.exists(AppConfig.VECTOR_DB_PATH):
            try:
                print("Attempting to load local FAISS DB...")
                vector_db = FAISS.load_local(AppConfig.VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)
                # Test nhanh retriever
                return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
            except Exception as e:
                print(f"Database corrupt or incompatible: {e}. Deleting and rebuilding...")
                shutil.rmtree(AppConfig.VECTOR_DB_PATH) # X√≥a ngay n·∫øu l·ªói
        
        # 2. N·∫øu kh√¥ng load ƒë∆∞·ª£c (ho·∫∑c ƒë√£ x√≥a), b·∫Øt ƒë·∫ßu Build m·ªõi
        if not os.path.exists(AppConfig.PDF_DIR): 
            st.warning(f"‚ö†Ô∏è Th∆∞ m·ª•c '{AppConfig.PDF_DIR}' ch∆∞a ƒë∆∞·ª£c t·∫°o.")
            return None
            
        files = glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf"))
        if not files: 
            st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF n√†o trong th∆∞ m·ª•c d·ªØ li·ªáu.")
            return None

        all_chunks = []
        progress_bar = st.progress(0, text="ƒêang b·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu...")
        
        for idx, f in enumerate(files):
            progress_bar.progress((idx + 1) / len(files), text=f"ƒêang ƒë·ªçc: {os.path.basename(f)} (B∆∞·ªõc d√πng AI ƒë·ªçc ·∫£nh/b·∫£ng bi·ªÉu)...")
            txt = RAGEngine._parse_pdf_with_llama(f)
            if len(txt) > 50:
                chunks = RAGEngine._structural_chunking(txt, {"source": os.path.basename(f)})
                all_chunks.extend(chunks)
            else:
                st.toast(f"File {os.path.basename(f)} kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c n·ªôi dung.", icon="‚ö†Ô∏è")
        
        progress_bar.empty()

        if all_chunks:
            try:
                with st.spinner("Dang t·∫°o ch·ªâ m·ª•c Vector (Vector Indexing)..."):
                    vector_db = FAISS.from_documents(all_chunks, embeddings)
                    vector_db.save_local(AppConfig.VECTOR_DB_PATH)
                    return vector_db.as_retriever(search_kwargs={"k": AppConfig.RETRIEVAL_K})
            except Exception as e:
                st.error(f"‚ùå L·ªói nghi√™m tr·ªçng khi t·∫°o FAISS DB: {str(e)}")
                return None
        else:
            st.error("‚ùå Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ PDF. Vui l√≤ng ki·ªÉm tra l·∫°i Key LlamaParse ho·∫∑c ƒë·ªãnh d·∫°ng File.")
            return None

    @staticmethod
    def generate_response(client, retriever, query) -> Generator[str, None, None]:
        # --- KI·ªÇM TRA TR·∫†NG TH√ÅI TI·ªÄN ƒêI·ªÄU KI·ªÜN ---
        if not client:
            yield "‚ùå L·ªói: Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c v·ªõi Groq AI (API Key Error)."
            return

        if not retriever:
            # N·∫øu retriever ch∆∞a s·∫µn s√†ng, th·ª≠ ki·ªÉm tra xem c√≥ ph·∫£i do ch∆∞a c√≥ d·ªØ li·ªáu kh√¥ng
            yield "‚ö†Ô∏è H·ªá th·ªëng ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra: \n1. ƒê√£ upload PDF v√†o th∆∞ m·ª•c 'PDF_KNOWLEDGE' ch∆∞a?\n2. B·∫•m n√∫t 'C·∫≠p nh·∫≠t d·ªØ li·ªáu' ·ªü c·ªôt b√™n tr√°i."
            return

        # --- B·∫ÆT ƒê·∫¶U QUY TR√åNH RAG ---
        try:
            # 1. Retrieval
            initial_docs = retriever.invoke(query)
            if not initial_docs:
                yield "Hi·ªán t·∫°i trong t√†i li·ªáu ch∆∞a c√≥ th√¥ng tin kh·ªõp v·ªõi c√¢u h·ªèi c·ªßa b·∫°n."
                return

            scored_docs = []
            for doc in initial_docs:
                src = doc.metadata.get('source', '')
                # ∆Øu ti√™n ngu·ªìn SGK
                bonus = 1.0 if ("SGK" in src or "Tin" in src) else 0.0
                scored_docs.append({"doc": doc, "bonus": bonus})

            # 2. Rerank (Optional)
            final_docs = []
            ranker = RAGEngine.load_reranker()
            if ranker:
                passages = [{"id": str(i), "text": x["doc"].page_content, "meta": x["doc"].metadata} for i, x in enumerate(scored_docs)]
                req = RerankRequest(query=query, passages=passages)
                results = ranker.rank(req)
                results.sort(key=lambda x: x['score'], reverse=True)
                final_docs = [Document(page_content=r['res']['text'], metadata=r['res']['meta']) for r in results[:AppConfig.FINAL_K]]
            else:
                scored_docs.sort(key=lambda x: x['bonus'], reverse=True)
                final_docs = [x["doc"] for x in scored_docs[:AppConfig.FINAL_K]]

            # 3. Context Construction
            valid_uids = {}
            context_parts = []
            for doc in final_docs:
                uid = doc.metadata.get('chunk_uid', 'unknown')
                src_name = doc.metadata.get('source', 'TL').replace('.pdf', '')
                lesson = doc.metadata.get('lesson', '')
                page = doc.metadata.get('page', '')
                
                # T·∫°o badge hi·ªÉn th·ªã
                source_display = f"{src_name} > {lesson}"
                if page and page != "N/A": source_display += f" (Tr.{page})"
                
                valid_uids[uid] = f'<span class="citation-badge">üìò {source_display}</span>'
                context_parts.append(f"--- [ID:{uid}] ---\nN·ªôi dung: {doc.page_content}\n----------------")

            full_context = "\n".join(context_parts)

            # 4. Prompting
            system_prompt = (
                "B·∫°n l√† Tr·ª£ l√Ω AI gi√°o d·ª•c KHKT.\n"
                "QUY T·∫ÆC:\n"
                "1. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT b√™n d∆∞·ªõi.\n"
                "2. N·∫øu kh√¥ng c√≥ th√¥ng tin, n√≥i 'T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin trong SGK'.\n"
                "3. M·ªçi √Ω ph·∫£i c√≥ tr√≠ch d·∫´n [ID:xxxx] ·ªü cu·ªëi c√¢u.\n\n"
                f"CONTEXT:\n{full_context}"
            )

            completion = client.chat.completions.create(
                model=AppConfig.LLM_MODEL,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": query}],
                temperature=AppConfig.LLM_TEMPERATURE,
                stream=True 
            )

            # 5. Streaming & Verification
            full_ans = ""
            for chunk in completion:
                token = chunk.choices[0].delta.content
                if token:
                    full_ans += token
                    # Clean output tr·ª±c ti·∫øp trong l√∫c stream n·∫øu c·∫ßn (·ªü ƒë√¢y stream raw ƒë·ªÉ nhanh)
                    yield token

            # (Optional) Verify IDs sau khi stream xong - C√≥ th·ªÉ th√™m logic highlight ·ªü ƒë√¢y

        except Exception as e:
            yield f"‚ö†Ô∏è ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}"

# ===================
# 4. MAIN APPLICATION
# ===================

def main():
    if not DEPENDENCIES_OK:
        st.error(f"‚ö†Ô∏è Thi·∫øu th∆∞ vi·ªán: {IMPORT_ERROR}")
        st.stop()

    UIManager.inject_custom_css()
    UIManager.render_sidebar()
    UIManager.render_header()

    # --- KH·ªûI T·∫†O CLIENTS ---
    groq_client = RAGEngine.load_groq_client()
    if not groq_client:
        st.error("‚ö†Ô∏è Ch∆∞a c·∫•u h√¨nh GROQ_API_KEY trong secrets.toml")
        st.stop()

    # --- KH·ªûI T·∫†O DATABASE (QUAN TR·ªåNG: CH·ªà L√ÄM 1 L·∫¶N) ---
    if "retriever_engine" not in st.session_state:
        # Ki·ªÉm tra PDF tr∆∞·ªõc
        if not os.path.exists(AppConfig.PDF_DIR) or not glob.glob(os.path.join(AppConfig.PDF_DIR, "*.pdf")):
             st.info("üëã Ch√†o m·ª´ng! H√£y t·∫°o th∆∞ m·ª•c 'PDF_KNOWLEDGE' v√† b·ªè file PDF gi√°o tr√¨nh v√†o ƒë√≥ ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
             st.session_state.retriever_engine = None
        else:
            with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông b·ªô n√£o AI (ki·ªÉm tra Vector Database)..."):
                embeddings = RAGEngine.load_embedding_model()
                st.session_state.retriever_engine = RAGEngine.build_hybrid_retriever(embeddings)
                
                if st.session_state.retriever_engine is None:
                    st.error("‚ùå Kh·ªüi t·∫°o th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i API Key ho·∫∑c d·ªØ li·ªáu ƒë·∫ßu v√†o.")

    # --- CHAT INTERFACE ---
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "üëã Ch√†o b·∫°n! Th·∫ßy Khanh v√† nh√≥m KHKT ƒë√£ n·∫°p d·ªØ li·ªáu cho m√¨nh. H√£y h·ªèi v·ªÅ Tin h·ªçc nh√©!"}]

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"], avatar=("üßë‚Äçüéì" if msg["role"] == "user" else "ü§ñ")):
            st.markdown(msg["content"], unsafe_allow_html=True)

    if user_input := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="üßë‚Äçüéì"):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar="ü§ñ"):
            response_placeholder = st.empty()
            full_response = ""
            
            # G·ªçi Generator
            response_gen = RAGEngine.generate_response(
                groq_client,
                st.session_state.retriever_engine,
                user_input
            )
            
            try:
                for chunk in response_gen:
                    full_response += chunk
                    # X·ª≠ l√Ω hi·ªÉn th·ªã badge m√†u ngay l·∫≠p t·ª©c (n·∫øu model tr·∫£ v·ªÅ d·∫°ng [ID:...])
                    display_text = re.sub(
                        r'\[ID:([a-fA-F0-9]+)\]', 
                        r'<span class="citation-badge" style="background:#444;">Ngu·ªìn \1</span>', 
                        full_response
                    )
                    response_placeholder.markdown(display_text + "‚ñå", unsafe_allow_html=True)
                
                # Final render
                # Thay th·∫ø ID th·∫≠t b·∫±ng Badge ƒë·∫πp d·ª±a tr√™n context (Advanced) - ·ªû ƒë√¢y l√†m ƒë∆°n gi·∫£n
                response_placeholder.markdown(full_response, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            except Exception as e:
                st.error(f"Stream Error: {e}")

if __name__ == "__main__":
    main()